---

title: 'OpenAI放“闪亮产品”高于安全，离职研究人员称'
date: 2024-05-19
author: ByteAILab

---

一位前OpenAI高级员工表示，负责ChatGPT的公司正在优先考虑“闪亮产品”而非安全，揭示他因对关键目标产生分歧而辞职。

---
Jan Leike曾是OpenAI的关键安全研究员，担任超级对准合作领导，确保强大的人工智能系统遵守人类的价值观和目标。他的介入发生在下周首尔举行的全球人工智能峰会之前，政治家、专家和科技高管将在峰会上讨论对该技术的监管。

Leike在旧金山公司推出最新的AI模型GPT-4o后几天辞职。他的离职意味着继OpenAI的联合创始人兼超级对准合作领导Ilya Sutskever辞职后，这家公司有两位高级安全人士本周离职。

Leike在周五发布的X内容中详细介绍了他辞职的原因，称安全文化已经变得不再是重中之重。“过去几年，安全文化和流程已经让位于闪亮产品，”他写道。

OpenAI成立的目标是确保人工通用智能，描述为“比人类智能更智能的人工智能系统”，造福全人类。Leike在X内容中表示，他与OpenAI领导层对公司优先事项持有不同意见已有一段时间，而这种对峙终于达到了“破裂点”。

Leike表示，OpenAI还开发了Dall-E图像生成器和Sora视频生成器，应该在下一代模型中投入更多资源解决安全、社会影响、机密性和安全性等问题。“这些问题很难解决，我担心我们无法朝着正确的方向发展。”他补充说，他的团队越来越难以开展研究。“构建比人类更聪明的机器是一项固有危险的工作。OpenAI肩负着代表全人类的重大责任，”Leike写道，并补充称，OpenAI“必须成为一个以安全为先的AGI公司”。

OpenAI首席执行官Sam Altman在X上回应了Leike的帖子，感谢他对公司安全文化的贡献。

Sutskever在他的X帖子宣布离职时写道，他相信OpenAI在当前领导下“将构建既安全又有益的AGI”。Sutskever最初支持去年11月将Altman撤出OpenAI的老板职位，然后在公司内部动荣数天后又支持他被重新任命。

正如国际人工智能专家组发布的一份首个AI安全报告所指出的，人们就强大的人工智能系统逃避人类控制的可能性存在分歧。然而，该报告警告说，监管者可能赶不上科技快速进步的步伐，警告存在“技术进步速度和监管响应速度之间潜在的鸿沟”。

探索更多有关这些主题的内容：

人工智能（AI）
观察家
ChatGPT
OpenAI
计算
新闻

[分享]：转载此内容

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。