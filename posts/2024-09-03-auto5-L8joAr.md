---

title: '用最直观的动画，讲解LLM如何存储事实，3Blue1Brown的这个视频又火了'
date: 2024-09-04
author: ByteAILab

---

本文根据视频整理而来，有听错或理解不当之处欢迎在评论区指出。

向大模型输入「Michael Jordan plays the sport of _____（迈克尔・乔丹从事的体育运动是……）」，然后让其预测接下来的文本，那么大模型多半能正确预测接下来是「basketball（篮球）」。

---


这说明在其数以亿计的参数中潜藏了有关这个特定个人的相关知识。用户甚至会感觉这些模型记忆了大量事实。

但事实究竟如何呢？

近日，3Blue1Brown 的《深度学习》课程第 7 课更新了，其中通过生动详实的动画展示了 LLM 存储事实的方式。视频浏览量高达 18 万次。[图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicYVK35Oyfp5y5ffH58eYv4NgG82EYibJb5jfsQXhiaOEVNPBSlCDoqIQXTyNSPFiat7QOpXInnsDibKg/640?wx_fmt=png&amp;from=appmsg)

去年 12 月，谷歌 DeepMind 的一些研究者发布了一篇相关论文，其中的具体案例便是匹配运动员以及他们各自的运动项目。[图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicYVK35Oyfp5y5ffH58eYv4ISEoEYScdU0E4E70j6XpLCOWBTvQeXFxWVrXAlOXOSWSdcY9Dsibl5g/640?wx_fmt=png&amp;from=appmsg)

虽然这篇论文并未完全解答有关 LLM 事实存储的问题，但也得到了一些颇为有趣的结果，其中的一个重点是：事实保存在网络中的一个特定部分，这个部分也就是我们熟知的多层感知器（MLP）。

在 3Blue1Brown 刚刚更新的这期视频中，他们用 23 分的视频演示了大型语言模型如何存储和处理信息，主要包括以下部分：

- LLM 中隐藏的事实是什么
- 快速回顾 Transformers
- 示例
- 多层感知器
- 计算参数[图片](https://image.jiqizhixin.com/uploads/editor/c197eb6f-7d19-46c6-8ba1-0830e489ca6e/1725259652030.png)

视频地址：[https://www.youtube.com/watch?v=9-Jl0dxWQs8](https://www.youtube.com/watch?v=9-Jl0dxWQs8)

在演示视频中，3b1b 的作者口齿清晰、语言标准，配合着高清画面，让读者很好地理解了 LLM 是如何存储知识的。

很多用户在看完视频后，都惊讶于 3Blue1Brown 教学质量：[图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicYVK35Oyfp5y5ffH58eYv4UDR7icz1M4efCYBMu2ehHMByA8vyAgzOfDdbHjm1e2dxc5AK3w01Fzw/640?wx_fmt=png)

还有网友表示，坐等更新这期视频已经很久了：[图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicYVK35Oyfp5y5ffH58eYv44GxBNtJRPXyQ55K8ykria5PE1IMVNAH1lYWDmFgApw3pSz6aZdJFkHA/640?wx_fmt=png)

接下来我们就深入 MLP 的细节吧。在这篇文章中，机器之心简要介绍了核心内容，感兴趣的读者可以通过原视频查看完整内容。

MLP 在大模型中的占比不小，但其实结构相比于注意力机制这些要简单许多。尽管如此，理解它也还是有些难度。为了简单，下面继续通过「乔丹打篮球」这个具体示例进行说明吧。[..更多内容]

---

。注意：Title、Date、Body 三个部分的内容，放入到对应的位置。最后只需要按照格式标准输出为Makedown源文件格式内容。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。