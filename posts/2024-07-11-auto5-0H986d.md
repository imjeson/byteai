---
title: '单一作者论文，谷歌提出百万专家Mixture，超越密集前馈、稀疏MoE'
date: 2024-07-12
author: ByteAILab

---

释放进一步扩展 Transformer 的潜力，同时还可以保持计算效率。

标准 Transformer 架构中的前馈（FFW）层会随着隐藏层宽度的增加而导致计算成本和激活内存的线性增加。

---
在大语言模型（LLM）体量不断增大的现在，稀疏混合专家（MoE）架构已成为解决此问题的可行方法，它将模型大小与计算成本分离开来。很多新兴的 MoE 模型都可以实现相同体量之上，更好的性能与更强大的表现。

最近发现的细粒度 MoE 扩展定律表明，更高的粒度可带来更好的性能。然而由于计算和优化方面的挑战，现有的 MoE 模型仅限于低数量专家。

本周二，**Google DeepMind 的新研究引入了一种参数高效的专家检索机制，其利用乘积密钥技术从一百万个微型专家中进行稀疏检索**。

链接：https://arxiv.org/abs/2407.04153

该方法尝试通过用于路由的学习索引结构有效地串联到大量微小专家，从而将计算成本与参数计数分离。与密集的 FFW、粗粒度 MoE 和产品密钥存储器 (PKM) 层相比，表现出卓越的效率。

...

PEER 研究只有一位作者 Xu He（Owen），他是 Google DeepMind 研究科学家，2017 年博士毕业于荷兰格罗宁根大学。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibCbWxdqrc5IDu2HDna2Qyb2cVWHrVpW6jl143fRToeib6ufxGGm2euswJjGuM3Rl03Nq2LnvmkoCw/640?wx_fmt=png&from=appmsg)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。