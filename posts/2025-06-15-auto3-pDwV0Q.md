---
title: 'KAYTUS推出升级版MotusAI，以加速大型语言模型部署'
date: 2025-06-16
author: ByteAILab
---

精简推理性能、工具兼容性、资源调度和系统稳定性，以快速推进大型AI模型的部署。![图片](https://ai-techpark.com/wp-content/uploads/KAYTUS-1.jpg){ width=60% }

---

KAYTUS，一家提供端到端AI和液冷解决方案的领先供应商，今天宣布在2025年高性能计算国际会议（ISC High Performance 2025）上发布其MotusAI AI DevOps平台的最新版本。升级后的MotusAI平台在大型模型推理性能方面进行了显著增强，并提供广泛的兼容性，支持覆盖大型模型全生命周期的多个开源工具。该平台旨在统一和动态地进行资源调度，显著提高大型AI模型开发和部署中的资源利用率和运营效率。MotusAI的最新发布将进一步加速AI的采用，并推动教育、金融、能源、汽车和制造等关键领域的商业创新。

随着大型AI模型越来越多地嵌入到现实应用中，企业以规模化的方式进行部署，以在多种行业中创造实际价值。然而，许多组织在人工智能的采用过程中仍面临着关键挑战，包括部署周期延长、严格的稳定性要求、碎片化的开源工具管理和低计算资源利用率。为了解决这些难点，KAYTUS推出了新的MotusAI AI DevOps平台，旨在简化AI部署，增强系统稳定性，并优化大型模型操作的AI基础设施效率。

增强推理性能以确保服务质量
部署AI推理服务是一项复杂的工作，涉及服务部署、管理和持续健康监测。这些任务需要严格的模型和服务治理标准、通过加速框架的性能调优以及长期服务稳定性，这通常需要在劳动力、时间和技术专长上的重大投资。升级版MotusAI提供强大的大型模型部署能力，使可见性和性能完美契合。通过整合优化框架，例如SGLang和vLLM，MotusAI确保高性能、分布式的推理服务，使企业能够快速、充满信心地进行部署。专为支持大参数模型而设计，MotusAI利用智能资源和网络亲和力调度，加速上市时间，同时最大化硬件利用率。其内置监控能力涵盖从硬件和平台到容器和服务的全栈，提供自动故障诊断和快速服务恢复。MotusAI还支持基于实时使用和资源监控的推理工作负载动态扩展，提供增强的服务稳定性。

全面的工具支持以加速AI采用
随着AI模型技术迅速发展，支持生态系统的开发工具也在不断复杂化。开发者需要一个简化的、通用的平台，以高效选择、部署和操作这些工具。升级后的MotusAI提供对众多领先开源工具的广泛支持，使企业用户能够按需配置和管理他们的模型开发环境。内置工具如LabelStudio，加速了跨多种类别的数据注释和同步，提高了数据处理效率，加快了模型开发周期。MotusAI还提供了整个AI模型生命周期的集成工具链，包括用于数据注释和治理的LabelStudio和OpenRefine、用于微调大型模型的LLaMA-Factory、用于大型模型应用开发的Dify和Confluence，以及用于文本到图像生成的Stable Diffusion。这些工具共同使用户能够快速采用大型模型并推动规模化的开发生产力。

在同一节点上实现混合训练-推理调度，以最大化资源效率
有效利用计算资源仍然是AI初创企业和中小企业在人工智能采用早期阶段的关键优先事项。传统的AI集群通常将计算节点分别分配给训练和推理任务，限制了两种工作负载之间的资源调度弹性和效率。升级后的MotusAI通过允许在单个节点上混合调度训练和推理工作负载，克服了传统的限制，实现不同任务类型的无缝集成和动态编排。配备先进的GPU调度能力，MotusAI支持按需资源分配，使用户能够根据工作负载要求高效管理GPU资源。MotusAI还具有多维GPU调度功能，包括细粒度分区和对多实例GPU（MIG）的支持，能够满足模型开发、调试和推理的广泛使用案例。MotusAI的增强调度器显著优于社区版本，为大规模POD部署提供5倍的任务吞吐量提升和5倍的延迟减少。它能够快速启动和环境准备数百个POD，同时支持训练和推理的动态工作负载扩展和潮汐调度。这些能力使得在各种实际AI场景中实现无缝任务编排成为可能。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。