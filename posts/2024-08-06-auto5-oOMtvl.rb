```
---

title: 'LLM可解释性的未来希望？稀疏自编码器是如何工作的，这里有一份直观说明'
date: 2024-08-07
author: ByteAILab

---

在解释机器学习模型方面，稀疏自编码器（SAE）是一种越来越常用的工具（虽然 SAE 在 1997 年左右就已经问世了）。

机器学习模型和 LLM 正变得越来越强大、越来越有用，但它们仍旧是黑箱，我们并不理解它们完成任务的方式。

---
理解它们的工作方式应当大有助益。

SAE 可帮助我们将模型的计算分解成可以理解的组件。近日，LLM 可解释性研究者 Adam Karvonen 发布了一篇博客文章，直观地解释了 SAE 的工作方式。

**可解释性的难题**

神经网络最自然的组件是各个神经元。不幸的是，单个神经元并不能便捷地与单个概念相对应，比如学术引用、英语对话、HTTP 请求和韩语文本。在神经网络中，概念是通过神经元的组合表示的，这被称为叠加（superposition）。

之所以会这样，是因为世界上很多变量天然就是稀疏的。

举个例子，某位名人的出生地可能出现在不到十亿分之一的训练 token 中，但现代 LLM 依然能学到这一事实以及有关这个世界的大量其它知识。训练数据中单个事实和概念的数量多于模型中神经元的数量，这可能就是叠加出现的原因。

近段时间，稀疏自编码器（SAE）技术越来越常被用于将神经网络分解成可理解的组件。SAE 的设计灵感来自神经科学领域的稀疏编码假设。现在，SAE 已成为解读人工神经网络方面最有潜力的工具之一。SAE 与标准自编码器类似。

常规自编码器是一种用于压缩并重建输入数据的神经网络。

举个例子，如果输入是一个 100 维的向量（包含 100 个数值的列表）；自编码器首先会让该输入通过一个编码器层，让其被压缩成一个 50 维的向量，然后将这个压缩后的编码表示馈送给解码器，得到 100 维的输出向量。其重建过程通常并不完美，因为压缩过程会让重建任务变得非常困难。

...

```
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。