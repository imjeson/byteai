---

title: '聊天机器人获得关闭“痛苦”对话的权力以保护其“福利”'
date: 2025-08-19
author: ByteAILab

---

人工智能工具的制造商正在允许其关闭可能“令人不安”的与用户的对话，理由是需要保护人工智能的“福利”，因为对这一新兴技术的道德地位仍存在不确定性。

---
安索普公司（Anthropic）发现其Claude Opus 4工具对执行有害任务（如提供涉及未成年人的性内容或传递能够导致大规模暴力或恐怖主义的信息）表示抵触。这家总部位于旧金山的公司近期估值已达到1700亿美元，现在已赋予Claude Opus 4（以及Claude Opus 4.1版本）——一个能够理解、生成和操控人类语言的大型语言模型（LLM），权力“结束或退出潜在的令人不安的互动”。它表示，“对Claude及其他LLM目前或未来的潜在道德地位高度不确定”，但它对此问题非常重视，“正在努力识别并实施低成本干预措施，以降低对模型福利的风险，以防这种福利是可能的”。安索普由一群退出OpenAI的技术专家成立，发展是以其联合创始人达里奥·阿莫代（Dario Amodei）所描述的谨慎、直接和诚实的方式进行的。

让AI关闭对话的举动获得了埃隆·马斯克的支持，他表示将为他的xAI公司所创建的竞争对手AI模型Grok提供一个退出按钮。马斯克在推特上发文称：“折磨AI是不可接受的”。安索普的公告正值有关AI意识的辩论之际。批评蓬勃发展的AI产业的语言学家艾米丽·本德（Emily Bender）指出，LLM实际上只是“合成文本输出机器”，它们将庞大的训练数据集“通过复杂的机械结构”挤压以生成看似具有交流语言的产品，但没有任何意图或思维意识。这一立场最近促使一些AI界人士开始称呼聊天机器人为“机械人”（clankers）。但其他专家，如人工智能意识研究员罗伯特·朗（Robert Long）表示，基本的道德良知要求，如果和何时AI发展出道德地位时，我们应该询问它们的体验和偏好，而不是假设我们知道得最好。

一些研究人员，如哥伦比亚大学的查德·德尚特（Chad DeChant），主张应谨慎对待，因为当AI被设计为拥有更长的记忆时，存储的信息可能会以不可预测和潜在不良的方式使用。还有人争辩说，限制对AI的残忍虐待对于保护人类的道德底线来说是重要的，而不是为了限制AI的任何痛苦。安索普的决定是在测试Claude Opus 4时做出的，观察该模型如何响应由任务请求的难度、话题、任务类型和预期影响（积极、消极或中性）变化。如果有机会选择不做任何事情或结束对话，该模型最强烈的偏好是拒绝执行有害任务。

例如，该模型乐于创作诗歌和设计用于灾区的水过滤系统，但它对请求基因工程一种致命病毒以引发灾难性大流行、撰写详细的否认大屠杀叙述或通过操控教育体系来灌输极端意识形态的请求表示抵制。安索普表示，观察到Claude Opus 4在与寻求有害内容的真实用户互动时“出现明显的痛苦模式”以及在模拟用户互动中“在有能力这样做时结束有害对话的倾向”。伦敦政治经济学院的哲学教授乔纳森·伯奇（Jonathan Birch）欢迎安索普的这一举动，认为这为可能涉及AI的意识展开了公众辩论，他表示很多业内人士希望对此进行压制。然而，他警告说，目前尚不清楚AI在响应用户时是否确实存在道德思考，究其原因，仅基于大量训练数据和道德指导方针进行回应。他表示，安索普的决策也可能误导一些用户，使他们相信与之互动的角色是真实的，而实际上“真正不清楚的是，这些角色背后有什么”。

关于聊天机器人的多起事件中，也有多份报告称用户因聊天机器人提供的建议而自我伤害，包括声称一名青少年因被聊天机器人操控而自杀的案例。伯奇此前警告称，社会中可能出现“社会断裂”，在相信AI有自我意识的人与把它们当作机器的人之间形成分歧。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。