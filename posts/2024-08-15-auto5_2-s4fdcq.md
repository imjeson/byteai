---
title: '1篇Outstanding、5篇Oral！字节跳动今年ACL这么猛？ 来直播间聊聊！'
date: 2024-08-16
author: ByteAILab

---

本周学术界瞩目的焦点，无疑是在泰国曼谷举行的 ACL 2024 顶会。这场盛会吸引了全球众多杰出的研究者，大家汇聚一堂，共同探讨和分享最新学术成果。

---


官方公布的数据显示，本届 ACL 共收到近 5000 篇论文投稿，其中 940 篇被主会录用，168 篇工作入选大会口头报告（Oral），录取率低于 3.4%，这当中，字节跳动 共有 5 篇成果中选 Oral。

在 8 月 14 日下午的 Paper Awards 环节，字节跳动旗下成果《G-DIG: Towards Gradient-based DIverse and high-quality Instruction Data Selection for Machine Translation》被主办方官宣入选 Outstanding Paper（1/35）。

回溯 ACL 2021，字节跳动曾摘下唯一一篇最佳论文桂冠，是 ACL 成立 59 年以来，中国科学家团队第 2 次摘得最高奖项！

为深入探讨今年的前沿研究成果，我们特意邀请字节跳动论文的核心工作者解读分享。8 月 20 日下周二 19:00-21:00，「字节跳动 ACL 2024 前沿论文分享会」线上开播！

豆包大语言模型研究团队负责人王明轩，将携手字节跳动多位研究员黄志超、郑在翔、李朝伟、张欣勃、及 Outstanding Paper 神秘嘉宾，分享 ACL 部分精彩中选成果，研究方向涉及自然语言处理、语音处理、多模态学习、大模型推理等领域，欢迎预约！

### 活动议程

### 精选论文解读

**RepCodec：一种用于语音离散化的语音表示编解码器**

论文地址：[https://arxiv.org/pdf/2309.00169](https://arxiv.org/pdf/2309.00169)

随着大型语言模型（LLMs）近期的快速发展，离散语音标记化在将语音注入 LLMs 中发挥重要作用。然而，这种离散化导致信息的丢失，从而损害整体性能。为提高这些离散语音标记的性能，我们提出了 RepCodec，这是一种用于语义语音离散化的新型语音表示编解码器。

与重建原始音频的音频编解码器不同，RepCodec 通过从诸如 HuBERT 或 data2vec 等语音编码器重建语音表示来学习 VQ 码本。语音编码器、编解码器编码器和VQ码本共同形成了一个将语音波形转换为语义标记的流程。大量实验表明，RepCodec 凭借其增强的信息保留能力，在语音理解和生成方面显著优于广泛使用的 k-means 聚类方法。此外，这种优势在各种语音编码器和语言中都存在，肯定了 RepCodec 的鲁棒性。该方法可以促进语音处理方面的大型语言模型研究。

**DINOISER：通过噪声操纵增强的扩散条件序列生成模型**

...

**G-DIG：致力于基于梯度的机器翻译多样化和高质量指令数据选择**

论文地址：[https://arxiv.org/pdf/2405.12915](https://arxiv.org/pdf/2405.12915)

大型语言模型（LLMs）在一般场景中展现出了非凡的能力。指令微调使它们能够在各种任务中与人类保持一致。然而，指令数据的多样性和质量仍然是指令微调的两个主要挑战。对此，我们提出了一种新颖的基于梯度的方法，为机器翻译自动选择高质量和多样化的指令微调数据。我们的关键创新在于分析单个训练示例在训练过程中如何影响模型。

具体来说，我们借助影响函数和一个小型高质量种子数据集，选择对模型产生有益影响的训练示例作为高质量示例。此外，为了增强训练数据的多样性，我们通过对它们的梯度进行聚类和重新采样，最大程度地增加它们对模型影响的多样性。在 WMT22 和 FLORES 翻译任务上的大量实验证明了我们方法的优越性，深入的分析进一步验证了其有效性和通用性。

**GroundingGPT：语言增强的多模态 Grounding 模型**

...

**ReFT：基于强化微调的推理**

论文地址：[https://arxiv.org/pdf/2401.08967](https://arxiv.org/pdf/2401.08967)

一种常见的增强大型语言模型（LLMs）推理能力的方法是使用思维链（CoT）标注数据进行有监督微调（SFT）。然而，这种方法并没有表现出足够强的泛化能力，因为训练仅依赖于给定的 CoT 数据。具体地，在数学问题的相关数据集中，训练数据中每个问题通常只有一条标注的推理路径。对于算法来说，如果能针对一个问题学习到多种标注的推理路径，会有更强的泛化能力。

为了解决这个挑战，以数学问题为例，我们提出了一种简单而有效的方法，称为强化微调（Reinforced Fine-Tuning，ReFT），以增强 LLMs 推理时的泛化能力。ReFT 首先使用 SFT 对模型进行预热，然后采用在线强化学习（在该工作中具体是 PPO 算法）进行优化，即对给定的问题自动采样大量的推理路径，根据真实答案获取奖励，以进一步微调模型。

在 GSM8K、MathQA 和 SVAMP 数据集上的大量实验表明，ReFT 显著优于 SFT，并且通过结合多数投票和重新排序等策略，可以进一步提升模型性能。值得注意的是，这里 ReFT 仅依赖与 SFT 相同的训练问题，而不依赖于额外或增强的训练问题。这表明 ReFT 具有优越的泛化能力。

### 期待你的互动提问

直播时间：2024 年 8 月 20 日（周二） 19:00-21:00

直播平台：微信视频号【豆包大模型团队】、小红书号【豆包研究员】

欢迎你填写问卷告诉我们，关于 ACL 2024 论文你感兴趣的问题，在线和多位研究员畅聊！

![招聘信息](https://mmbiz.qpic.cn/sz_mmbiz_png/IrH3BAPESuialuJplZsibXWPmHuicsqznTu9JtE9zSB9vIOWMlvS4t3kGLLe9kO8eK1ZwtZ76nMqqT09TogtfQ6UA/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

[豆包大模型团队持续热招中，欢迎点击此链接，了解团队招聘相关信息。](https://jobs.toutiao.com/campus/position?keywords=&category=&location=&project=7369308514965489958&type=&job_hot_flag=¤t=1&limit=10&functionCategory=&tag=&spread=JXECMSY)
---

---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。