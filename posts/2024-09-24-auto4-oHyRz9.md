---

title: '带你认识微信多模态大模型 POINTS'
date: 2024-09-25
author: ByteAILab

---

近来，随着大型语言模型的发展，视觉语言大型模型的能力也在逐步增强，GPT-4、Gemini Pro 1.5和Claude 3等著名的闭源模型成功将 LLM 扩展到视觉语言模型领域。LLaVA，InternVL等开源模型也在迅速发展。

---
目前，视觉语言模型领域存在一些关键问题亟待解决：闭源模型很少公开关于其架构的详细信息。相比之下，开源模型虽公开了其训练策略，但这些策略的详细消融并没有完全披露。在目前的开源工作中，对于预训练阶段，大多都是凭经验添加不同来源的数据集，这使得预训练过程难以得到深入的探索。在微调阶段，绝大多数工作关注的重点通常是添加和消融更多的数据集，这样性能会较快触及瓶颈。我们针对以上几点给出了我们的方案，并进行了清晰充分的实验论证。

POINTS 主要包含 3 个部分:

1. 强 baseline: 通过融入当前主流的前沿技术，我们为进一步探索创造了一个较强的 baseline

2. pre-train 数据集过滤策略：借鉴 LLM 中常用的利用 ppl 来过滤数据的思想，我们利用一个离线模型使用 ppl 的方式来过滤 pre-train 数据，最终得到 1M 高效的 pre-train 数据

3. model soup: 在指令微调阶段，过滤大部分工作都集中在消融更多的数据集来增强模型性能，但这种方式很可能达到一个阶段后带来的增益就比较有限，我们率先提出在不同指令微调数据集上训练得到的模型进行 model soup，实验结果表明，模型的性能可以得到进一步较大的提升

......

[查看完整文章](https://www.aixinzhijie.com/article/6846812)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。