title: 'Ai2推出MolmoAct，3D空间推理的AI模型'
date: 2025-08-14
author: ByteAILab

---

首个将空间规划与视觉推理相结合的模型，旨在实现更安全、更灵活的机器人控制
今日，Ai2（艾伦人工智能研究所）宣布发布MolmoAct 7B，这是一款突破性的具身AI模型，将最先进的AI模型智能引入物理世界。![图片](https://ai-techpark.com/wp-content/uploads/Ai2-Unveils.jpg){ width=60% }

---
MolmoAct通过生成视觉推理token，能够真正“看”它的周围环境，理解空间、运动和时间之间的关系，并据此规划其移动。该模型将2D图像输入转化为3D空间计划，使机器人能够更智能、更高效地导航物理世界。

虽然空间推理并不新颖，但现代大多数系统依赖于封闭的端到端架构，基于大量的专有数据集进行训练。这些模型难以复现，扩展成本高昂，且通常作为不透明的黑箱运作。MolmoAct提供了一种根本不同的方法：它完全基于开放数据训练，设计为透明，并为实际世界广泛化而构建。其逐步的视觉推理轨迹使用户能够轻松预览机器人计划执行的内容，并在条件变化时实时调整其行为。

“具身AI需要新的基础，优先考虑推理、透明和开放性，”Ai2首席执行官Ali Farhadi表示。“通过MolmoAct，我们不仅是在发布一个模型；我们是在为一个新的AI时代奠定基础，将强大AI模型的智慧带入物理世界。这是朝着能够以更符合人类方式进行推理和导航的AI迈出的一步，使其能够安全有效地与我们协作。”

新型模型：动作推理
MolmoAct是Ai2称之为动作推理模型（Action Reasoning Model, ARM）的首个代表，该模型解读高层次自然语言指令，并通过一系列物理动作进行推理，从而在现实世界中实现任务。与传统的端到端机器人模型将任务视为单一步骤的做法不同，ARMs解读高层指令，然后将其分解为透明的空间基础决策链：

- 3D感知：利用深度和空间上下文支撑机器人对环境的理解
- 视觉路径规划：在图像空间中绘制逐步任务轨迹
- 动作解码：将规划转换为精确的、特定于机器人的控制命令

这种分层推理使MolmoAct能够将“对这个垃圾堆进行分类”理解为一个结构化的子任务序列：识别场景，将物体按类型分组，一次抓取一个物体，然后重复。

为广泛应用而构建并经过效率训练
MolmoAct 7B是该模型系列的第一款，训练时使用了大约12,000个来自真实环境（如厨房和卧室）的“机器人集”的经过整理的数据集。这些示例被转化为机器人推理序列，展示了复杂指令如何映射到有目标的具体行动上。我们还发布了MolmoAct后训练数据集，包含约12,000个不同的“机器人集”。Ai2研究人员花费数月时间整理机器人在多种家庭环境中执行动作的视频，从在客厅沙发上整理枕头到在卧室整理洗衣物。

尽管表现强劲，MolmoAct的训练效率出奇地高。它只需要18百万个样本，在256个NVIDIA H100 GPU上进行约24小时的预训练，以及在64个GPU上仅需额外2小时的微调。相比之下，许多商业模型需要数亿个样本和大量计算资源。然而，MolmoAct在多个关键基准测试中超越了许多这样的系统，包括在SimPLER上的71.9%的成功率，证明高质量的数据和深思熟虑的设计能够超越那些需要更大量数据和计算的模型。

可理解的AI供你构建
与大多数操作作为不透明系统的机器人模型不同，MolmoAct被构建为透明。用户可以在执行之前预览模型的计划运动，运动轨迹会叠加在摄像机图像上。用户可以使用自然语言或在触摸屏上快速绘制修正来调整这些计划，提供细致的控制并增强在家庭、医院和仓库等现实环境中的安全性。

MolmoAct符合Ai2的使命，完全开源且可复现。Ai2发布了构建、运行和扩展模型所需的所有内容：训练管道、训练前后数据集、模型检查点和评估基准。

MolmoAct为具身AI设定了新标准——安全、可解释、适应性强并且真正开放。Ai2将继续在模拟和现实环境中扩展测试，目标是推动更强大、更具协作能力的AI系统的发展。

从Ai2的Hugging Face仓库下载模型和模型文档，包括训练检查点和评估。

---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。