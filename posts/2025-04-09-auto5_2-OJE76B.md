---
title: 'CVPR 2025 HighLight｜打通视频到3D的最后一公里，清华团队推出一键式视频扩散模型VideoScene'
date: 2025-04-10
author: ByteAILab
---

![图片](https://image.jiqizhixin.com/uploads/editor/aef2b002-4670-43e1-b1dc-2ba24a1b7b55/640.png)

**论文有两位共同一作。汪晗阳，清华大学计算机系本科四年级，研究方向为三维视觉、生成模型，已在CVPR、ECCV、NeurIPS等会议发表论文。

---
刘芳甫，清华大学电子工程系直博二年级，研究方向为生成模型 (3D AIGC和Video Generation等)，已在CVPR、ECCV、NeurIPS、ICLR、KDD等计算机视觉与人工智能顶会发表过多篇论文。**

![图片](https://image.jiqizhixin.com/uploads/editor/32a74032-0ffd-41f9-b8dd-349836c4e64d/640.gif)

&nbsp;  &nbsp;  &nbsp;  &nbsp;从视频到 3D 的桥梁：VideoScene 一步到位

随着 VR/AR、游戏娱乐、自动驾驶等领域对 3D 场景生成的需求不断攀升，从稀疏视角重建 3D 场景已成为一大热点课题。但传统方法往往需要大量图片、繁琐的多步迭代，既费时又难以保证高质量的 3D 结构重建。

**来自清华大学的研究团队首次提出 VideoScene：一款 “一步式” 视频扩散模型，专注于 3D 场景视频生成。**它利用了 3D-aware leap flow distillation 策略，通过跳跃式跨越冗余降噪步骤，极大地加速了推理过程，同时结合动态降噪策略，实现了对 3D 先验信息的充分利用，从而在保证高质量的同时大幅提升生成效率。

- 论文标题：VideoScene：Distilling Video Diffusion Model to Generate 3D Scenes in One Step
- 论文地址: [https://arxiv.org/abs/2504.01956](https://arxiv.org/abs/2504.01956)
- 项目主页: [https://hanyang-21.github.io/VideoScene](https://hanyang-21.github.io/VideoScene)
- Github 仓库: [https://github.com/hanyang-21/VideoScene](https://github.com/hanyang-21/VideoScene)

**稀疏视角重建方法挑战**

在稀疏视角重建领域，从少量图像中精准恢复 3D 场景是个极具挑战性的难题。传统方法依赖多视角图像间的匹配与几何计算，但当视角稀疏时，匹配点不足、几何约束缺失，使得重建的 3D 模型充满瑕疵，像物体结构扭曲、空洞出现等。

为突破这一困境，一些前沿方法另辟蹊径，像 ReconX 就创新性地借助视频生成模型强大的生成能力，把重建问题与生成问题有机结合。它将稀疏视角图像构建成全局点云，编码为 3D 结构条件，引导视频扩散模型生成具有 3D 一致性的视频帧，再基于这些帧重建 3D 场景，在一定程度上缓解了稀疏视角重建的不适定问题。

不过，当前大多数 video to 3D 工具仍存在效率低下的问题。一方面，生成的 ** 3D 视频质量欠佳**，难以生成三维结构稳定、细节丰富、时空连贯的视频。在处理复杂场景时，模型容易出现物体漂移、结构坍塌等问题，导致生成的 3D 视频实用性大打折扣。另一方面，基于扩散模型的视频生成通常需要 **多步降噪过程**，每一步都涉及大量计算，不仅 **耗时久**，还带来 **高昂的计算开销**，限制了其在实际场景中的应用。

**继承与超越：ReconX 理念的进化**

此前研究团队提出 video-to-3D 的稀释视角重建方法 ReconX，核心在于将 3D 结构指导融入视频扩散模型的条件空间，以此生成 3D 一致的帧，进而重建 3D 场景。它通过构建全局点云并编码为 3D 结构条件，引导视频扩散模型工作，在一定程度上解决了稀疏视角重建中 3D 一致性的问题。

VideoScene 继承了 ReconX 将 3D 结构与视频扩散相结合的理念，并在此基础上实现了重大改进，堪称 **ReconX 的 “turbo 版本”。**

在 3D 结构指导方面，VideoScene 通过独特的 ** 3D 跃迁流蒸馏策略**，巧妙地跳过了传统扩散模型中耗时且冗余的步骤，直接从含有丰富 3D 信息的粗略场景渲染视频开始，加速了整个扩散过程。同时也使得 3D 结构信息能更准确地融入视频扩散过程。在生成视频帧时，VideoScene 引入了更强大的动态降噪策略，不仅仅依赖于固定的降噪模式，而是根据视频内容的动态变化实时调整降噪参数，从而既保证了生成视频的高质量，又极大地提高了效率。

![图片](https://image.jiqizhixin.com/uploads/editor/bb6d03fb-6719-4c74-a2a2-b37a3adf839d/640.png)

&nbsp;  &nbsp;  &nbsp;  &nbsp;研究团队提出的 VideoScene 方法流程图

**实验结果**

通过在多个真实世界数据集上的大量实验，VideoScene 展示出了 **卓越的性能**。它不仅在 **生成速度上远超现有的视频扩散模型**，而且在生成质量上也毫不逊色，甚至在某些情况下还能达到更好的效果。这意味着 VideoScene 有望成为未来视频到 3D 应用中的一个 **重要工具**。在实时游戏、自动驾驶等需要高效 3D 重建的领域，有潜力能发挥巨大的作用。

![图片](https://image.jiqizhixin.com/uploads/editor/c19d0560-9de6-48d9-b114-56ae4222b1d4/640.png)

&nbsp;  &nbsp;  &nbsp;  &nbsp;VideoScene 单步生成结果优于 baseline 模型 50 步生成结果

![图片](https://image.jiqizhixin.com/uploads/editor/00552e86-1923-4b28-a6e6-cfa4c1cd918c/640.png)

&nbsp;  &nbsp;  &nbsp;  &nbsp; 视频扩散模型在不同去噪步数下的表现

如果你对 VideoScene 感兴趣，想要深入了解它的技术细节和实验结果，可访问论文原文、项目主页和 GitHub 仓库。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。