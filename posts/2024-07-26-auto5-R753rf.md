---

title: '英伟达对话模型ChatQA进化到2.0版本，上下文长度提到128K'
date: 2024-07-27
author: ByteAILab

---

开放 LLM 社区正是百花齐放、竞相争鸣的时代，你能看到 Llama-3-70B-Instruct、QWen2-72B-Instruct、Nemotron-4-340B-Instruct、Mixtral-8x22BInstruct-v0.1 等许多表现优良的模型。但是，相比于以 GPT-4-Turbo 为代表的专有大模型，开放模型在很多领域依然还有明显差距。

---

在通用模型之外，也有一些专精关键领域的开放模型已被开发出来，比如用于编程和数学的 DeepSeek-Coder-V2、用于视觉 - 语言任务的 InternVL 1.5（其在某些领域可比肩 GPT-4-Turbo-2024-04-09）。
作为「AI 淘金时代的卖铲王」，英伟达自身也在为开放模型领域做贡献，比如其开发的 ChatQA 系列模型，参阅机器之心报道英伟达新对话 QA 模型准确度超 GPT-4，却遭吐槽：无权重代码意义不大。今年初，ChatQA 1.5 发布，其整合了检索增强式生成（RAG）技术，在对话问答方面的表现超过了 GPT-4。
现在，ChatQA 进化到 2.0 版，这一次改进的主要方向是扩展上下文窗口。
论文标题：ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities
论文地址：https://arxiv.org/pdf/2407.14482
近段时间，扩展 LLM 的上下文窗口长度是一大研究和开发热点，比如机器之心曾报道过的直接扩展到无限长，谷歌 Infini-Transformer 终结上下文长度之争。
所有领先的专有 LLM 都支持非常大的上下文窗口 —— 你可以在单个 prompt 中向其灌输数百页文本。比如 GPT-4 Turbo 和 Claude 3.5 Sonnet 的上下文窗口大小分别为 128K 和 200K。而 Gemini 1.5 Pro 可支持 10M 长度的上下文，让人叹为观止。
不过开源大模型也在加紧追赶。比如 QWen2-72B-Instruct 和 Yi-34B 各自支持 128K 和 200K 的上下文窗口。但是，这些模型的训练数据和技术细节并未公开，因此很难去复现它们。此外，这些模型的评估大都基于合成任务，无法准确地代表在真实下游任务上的性能。比如，有多项研究表明开放 LLM 和领先的专有模型在真实世界长上下文理解任务上依然差距明显。
而英伟达的这个团队成功让开放的 Llama-3 在真实世界长上下文理解任务上的性能赶上了专有的 GPT-4 Turbo。
在 LLM 社区中，长上下文能力有时被认为是一种能与 RAG 竞争的技术。但实事求是地说，这些技术是可以相互增益的。
对具有长上下文窗口的 LLM 来说，根据下游任务以及准确度和效率之间的权衡，可以考虑在 prompt 附带大量文本，也可以使用检索方法从大量文本中高效地提取出相关信息。RAG 具有明显的效率优势，可为基于查询的任务轻松地从数十亿 token 中检索出相关信息。这是长上下文模型无法具备的优势。另一方面，长上下文模型却非常擅长文档总结等 RAG 可能并不擅长的任务。
因此，对一个先进的 LLM 来说，这两种能力都需要，如此才能根据下游任务以及准确度和效率需求来考虑使用哪一种。
此前，英伟达开源的 ChatQA 1.5 模型已经能在 RAG 任务上胜过 GPT-4-Turbo 了。但他们没有止步于此，如今又开源了 ChatQA 2，将足以比肩 GPT-4-Turbo 的长上下文理解能力也整合了进来！
具体来说，他们基于 Llama-3 模型，将...
---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。