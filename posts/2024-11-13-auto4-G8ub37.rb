---
title: 'OpenAI大改下代大模型方向，scaling law撞墙？AI社区炸锅了'
date: 2024-11-14
author: ByteAILab

---

昨天，The Information 的一篇文章让 AI 社区炸了锅。

这篇文章透露，OpenAI 下一代旗舰模型的质量提升幅度不及前两款旗舰模型之间的质量提升，因为高质量文本和其他数据的供应量正在减少，原本的 Scaling Law（用更多的数据训练更大的模型）可能无以为继。

---
此外，OpenAI 研究者 Noam Brown 指出，更先进的模型可能在经济上也不具有可行性，因为花费数千亿甚至数万亿美元训练出的模型会很难盈利。

这篇文章引发了业界对于未来 AI 迭代方向的讨论 —— 虽然 Scaling Law 放缓这一说法令人担忧，但其中也不乏乐观的声音。有人认为，虽然从预训练来看，Scaling Law 可能会放缓；但有关推理的 Scaling Law 还未被充分挖掘，OpenAI o1 的发布就证明了这一点。它从后训练阶段入手，借助强化学习、原生的思维链和更长的推理时间，把大模型的能力又往前推了一步。这种范式被称为「测试时计算」，相关方法包括思维链提示、多数投票采样（self-consistency）、代码执行和搜索等。

其实，除了测试时计算，还有另外一个近来非常受关注的概念 —— 测试时训练（ Test-Time Training ，TTT），二者都试图在测试（推理）阶段通过不同的手段来提升模型的性能，但 TTT 会根据测试时输入，通过显式的梯度步骤更新模型。这种方法不同于标准的微调，因为它是在一个数据量极低的环境中运行的 —— 通常是通过单个输入的无监督目标，或应用于一个或两个 in-context 标注示例的有监督目标。

不过，TTT 方法的设计空间很大。目前，对于哪些设计选择对 LM（特别是对新任务学习）最有效，人们的了解还很有限。

在一篇新论文中，来自 MIT 的研究者系统地研究了各种 TTT 设计选择的影响，以及它与预训练和采样方案之间的相互作用。看起来，TTT 的效果非常好，至少从论文标题上看，它的抽象推理能力惊人（surprising）。

论文标题：The Surprising Effectiveness of Test-Time Training for Abstract Reasoning
论文链接：https://ekinakyurek.github.io/papers/ttt.pdf

具体来说，作者确定了将 TTT 有效应用于 few-shot 学习的几个关键要素：

1. 在与测试时类似的合成任务上进行初始微调；
2. 用于构建测试时数据集的增强型 leave-1-out 任务生成策略；
3. 训练适用于每个实例的适应器；
4. 可逆变换下的自我一致性（self-consistency）方法。

实验环节，研究者在抽象与推理语料库（ARC）中对这些方法进行了评估。ARC 语料库收集了很多极具挑战性的 few-shot 视觉推理问题，被认为是测试 LM 泛化极限的理想基准。目前的大多语言模型在 ARC 上均表现不佳。

ARC 推理任务示例。可以看到，这是一组类似于智力测试的问题，模型需要找到图形变换的规则，以推导最后的输出结果。

通过对这些部分的精心选择，TTT 可以显著提高 LM 在 ARC 上的性能 —— 在 1B 模型上将准确率提高到原来的 6 倍，使用 8B 模型时也超过其它已发布的 SOTA 纯神经模型方法。

事实上，他们的研究结果表明，当配备测试时训练时，普通的语言模型可以在 ARC 任务上达到或超过许多神经 - 符号方法的性能。

这些结果挑战了这样一个假设：解决这类复杂任务必须严格依赖符号组件。相反，它们表明解决新推理问题的关键因素可能是在测试时分配适当的计算资源，也许与这些资源是通过符号还是神经机制部署无关。

数据科学家 Yam Peleg 高度评价了这项研究：

美国 Jackson 实验室基因组学部教授 Derya Unutmaz 则表示这是一项「令人震惊的研究」，因为如果 TTT 与 LLM 相结合足以实现抽象推理，我们就有可能消除对显式、老式符号逻辑的需求，并找到实现 AGI 的可行途径。

不过，过完一关还有一关：Epoch AI 与 60 多位顶尖数学家合作打造的 FrontierMath，已经成为评估人工智能高级数学推理能力的新基准，恐怕接下来各位 AI 研究者有的忙了。

---

该文章原始链接：<a href="https://www.aixinzhijie.com/article/6847214" rel="nofollow noopener">https://www.aixinzhijie.com/article/6847214</a>【原文图片链接保留，请注意查看】
​
【图片展示请参考原文链接】
​
​
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。