---

title: '人工智能公司被警告评估超级智能的威胁，否则可能失去对其的控制'
date: 2025-05-11
author: ByteAILab

---

人工智能公司被敦促复制支撑罗伯特·奥本海默首次核试验的安全计算，以便在释放所有强大的系统之前。

---
人工智能安全活动家马克斯·泰格马克表示，他进行了类似美国物理学家阿瑟·康普顿在“三位一体”实验前所做的计算，并发现高度先进的人工智能有90%的概率会构成生存威胁。

美国政府在1945年进行了“三位一体”实验，之前是基于对大气引发核反应的可能性几乎为零的保证。在泰格马克与他的三名学生在麻省理工学院（MIT）共同发表的一篇论文中，他们建议计算“康普顿常数”，该常数在论文中被定义为强大人工智能逃避人类控制的概率。在1959年接受美国作家珍·巴克的采访时，康普顿表示他在计算出失控的核聚变反应的几率“略低于”三百万分之一后，批准了这一实验。

泰格马克说，人工智能公司应对计算是否失去对人工超级智能（ASI，即一种在各方面优于人类智能的理论系统）控制的责任。他说：“构建超级智能的公司需要计算康普顿常数，即我们失去对它控制的概率。他们不能仅仅说‘我们对此感到乐观’。他们必须计算百分比。”

泰格马克表示，由多家公司共同计算的康普顿常数共识将创造出全球达成人工智能安全机制的“政治意愿”。泰格马克是麻省理工学院物理学教授和人工智能研究员，也是未来生命研究所的联合创始人，该非营利组织支持安全开发人工智能。该所于2023年发布了一封公开信，呼吁暂停开发强大人工智能。信件获得如埃隆·马斯克等人——该机构的早期支持者，以及苹果公司联合创始人史蒂夫·沃兹尼亚克的签名。这封信是在ChatGPT发布几个月后产生的，标志着人工智能开发的新纪元，警告人工智能实验室陷入一种“失控的竞争”中，部署“越来越强大的数字智能”，“没人能够理解、预测或可靠控制”。

泰格马克在接受《卫报》采访时表示，一组包括科技行业专业人士、国家安全机构代表和学者的人工智能专家正在制定新的安全开发方法。《新加坡共识：全球人工智能安全研究优先事项报告》是由泰格马克、世界领先的计算机科学家约书亚·本吉奥和来自OpenAI以及Google DeepMind等主要人工智能公司的员工共同编写的。该报告列出了人工智能安全研究的三个广泛优先领域：开发测量当前和未来人工智能系统影响的方法；说明人工智能应如何行为并设计实现该行为的系统；管理和控制系统的行为。

提及该报告时，泰格马克表示，人工智能安全开发的论点在巴黎最新的政府人工智能峰会后重新回到了正轨。当时美国副总统JD·范斯表示，人工智能的未来“不会通过对安全的忧虑来赢得”。泰格马克说道：“巴黎的忧虑似乎已经消散，国际合作正在迅速回归。”

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。