---
title: '上山下海的数据中心，背得动AI能耗的锅吗？'
date: 2024-04-30
author: ByteAILab

---

文章来源：产业象限编辑部 产业象限
作者丨山茶 
编辑丨钱江 

能源焦虑似乎是一个永恒的话题，从蒸汽机到内燃机，从煤炭到石油，世界的发展总是围绕着能源打转，到了AI时代，这一点也未曾改变。 OpenAI CEO Sam Altman在多个场合提到，未来的人工智能需要能源方面的突破，因为AI消耗的电力将远远超过人们的预期。

---
 马斯克也曾预言，未来两年内将由“缺硅”变为“缺电”。 数据显示，ChatGPT每天要响应大约2亿个请求，这个过程会消耗超过50万度电力。预计到2027年，整个人工智能行业每年将消耗85至134太瓦时（1太瓦时=10亿千瓦时）的电力，约等于北京市2023年全年的耗电总量（135.78太瓦时）。
巨大的能源消耗主要产生在两个方面，其一是驱动AI服务器计算、存储消耗的能源，其二则是为AI服务器降温消耗的能源，这里面前者占60%，后者占40%。不过，虽然前者占比更高，但由于AI的发展对算力的需求越来越大，所以即便AI芯片的能效一直在提高，但还是无法改变整体功耗持续上涨的趋势。因此，如何降低服务器冷却时消耗的能源，就成为了降低AI运营成本、减少能源消耗的关键。而为此，大到Google、微软这样的科技巨头，小到服务器研发供应商，尖端前沿如SpaceX也都在为此绞尽脑汁。

进阶的服务器 从风冷到液冷

虽然大模型的风是从2023年才刮起来的，但服务器冷却并不是什么新课题。 1945年，世界上第一台通用计算机ENIAC诞生。为了解决ENIAC耗电量巨大且发热高的问题，当时的工程师就采用了风扇来为ENIAC降温，这差不多是最早的服务器制冷技术。 但风扇制冷的效果毕竟有限，而当时的计算机又都是大块头，且耗电量...
---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。