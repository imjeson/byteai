---

title: '12年前上手深度学习，Karpathy掀起一波AlexNet时代回忆杀，LeCun、Goodfellow等都下场'
date: 2024-05-07
author: ByteAILab

---

没想到，自 2012 年 AlexNet 开启的深度学习革命已经过去了 12 年。
而如今，我们也进入了大模型的时代。

---

近日，知名 AI 研究科学家 Andrej Karpathy 的一条帖子，让参与这波深度学习变革的许多大佬们陷入了回忆杀。从图灵奖得主 Yann LeCun 到 GAN 之父 Ian Goodfellow，纷纷忆往昔。
到目前为止，该帖子已经有 63 万 + 的浏览量。
在帖子中，Karpathy 提到：有一个有趣的事实是，很多人可能听说过 2012 年 ImageNet/AlexNet 的时刻，以及它开启的深度学习革命。不过，可能很少有人知道，支持这次竞赛获胜作品的代码是由 Alex Krizhevsky 从头开始，用 CUDA/C++ 手工编写的。这个代码仓库叫做 cuda-convnet, 当时托管在 Google Code 上:
https://code.google.com/archive/p/cuda-convnet/
Karpathy 想着 Google Code 是不是已经关闭了 (?)，但他在 GitHub 上找到了一些其他开发者基于原始代码创建的新版本，比如:
https://github.com/ulrichstern/cuda-convnet
“AlexNet 是最早将 CUDA 用于深度学习的著名例子之一。”Karpathy 回忆说，正是因为使用了 CUDA 和 GPU，AlexNet 才能处理如此大规模的数据 (ImageNet)，并在图像识别任务上取得如此出色的表现。“AlexNet 不仅仅是简单地用了 GPU，还是一个多 GPU 系统。比如 AlexNet 使用了一种叫做模型并行的技术，将卷积运算分成两部分，分别运行在两个 GPU 上。”
Karpathy 提醒大家，你要知道那可是 2012 年啊！“在 2012 年 (大约 12 年前)，大多数深度学习研究都是在 Matlab 中进行，跑在 CPU 上，在玩具级别的数据集上不断迭代各种学习算法、网络架构和优化思路。” 他写道。但 AlexNet 的作者 Alex、Ilya 和 Geoff 却做了一件与当时的主流研究风格完全不同的事情 ——“不再纠结于算法细节，只需要拿一个相对标准的卷积神经网络 (ConvNet)，把它做得非常大，在一个大规模的数据集 (ImageNet) 上训练它，然后用 CUDA/C++ 把整个东西实现出来。”
Alex Krizhevsky 直接使用 CUDA 和 C++ 编写了所有的代码，包括卷积、池化等深度学习中的基本操作。这种做法非常创新也很有挑战性，需要程序员对算法、硬件架构、编程语言等有深入理解。
从底层开始的编程方式复杂而繁琐，但可以最大限度地优化性能，充分发挥硬件计算能力，也正是这种回归根本的做法为深度学习注入了一股强大动力，构成深度学习历史上的转折点。
有意思的是，这一段描述勾起不少人的回忆，大家纷纷考古 2012 年之前自己使用什么工具实现深度学习项目。纽约大学计算机科学教授 Alfredo Canziani 当时用的是 Torch，“从未听说有人使用 Matlab 进行深度学习研究......”。
对此 Yann lecun 表示同意，2012 年大多数重要的深度学习都是用 Torch 和 Theano 完成的。
Karpathy 有不同看法，他接话说，大多数项目都是在用 Matlab ，自己从未使用过 Theano，2013-2014 年使用过 Torch。
一些网友也透露 Hinton 也是用 Matlab。
看来，当时使用 Matlab 的并不少：
知名的 GAN 之父 Ian Goodfellow 也现身说法，表示当时 Yoshua 的实验室全用 Theano，还说自己在 ImageNet 发布之前，曾为 Alex 的 cuda-convnet 编写了 Theano 捆绑包。
谷歌 DeepMind 主管 Douglas Eck 现身说自己没用过 Matlab，而是 C++，然后转向了 Python/Theano。
纽约大学教授 Kyunghyun Cho 表示，2010 年，他还在大西洋彼岸，当时使用的是 Hannes SChulz 等人做的 CUV 库，帮他从 Matlab 转向了 python。
Lamini 的联合创始人 Gregory Diamos 表示，说服他的论文是吴恩达等人的论文《Deep learning with COTS HPC systems》。
论文表明 Frankenstein CUDA 集群可以击败 10,000 个 CPU 组成的 MapReduce 集群。
论文链接：https://proceedings.mlr.press/v28/coates13.pdf
不过，AlexNet 的巨大成功并非一个孤立的事件，而是当时整个领域发展趋势的一个缩影。一些研究人员已经意识到深度学习需要更大的规模和更强的计算能力，GPU 是一个很有前景的方向。Karpathy 写道，“当然，在 AlexNet 出现之前，深度学习领域已经有了一些向规模化方向发展的迹象。例如，Matlab 已经开始初步支持 GPU。斯坦福大学吴恩达实验室的很多工作都在朝着使用 GPU 进行大规模深度学习的方向发展。还有一些其他的并行努力。”
考古结束时，Karpathy 感慨道  “在编写 C/C++ 代码和 CUDA kernel 时，有一种有趣的感觉，觉得自己仿佛回到了 AlexNet 的时代，回到了 cuda-convnet 的时代。”
当下这种 "back to the basics" 的做法与当年 AlexNet 的做法有着异曲同工 ——AlexNet 的作者从 Matlab 转向 CUDA/C++，是为了追求更高的性能和更大的规模。虽然现在有了高级框架，但在它们无法轻松实现极致性能时，仍然需要回到最底层，亲自编写 CUDA/C++ 代码。
对了，当时国内的研究者们都是用什么？欢迎留言讨论。

---

[图片1](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibZxsqoibnQxzoqWunSBMcUL1Qbh7iaGWcYGdwgrojLaEXKZXxHq1XVIKia4tFviaEvPKabeZQIcibEB8Q/640?wx_fmt=png&from=appmsg)
[图片2](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibZxsqoibnQxzoqWunSBMcULNyE68JWD9d8EuticBGUWQWAdbVq7cwoN99xjnzowoN0liaBNfDZ1KFxg/640?wx_fmt=png&from=appmsg)
[图片3](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibZxsqoibnQxzoqWunSBMcULSrAYUf3OHn0WgEndD2Y15zkhvDu3XVdgvkDtcLLfrUoBmVHAZAphUg/640?wx_fmt=png&from=appmsg)
[图片4](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibZxsqoibnQxzoqWunSBMcUL3hTBPzNccSnP1o8zIXZP9KXYUjSFk2jl4CagdcAq9YmomEVr05zaPw/640?wx_fmt=png&from=appmsg)
[图片5](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibZxsqoibnQxzoqWunSBMcULyGk41IdTCXrkhhnACrKfkzqCbZnibPAaV9BN2LUYZ4bTwO94vjL6KgQ/640?wx_fmt=png&from=appmsg)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。