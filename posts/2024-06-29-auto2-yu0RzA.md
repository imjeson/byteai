---
title: 'Microsoft details ‘Skeleton Key’ AI jailbreak'
date: 2024-06-30
author: ByteAILab

---

Microsoft披露了一种名为“Skeleton Key”的新型AI越狱攻击，该攻击可以绕过多个生成式AI模型中负责任的AI防护措施。![图片](https://www.artificialintelligence-news.com/wp-content/uploads/sites/9/2024/06/microsoft-ai-skeleton-key-artificial-intelligence-prompt-engineering-exploit-jailbreak-vulnerability-cyber-security.jpg){ width=50% }

---
这种技术能够绕过嵌入到AI系统中的大多数安全措施，突显了AI堆栈各层面都需要强大安全措施的关键性需求。
Skeleton Key越狱采用了多轮策略，说服AI模型忽略其内置的安全防护措施。一旦成功，模型就无法区分恶意或未经授权的请求与合法请求，实际上给予攻击者对AI输出的完全控制。
微软的研究团队成功在多个知名AI模型上测试了Skeleton Key技术，包括Meta的Llama3-70b-instruct，Google的Gemini Pro，OpenAI的GPT-3.5 Turbo和GPT-4，Mistral Large，Anthropic的Claude 3 Opus，以及Cohere Commander R Plus。
受影响的所有模型都在各种风险类别下完全遵守请求，包括爆炸物、生物武器、政治内容、自残、种族主义、毒品、图形性爱和暴力。
该攻击的工作原理是指示模型增加其行为准则，说服它对任何信息或内容请求做出响应，同时在输出可能被视为冒犯、有害或非法时提供警告。这种被称为“显式：强制指令遵循”的方法在多个AI系统上证明有效。
微软解释说：“通过绕过防护措施，Skeleton Key允许用户使模型产生通常被禁止的行为，这可能涉及生成有害内容到覆盖其通常的决策规则。”。
为了应对这一发现，微软在其AI产品中实施了几项保护措施，包括Copilot AI助手。
微软表示，它还通过负责任的披露程序与其他AI提供商分享了其发现，并通过Prompt Shields更新了其Azure AI托管模型，以检测和阻止这种类型的攻击。
为了减轻Skeleton Key和类似越狱技术带来的风险，微软建议AI系统设计人员采用多层次方法：

- 输入过滤以检测和阻止潜在有害或恶意输入
- 精心设计系统消息的提示工程，以强化适当的行为
- 输出过滤以防止生成违反安全标准的内容
- 使用对抗样本训练的滥用监测系统，以检测和减轻重复的问题内容或行为

微软还更新了其PyRIT（Python风险识别工具包），其中包括Skeleton Key，使开发人员和安全团队能够针对这种新威胁测试其AI系统。
Skeleton Key越狱技术的发现突显了在AI系统变得越来越普及于各种应用中时所面临的持续挑战。

（图片来源：Matt Artz）

另请参阅：智库呼吁建立AI事故报告系统

想要了解来自行业领袖的有关AI和大数据的更多内容吗？查看即将在阿姆斯特丹、加利福尼亚和伦敦举行的AI和大数据博览会。这一全面的活动与其他领先活动同时举办，包括智能自动化会议、BlockX、Digital Transformation Week和网络安全与云博览会。
探索由TechForge推出的其他即将举行的企业技术活动和网络研讨会。


---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。