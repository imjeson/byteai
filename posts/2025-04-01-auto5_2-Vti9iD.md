---

title: '我下下决心再给老板发哈哈哈'
date: 2025-04-02
author: ByteAILab

---

编辑 | 萝卜皮

原子结构的高分辨率可视化对于理解材料微观结构与宏观性质之间的关系具有重要意义。然而，在原子分辨率显微镜中，快速、准确、稳健地自动解析复杂模式的方法仍然难以实现。

---


北京大学、厦门大学、中南大学以及深势科技等组成的研究团队，提出了一种基于 Trident 策略增强的解缠结表示学习方法（生成模型）。该方法利用少量未标记的实验图像和大量低成本的模拟图像来生成大量与实验结果非常相似的带注释的模拟数据，从而产生高质量、大容量的训练数据集。

基于残差神经网络的结构推理模型，能精准识别多种材料（如 MoS₂、WS₂ 等）在范德华界面的复杂堆叠（stack）方式，无论是双层还是三层结构，其层间滑移和旋转的测量精度可达皮米级，且抗干扰能力强，不受缺陷、成像噪声或表面污染影响。

该模型不仅能捕捉堆叠模式的渐变过渡，还能区分频域分析难以辨别的莫尔条纹，其高通量特性更揭示了范德华外延中多种热力学稳定构型的共存规律。

该研究以「Auto-resolving the atomic structure at van der Waals interfaces using a generative model」为题，于 2025 年 3 月 25 日发布在《Nature Communications》。

![图片](https://image.jiqizhixin.com/uploads/editor/2d7a8100-e2d4-4506-b426-4dabb8f78164/640.png)

**背景**

二维范德华材料通过层间扭转和滑移可调控莫尔超晶格，从而诱导超导性、铁电性等新颖物性，虽然扫描透射电子显微镜（STEM）能够解析其原子级结构，但传统人工分析方法存在效率低、误差大等瓶颈。

机器学习为此提供了新思路：无监督学习可自动聚类结构特征但解释性差，而监督学习虽精度更高却受限于标注数据——实验数据标注成本高昂，模拟数据则因与真实图像的风格差异导致质量不足，这使得当前方法仅能处理简单的分类任务（如缺陷识别）。

然而，针对连续变化的复杂问题（如皮米级层间位移或转角测量），现有方法仍面临挑战，虽有人尝试通过 CycleGAN 增强模拟数据的真实性，但其图像内容保真度不足。

因此，开发高效、高精度的监督学习框架成为关键突破方向，需在数据质量与规模之间取得平衡，并提升算法对噪声、污染等实际条件的稳健性，最终实现从「识别离散结构」到「求解连续变量」的跨越，完成复杂堆叠模式的自动化定量分析。

在最新的研究中，北京大学等机构的研究人员开发了一种 Trident 策略增强的解缠结表示 (DR) 学习方法，该方法利用一小组未标记的实验 STEM 图像和大量低成本模拟图像来生成一个大型带注释的训练数据集，该数据集与实验图像风格非常相似，并且在风格转换后严格保持模拟图像内容，从而在训练数据的质量和数量之间显示出优越的平衡。

然后利用残差神经网络对这些高质量的模拟图像进行结构推理模型训练，以端到端的方式直接输出多样化复杂堆叠模式的层间滑移和旋转，精度达到皮米级。

**模型概述**

具体来说（下图所示），该框架的第一步是通过解缠结表示图像到图像转换（DRIT）算法训练生成模型，该算法可以生成高质量的 STEM 模拟图像。它是通过将软件生成的低质量、无噪声模拟图像中的结构信息（例如原子的位置、亮度和大小）与实验图像中的视觉风格相结合来实现的。

![图片](https://image.jiqizhixin.com/uploads/editor/e42eb5de-2053-4f6f-8d9e-bab778852bf6/640.png)

图示：机器学习工作流程。（来源：论文）

第二步是定义滑移和扭曲堆叠的结构描述符，以表示所有潜在的堆叠配置，然后在第一步通过训练有素的 DRIT 模型生成逼真的 STEM 模拟图像。由此获得了具有精确标记且与实验图像风格相似度高的大型训练数据集，缓解了因 STEM 实验成本高以及逐个原子手动标记实验图像效率低下而导致的数据稀缺问题。滑移堆叠的描述子是通过沿单层晶胞面内两个基矢量方向分解滑移矢量 D 得到的滑移坐标（Da,Db），而对于扭曲堆叠，则应用层间旋转角 θ。

第三步，以 ResNet-50 架构作为回归网络的主干，训练端到端的堆叠结构识别模型。通过两个 ResNet 模型分别学习堆叠结构标签，即（Da，Db）和 θ，与真实的STEM模拟图像之间的关系，从而能够从实验图像中直接、准确、高效地自动解析范德华界面处的层间滑动和扭曲。

整个工作流程的关键在于 DRIT 模型的训练，它决定着能否将大量、低成本但质量较低的 STEM 模拟图像成功转化为结构信息严格不变、视觉风格与实验图像高度相似的高质量模拟图像，从而为后续的监督学习获取大量的训练数据。

有两点需要深入理解：一是选择 DRIT 算法进行风格转换的原因；二是需要对基本 DRIT 模型进行修改才能更好地完成任务。

**能力评估**

该框架首先被用于解决滑移堆叠范德华双层的原子 registries 问题，这些双层具有旋转排列的顶层和底层（无层间扭曲），但在层间滑动中表现出亚埃级的差异，从而表现出各种物理特性。虽然不同滑移堆叠的结构信息被编码在其复制二维快速傅里叶变换（FFT）中，并且可以通过四维 STEM 布拉格干涉法等先进的衍射技术来解析，但对实空间高分辨率 ADF-STEM 图像进行逐原子分析仍然是最简单、最快捷的识别方法，而且对设备的要求也不高。

实验中观察到双层 ReS2 显示出多样化的滑移堆叠模式，因此研究人员选择该测试案例来评估该框架的四种能力：（i）从原始 ADF-STEM 图像中解析滑移堆叠配置，（ii）定量感知模式的细微结构演变，（iii）准确定位模式转换界面，以及（iv）有效地对大量数据进行统计分析并为创新发现做出贡献。

结果显示，他们的推理模型可以快速计算出滑移矢量坐标，随后将其自动转换为原子模型（底部面板），并根据专家知识和图像模拟验证其正确性。

研究人员使用步长为 0.05 Å 的 DRIT 生成的双层 ReS2 图像作为测试数据集，并使用欧氏距离 ∆D 来评估由步长范围为 0.1 至 0.4 Å（以 0.1 为增量）的 DRIT 生成的图像训练的不同推理模型推断的滑移坐标的准确性。欧氏距离 ∆D 表示如下：

![图片](https://image.jiqizhixin.com/uploads/editor/fa8d7173-d09b-49f4-8c5c-17a583d309ec/640.png)

下图中的箱线图显示，随着步长的增加，平均 ∆D 和对应于中间 95% 数据的 ∆D（箱上限）都在增加，这意味着模型精度下降。考虑到推理模型精度和训练成本之间的平衡，研究人员选择 0.1 Å 的步长来构建模拟图像数据集，得到的平均 ∆D 为 0.03 Å，95% 的推断结果与真实值的偏差小于 0.05 Å，这足以应对实验图像，其空间分辨率为 ～0.7 Å。

![图片](https://image.jiqizhixin.com/uploads/editor/543ef9ec-eed5-4ef6-96e8-3f3c9380d46f/640.png)

图示：滑移堆叠范德华 (vdW) 双层的自动结构分析。（来源：论文）

这个达到皮米级精度框架可以很容易地推广到测量大面积 STEM 图像中微弱的滑移堆叠位移。另外，当实验图像存在一定浓度缺陷或信噪比较低时，该模型的推理精度仍然保持稳健。

ML 框架还可以根据 STEM 图像捕获的莫尔条纹直接解析范德华材料的扭曲角，这对于理解此类超晶格的结构-性质关系至关重要。

![图片](https://image.jiqizhixin.com/uploads/editor/a5cfa6bd-9d64-4b0c-a866-0b0630534adf/640.png)

图示：滑移堆叠分析推理模型的稳健性和普遍性。（来源：论文）

该模型可以很容易地推广到滑移堆叠三层的结构分析，在这种情况下，由于结构复杂性的激增，人类专家只能通过反复试验，就像玩拼图游戏一样，来推断潜在的答案。

**结语**

总而言之，基于 Trident 策略增强的 DR 学习算法，解决了监督学习中的一个关键问题，即如何轻松获得高质量、大量的训练数据。

利用 DRIT 生成的高质量模拟图像训练的结构推断模型可以根据不同堆叠方式（滑移和扭曲）、层数（双层和三层）和成像状态（缺陷率、信噪比、污染）的各种材料的 STEM 图像中的堆叠模式直接、快速、准确地确定范德华界面处的原子尺度结构，并有可能扩展到其他复杂的微观结构分析。

ML 方法的自动化和高通量处理能力引起了范德华外延模式的发现，其中多种热力学有利的滑移堆叠与几乎连续的变化共存，展示了 ML 对知识涌现的贡献。

这项工作扩展了监督学习的能力，从识别离散和简单的微结构到分析复杂且不断变化的主题。机器学习方法在效率、准确性和解决问题的复杂性方面表现出优于人类专家的优势，这可能会彻底改变显微镜图像中原子结构的表征和解释模式，为快速、准确、自动和基于统计的纳米材料信息提取铺平道路。

论文链接：[https://www.nature.com/articles/s41467-025-58160-3](https://www.nature.com/articles/s41467-025-58160-3)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。