---
title: 'Skywork-Reward-V2：引领开源奖励模型的新里程碑'
date: 2025-07-08
author: ByteAILab
---

在2024年9月，Skywork首次开源了Skywork-Reward系列模型及相关数据集。![图片](https://ai-techpark.com/wp-content/uploads/Skywork.jpg){ width=60% }

---
在过去的九个月中，这些模型和数据得到了开源社区的广泛采用，累计在HuggingFace平台上下载超过750,000次，帮助多个前沿模型在RewardBench等权威评估中取得了优秀的结果。  
在2025年7月4日，Skywork继续开源第二代奖励模型——Skywork-Reward-V2系列，包含8个基于不同基础模型、参数范围从6亿到80亿的奖励模型。这些模型在七个主要的主流奖励模型评估基准上取得了最高排名。

**Skywork-Reward-V2 下载链接**  
HuggingFace: [https://huggingface.co/collections/Skywork/skywork-reward-v2-685cc86ce5d9c9e4be500c84](https://huggingface.co/collections/Skywork/skywork-reward-v2-685cc86ce5d9c9e4be500c84)  
GitHub: [https://github.com/SkyworkAI/Skywork-Reward-V2](https://github.com/SkyworkAI/Skywork-Reward-V2)  
技术报告: [https://arxiv.org/abs/2507.01352](https://arxiv.org/abs/2507.01352)  

奖励模型在基于人类反馈的强化学习（RLHF）过程中发挥着至关重要的作用。在开发这一新一代奖励模型时，我们构建了一个名为Skywork-SynPref-40M的混合数据集，包含总计4000万个偏好对。  
为了实现大规模、高效的数据筛选和过滤，Skywork特别设计了一个两阶段的人机协作流程，结合高质量的人类标注与模型的可扩展处理能力。在这一过程中，人类提供经过严格验证的高质量标注，而大型语言模型（LLMs）则基于人类指导自动整理和扩展标记数据。  
基于上述高质量的混合偏好数据，我们开发了Skywork-Reward-V2系列，这一系列模型在多个能力维度上表现出良好的适用性和优秀性能，包括与人类偏好的总体一致性、客观正确性、安全性、抗风格偏见能力及最佳的N比评分能力。实验验证表明，该系列模型在七个主流奖励模型评估基准上取得了最佳性能。

### 01 Skywork-SynPref-40M：人机协作的百万规模人类偏好数据筛选

即使是当前最先进的开源奖励模型，在大多数主流评估基准上仍表现不佳。它们在有效捕捉人类偏好的细致和复杂特征方面特别不足，尤其是在面临多维、多层次反馈时。此外，许多奖励模型往往在特定的基准任务上表现出色，而在新任务或场景中转移时却艰难，表现出明显的“过拟合”现象。虽然现有研究尝试通过优化目标函数、改进模型架构以及新近出现的生成奖励模型来提高性能，但整体有效性仍然相当有限。  
我们认为，当前奖励模型的脆弱性主要源于现有偏好数据集的局限性，这些数据集通常覆盖有限、标注生成方法机械或缺乏严格的质量控制。因此，在开发新一代奖励模型时，我们不仅延续了第一代的数据优化经验，还引入了更为多样化和大规模的真实人类偏好数据，努力在提高数据规模的同时保持数据质量。  
因此，Skywork提出了Skywork-SynPref-40M——迄今为止规模最大的混合偏好数据集，包含总计4000万个偏好样本对。其核心创新在于一个“人机协作、两阶段迭代”的数据选择流程。

**第一阶段：人类指导的小规模高质量偏好构建**  
团队首先构建一个未经验证的初始偏好池，并使用大型语言模型（LLMs）生成与偏好相关的辅助属性，如任务类型、客观性和争议性。在此基础上，人类标注者遵循严格的验证协议，并使用外部工具和先进的LLMs对部分数据进行详细审查，最终构建出一个小规模但高质量的“黄金标准”数据集，作为后续数据生成和模型评估的基础。随后，我们以黄金标准数据的偏好标签作为指导，结合LLM的大规模生成高质量“银标准”数据，从而实现数据量的扩展。团队还进行了多轮迭代优化：在每一轮中，训练奖励模型并根据其在黄金标准数据上的表现识别模型弱点；然后检索相似样本并使用多模型共识机制进行自动标注，以进一步扩展和增强银标准数据。这一人机协作的闭环过程持续进行，不断提高奖励模型对偏好的理解和区分能力。

**第二阶段：全自动大规模偏好数据扩展**  
在获得初步高质量模型后，第二阶段转向自动化的大规模数据扩展。本阶段不再依赖人工审核，而是使用训练好的奖励模型进行一致性过滤：  
- 如果样本的标签与当前最佳模型的预测不一致，或者模型的信心较低，则调用LLMs进行自动重新标注；  
- 如果样本标签与“黄金模型”（即仅基于人类数据训练的模型）预测一致，并得到当前模型或LLM的支持，则可以直接通过筛选。  

通过这一机制，团队从原始4000万个样本中成功筛选出2600万条选定数据点，实现了偏好数据规模与质量的良好平衡，同时大幅减少了人工标注的负担。

### 02 Skywork-Reward-V2：以小模型规模匹配大模型性能

与前一代Skywork-Reward相比，新发布的Skywork-Reward-V2系列提供了8个基于Qwen3和LLaMA3系列模型训练的奖励模型，参数规模涵盖从6亿到80亿。  
在包括Reward Bench v1/v2、PPE Preference & Correctness、RMB、RM-Bench和JudgeBench在内的七个主流奖励模型评估基准上，Skywork-Reward-V2系列全面达到了当前最新的状态（SOTA）水平。

**用数据质量与丰富性弥补模型规模限制**  
即使是最小的模型Skywork-Reward-V2-Qwen3-0.6B，其总体性能几乎与上一代最强的模型Skywork-Reward-Gemma-2-27B-v0.2持平，而最大的模型Skywork-Reward-V2-Llama-3.1-8B在所有主流基准测试中表现全面优越，成为当前性能最佳的开源奖励模型。  

**多维人类偏好能力的广泛覆盖**  
此外，Skywork-Reward-V2在多个高级能力评估中取得了领先的结果，包括Best-of-N (BoN)任务、偏差抵抗能力测试（RM-Bench）、复杂指令理解及真实性判断（RewardBench v2），展现了出色的泛化能力和实用性。  

**高可扩展的数据筛选过程显著提高了奖励模型性能**  
超越评估中的卓越表现，团队还发现，在“人机协作、两阶段迭代”的数据构建过程中，经过仔细筛选和过滤的偏好数据可以通过多轮迭代训练持续有效地提高奖励模型的总体性能，尤其是在完全自动化数据扩展的第二阶段展现出了显著的表现。相反，盲目扩展原始数据不仅未能提升初始性能，还可能引入噪声和负面影响。为了进一步验证数据质量的关键作用，我们对早期版本的1600万数据点进行了实验。结果显示，使用仅占1.8%（约290,000）的高质量数据训练一个80B规模的模型，已经超越了当前70B级别的SOTA奖励模型的性能。这一结果再次确认，Skywork-SynPref数据集不仅在规模上领先，还有显著的数据质量优势。

### 03 欢迎开源奖励模型新里程碑：助力构建未来AI基础设施

在针对第二代奖励模型Skywork-Reward-V2的研究工作中，团队提出了Skywork-SynPref-40M，这个混合数据集包含4000万个偏好对（其中2600万个经过仔细筛选的对），以及Skywork-Reward-V2，这一系列具有广泛任务适用性的状态-of-the-art性能的奖励模型。  
我们相信，这项研究工作和对奖励模型的持续迭代将有助于推动开源奖励模型的发展，更广泛地促进基于人类反馈的强化学习（RLHF）研究的进展。这是该领域向前迈进的重要一步，有助于加速开源社区的繁荣。  
Skywork-Reward-V2系列模型专注于偏好数据的扩展研究。未来，团队的研究范围将逐渐扩展到其他尚未充分探索的领域，如替代训练技术和建模目标。同时，考虑到当前在该领域的发展趋势——奖励模型和奖励塑造机制已成为现今大规模语言模型训练流程的核心组成部分，适用于不仅基于人类偏好学习和行为引导的RLHF，还包括数学、编程或一般推理任务的RLVR，以及基于代理的学习场景。  
因此，我们预计，奖励模型或更广泛的统一奖励系统有望成为未来AI基础设施的核心。它们将不再仅仅作为行为或正确性的评估者，而是成为“指南针”，引导智能系统在复杂环境中航行，帮助它们与人类价值观保持一致，并不断朝着更有意义的目标演化。此外，Skywork于5月发布了全球首个深度研究AI工作空间代理，您可以访问 [skywork.ai](https://skywork.ai) 体验。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。