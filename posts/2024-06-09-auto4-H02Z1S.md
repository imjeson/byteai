---
title: '速递｜前 OpenAI 超级对齐研究员宣布成立 AGI 投资机构！获 AI Grant 与 Stripe 创始人投资'
date: 2024-06-10
author: ByteAILab

---

近日，前 OpenAI 超级对齐研究员 Leopold Aschenbrenner 宣布成立了一家 AGI 投资机构，将其定位为对冲基金与智库的结合体。

据悉，Aschenbrenner 的资金主要来自前 Github CEO Nat Friedman、投资者 Daniel Gross、Stripe CEO Patrick Collision 以及 Stripe 总裁 John Collision。

---


▍从经济学研究到 AI 超级对齐
在加入 OpenAI 前，Aschenbrenner 曾在多个研究机构工作。Aschenbrenner 早年从德国移居美国，年仅 15 岁就进入哥伦比亚大学学习，并于19岁以优秀的成绩毕业，在哥伦比亚大学主修经济学和数学统计学，并担任经济学和政治学部门的研究助理。

2020 年至今，他成为牛津大学全球优先研究所（Global Priorities Institute）的研究员，专注于经济增长和长期风险的研究。

在加入 OpenAI 之前，Aschenbrenner 还在 FTX 创始人 Sam Bankman-Fried 创办的慈善基金 Future Fund 短暂任职，该基金旨在资助“改善人类长期前景”的项目。

2023 年，Aschenbrenner 加入 OpenAI 超对齐团队（Superalignment Team），专注于解决 AI 对齐问题，以确保 AI 系统按照人类的意图行事。

▍因泄露信息遭 OpenAI 解雇
今年，OpenAI 因涉嫌泄露信息解雇了两名员工，分别是 Leopold Aschenbrenner 和 Pavel Izmailov，他们曾在一个专注于 AI 安全的团队工作，这个团队由 Ilya Sutskever 在去年夏天组建，旨在开发控制和引导先进 AI 的技术。

对于解雇的解释，Aschenbrenner 表示他曾撰写了一份内部备忘录，批评 OpenAI 在保护关键算法信息和模型权重方面的能力，之后这份文件最初在他的同事和一些领导之间分发，后来因一次显著的安全事件被提交给董事会。

随后，OpenAI HR 向他发出正式警告，认为他的备忘录是一个重大问题。尽管如此，他仍然坚持推动更好的安全措施。据称，OpenAI 管理层对他决定通知董事会特别不满，随后对安全问题施加了压力。

此外，Aschenbrenner 被解雇的直接原因是他与外部研究人员分享了一份文件，OpenAI 认为该文件包含敏感信息，但 Aschenbrenner 坚持认为文件中没有机密信息，并且分享以获取反馈是公司的常规做法，该文件包括一个 2027～2028 年实现 AGI 时间表，Aschenbrenner 认为这已经是公开信息。

除了 Aschenbrenner，前 OpenAI 安全主管 Jan Leike 近期因对 OpenAI AI 安全方法表示不满而辞职，并转投 Anthropic，他还公开引用了与 OpenAI 管理层在公司核心优先事项上的分歧。

▍2027 年实现 AGI，伴随超智能
Aschenbrenner 将当前关于 AI 的讨论分为两派：一种是完全停止AI发展（如Eliezer Yudkowsky 所倡导的），另一种是全速推进（无视风险），他认为这两种极端立场都是不可行的。

他建议最大限度地加速 AGI 对齐的研究，类似于“曲速行动”（Operation Warp Speed）加速疫苗生产，同时在 AI 实验室的自我监管和独立监控下，通过独立评估（如 ARC 评估）确保安全。

Aschenbrenner 还指出，美国正经历一场前所未有的工业运动，以应对 AGI 需求。数万亿美元将被投入到GPU、数据中心和电力建设上，美国的电力生产预计将在本十年末增长数十个百分点。

他认为，到 2027 年实现 AGI 是非常有可能的。从 GPT-2 到 GPT-4 的进步显示了 AI 能力的巨大跃升，他预计在未来几年内，计算能力和算法效率的持续提升将使AGI 成为现实。

此外，Aschenbrenner 还认为，AI 进步不会停留在与人类相当的水平。数百万的AGI 将能够自动化 AI 研究，可能在一年内压缩十年的算法进步，从而迅速从人类水平发展到超智能。

▍对 AI 安全投入不足
Aschenbrenner 指出，目前美国 AI 实验室在安全方面投入不足，亟需加强防护措施，控制比人类更聪明的 AI 系统是一个尚未解决的技术问题。虽然这个问题是可以解决的，但在快速的智能爆炸期间，可能会出现失控的风险，这将是极其紧张的局面。

他还认为超智能将赋予决定性的经济和军事优势，AGI 将是人类有史以来最强大的武器，与冷战时期的核武器相似，失控的 AGI 可能带来灾难性后果。

此外，他预测到 2027～2028 年，美国将启动某种形式的 AGI 项目，因为仅靠初创公司无法应对超智能的挑战。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。