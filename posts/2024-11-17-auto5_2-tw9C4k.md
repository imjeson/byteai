---
title: '从未见过现实世界数据，MIT在虚拟环境中训练出机器狗，照样能跑酷'
date: 2024-11-18
author: ByteAILab

---

如今，机器人学习最大的瓶颈是缺乏数据。与图片和文字相比，机器人的学习数据非常稀少。

---
目前机器人学科的主流方向是通过扩大真实世界中的数据收集来尝试实现通用具身智能，但是和其他的基础模型，比如初版的 StableDiffusion 相比，即使是 pi 的数据都会少七八个数量级。MIT 的这个团队希望用生成模型来作为机器人学习的新数据源，用工程手段来取代传统的数据收集，实现一条通过由生成模型加持的物理仿真来训练机器人视觉的技术路线。

随着机器人在训练过程中持续进化，进一步提升技能所需的数据也在增长。因此获取足够的数据对于提升机器人的性能至关重要，但在当前实践中，针对新场景和新任务获取数据是一个从头开始不断重复的手动过程。

另一种替代方法则是在模拟环境中训练，从中可以对更多样化的环境条件进行采样，并且机器人可以安全地探索故障案例并直接从它们自己的行为中学习。尽管业界已经在模拟物理和渲染方面投入了大量资金，但目前为实现真实性所做的最佳实践仍与现实存在差距。

一方面渲染真实的图像意味着要制作细致、逼真的场景内容，但大规模手动制作此类内容以获得机器人 sim-to-real（模拟到现实）迁移所需要的多样性，成本过高。另一方面，如果缺少多样化和高质量的场景内容，在模拟环境中训练的机器人在迁移到真实世界时表现得太脆弱。

因此，如何在无限的虚拟环境中匹配现实世界，并将色彩感知融入到 sim-to-real 学习中，这是一个关键挑战。

近日， MIT CSAIL 的研究者开发出了一套解决方案，他们将生成模型作为机器人学习的新数据源，并使用视觉跑酷（visual parkout）作为试验场景，让配备单色相机的机器狗快速攀爬障碍物。

研究者的愿景是完全在生成的虚拟世界中训练机器人，而核心在于找到精确控制语义组成和场景外观的方法，以对齐模拟物理世界，同时保持对于实现 sim-to-real 泛化至关重要的随机性。

![图片](https://image.jiqizhixin.com/uploads/editor/b0353009-d556-4845-873a-02d76f7ac595/640.png)

- arXiv 地址：https://arxiv.org/pdf/2411.00083
- 项目主页：https://lucidsim.github.io/
- 论文标题：Learning Visual Parkour from Generated Images

下图 2 为本文 LucidSim 方法概览：采用了流行的物理引擎 MuJoCo，并在每一帧上渲染了深度图像和语义掩码，这些一起被用作深度条件 ControlNet 的输入。然后从已知的场景几何和相机姿态变化中计算真值密集光流，并在接下来的六个时间步中扭曲原始生成帧以生成时间一致的视频序列。

在学习方面，研究者训练的视觉策略分为两个阶段完成：首先优化策略以模拟从特权教师收集的 rollout 中获得的专家行为。在经过这一预训练步骤后，策略表现不佳。因此，后训练步骤包括从视觉策略本身收集 on-policy 数据，并与当前收集的所有数据的学习交错进行。重复这一步骤三次使得该视觉策略显著提升了自身性能。

研究者表示，该策略足够稳健，在他们的测试场景中可以将零样本转换为真实世界的色彩观察。

![图片](https://image.jiqizhixin.com/uploads/editor/28f2ba8b-41d9-4add-bfb6-d73c5c704f85/640.png)

下面我们来看一段视频展示：[视频链接](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650943282&idx=2&sn=52100ed9622689d5d763cc0aded327e0&chksm=84e7eb4cb390625a40d712259555c98d2cb7a09b8919c3f811e986816b94ebf38a379db14905&token=2009566885&lang=zh_CN#rd)

**LucidSim：利用物理引导生成多样化视觉数据**

研究者考虑了这样一种 sim-to-real 设置，机器人在模拟环境中接受训练，并无需进一步调整就能迁移到现实世界。他们对自己打算部署机器人的环境已经有部分了解，可能是粗略的描述或者参考图像。

由于信息不完整，研究者依赖生成模型内部的先验知识来填补空白。他们将这一引导过程称为先验辅助域生成（Prior -Assisted Domain Generation，PADG），并首先采用对合成不同域至关重要的自动提示技术。

LLM 成为多样化、结构化的提示来源。研究者很早就观察到，从同一提示中重复采样往往会重现类似的图像。因此，为了获得多样化的图像，他们首先使用了包含标题块、查询详情的「元」提示，以提示 ChatGPT 生成批量结构化的图像块，最后以一个要求 JSON 结构化输出的问题结束。

![图片](https://image.jiqizhixin.com/uploads/editor/8f6d4390-537e-4919-a2a8-40497d79d03e/640.png)

研究者的要求包括特定天气、一天中的时间、光照条件和文化遗址。手动编辑生成的图像提示是不切实际的，因而他们通过生成少量图像来调整元提示，并进行迭代直到它们始终可以生成合理的图像。下图 5 下面一行显示了相同元提示、不同图像提示的多样化样本示例。

在几何和物理引导下生成图像。研究者增强了一个原始文本到图像模型，在增加额外语义和几何控制的同时，使它与模拟物理保持一致。他们首先将图像的文本提示替换为提示和语义掩码对，其中每个对应一种资产类型。比如在爬楼梯场景中，研究者通过文本指定了粗略轮廓内台阶的材质和纹理。

为了使图像在几何上保持一致，研究者采用了现成的 ControlNet，该模型使用来自 MiDAS 的单目深度估计进行训练。条件深度图像则通过反转 z 缓冲区并在每一张图像内进行归一化处理来计算。此外，调整控制强度以避免丢失图像细节非常重要。他们采用的场景几何是以往工作中出现的简单地形，包括可选的侧墙。同时避免随机化几何地形以专注视觉多样性分析。

为了制作短视频，研究者开发了 Dreams In Motion（DIM）技术，它根据场景几何计算出的真值光流以及两帧之间机器人相机视角的变化，将生成图像扭曲成后续帧。生成的图像堆栈包含对跑酷至关重要的计时信息。生成速度也很重要，DIM 显著提高了渲染速度，这得益于计算流和应用扭曲要比生成图像快得多。

**通过 on-policy 闭环训练来学习稳健的真实世界视觉策略**

训练过程分为两个阶段：一是预训练阶段，通过模拟有权直接访问高度图的特权专家来引导视觉策略，其中高度图通过 RL 进行训练。研究者从专家及其不完美的早期检查点收集 rollout，并向专家查询动作标签以监督视觉策略。该视觉策略在预训练后表现不佳，但在第二阶段即后训练阶段做出了足够合理的决策来收集 on-policy 数据。

研究者遵循 DAgger，将 on-policy rollout 与上一步中的教师 rollout 相结合。他们从专家教师那里收集了动作标签，并用余弦学习率计划下使用 Adam 优化器运行 70 个梯度下降 epoch。研究者在实验中仅需重复迭代 DAgger 三次就可以实现接近专家表现程度的视觉控制器。实际上第二阶段中的闭环训练过程是机器人出色表现的主要原因。

![图片](https://image.jiqizhixin.com/uploads/editor/682c5d06-f20e-48dd-813c-5042be4b3b07/640.png)

一个简单的 transformer 控制模型架构。研究者提出了一个简单的 transformer 架构，与之前 [extreme parkour](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650892042&idx=3&sn=d09222e36c8fd3098f8afad26dd27015&chksm=84e4a374b3932a62f757f3e33dcb1c531516c7f5358ef881c446e795a8424255ffae98865682&scene=21#wechat_redirect)， 使用 transformer 大大减少了处理多模态输入时控制模型架构的复杂度。

研究者使用了带有多查询注意力的五层 transformer 骨干网络，输入的相机视频被切成小块，并由一个卷积层并行处理。然后，他们将这些 token 与同一时间步的本体感受观察的线性嵌入堆叠在一起。研究者对所有时间步重复此操作，并在 token 级添加了可学习的嵌入。他们发现，对于 RGB 图像，在卷积之前包含批归一化层也有帮助。

最后，研究者通过堆叠在输入序列末尾的额外类 token 来计算动作输出，然后是 ReLU 潜在层和线性映射。

**实验结果**

在实验环节，研究者考虑了以下任务：

- 追踪足球（chase-soccer）；
- 追踪橙色交通锥（chase-cone）；
- 攀爬各种材质的楼梯（stairs）。

他们分别在现实世界和一小部分使用 3D 高斯泼溅来模拟创建的真实世界场景中评估学习到的控制器性能。这些基准环境的示例如下图 9 所示。

![图片](https://image.jiqizhixin.com/uploads/editor/df6ea26a-1165-435a-ad0d-679eeb49da94/640.png)

此外，研究者进行了以下基线比较：

- 需要特权地形数据（障碍）的专家策略；
- 使用相同 pipeline 训练的深度学生策略；
- 使用纹理上经典域随机化训练的 RGB 学生策略；
- 以及本文基于 DIM 生成的帧堆栈进行训练的 LucidSim。

**从生成图像中学习要优于域随机化**

在模拟评估中，研究者观察到 LucidSim 在几乎所有评估中都优于经典域随机化方法。其中，域随机化基线方法能够在模拟中非常高效地爬楼梯，但在跨越障碍任务中表现不佳。这是因为深度学生网络在 3D 场景中遭遇了微妙且常见的 sim-to-real 差距。

比如由于受到栏杆的影响，Oracle 策略在其中一个爬楼梯场景（Marble）中表现不佳，因为它在训练环境中从未见过栏杆。相反，LucidSim 受到的影响较小。

![图片](https://image.jiqizhixin.com/uploads/editor/d9e8fc35-c807-45bc-8008-b9c6d67dd515/640.png)

![图片](https://image.jiqizhixin.com/uploads/editor/a29cfb1f-27d4-466a-9f52-f0c64b9f4508/640.png)

**从零样本迁移到现实世界**

研究者在配备廉价 RGB 网络摄像头的 Unitree Go1 上部署了 LucidSim，在 Jetson AGX Orin 上运行了推理。每个任务都在多种场景中进行评估，并记录了机器人是否追到了目标物（追逐）或成功跨越障碍物。

下图 11 展示了 LucidSim 与域随机化方法的比较结果，其中 LucidSim 不仅能够识别经典的黑白足球，而且由于之前
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。