---

title: 'OpenAI「草莓」模型再次跳票，凌晨发布的SWE-bench Verified是个啥？'
date: 2024-08-15
author: ByteAILab

---

有人说，「我们期待的是草莓，但他们发布的是羽衣甘蓝。」我们来看看这个「羽衣甘蓝」是做什么用的。

---


一直以来，大模型的编程能力都备受关注，超强 AI 程序员 Devin 的问世更是将「AI 能否替代程序员」这一话题推上了风口浪尖。最近，Devin 也迎来了新对手 —— 初创公司 Cosine 推出的自主 AI 程序员 Genie。该公司表示，Genie 的表现轻松超越了 Devin，在第三方基准测试 SWE-bench 上的得分为 30%，而 Devin 的得分仅为 13.8%。

这个 SWE-Bench 是一个用于评估 LLM 解决 GitHub 上真实软件问题能力的基准测试数据集。它收集了来自 12 个流行的 Python 仓库的 2,294 个 Issue-Pull Request 对。在测试时，LLM 会拿到一个代码库和 issue 描述，然后生成一个补丁来解决 issue 描述的问题。这个数据集在 AI 编程能力的评估中已被广泛使用。

在 AI 编程能力进化的同时，这个基准也在进化。今天凌晨，网传的 OpenAI「草莓」模型再次跳票，但 OpenAI 确实发布了新东西，就是 SWE-bench Verified。

OpenAI 指出，原始的 SWE-bench 存在一些问题，可能导致模型的自主软件工程能力被低估。因此，在改进过程中，他们与 SWE-Bench 原作者合作，进行了人工筛选和改进，确保单元测试的范围适当且问题描述明确。

在 SWE-bench Verified 上进行的新测试中，很多 AI 编程智能体的得分都比原来要高。其中，UIUC 的无 Agent 方案 Agentless 甚至实现了得分翻倍，OpenAI 认为，这证明之前的基准确实存在低估 AI 编程能力的缺陷。

但对于蹲守「草莓」的全世界网友来说，这个发布还是过于敷衍了。有人说，「我们期待的是草莓，但他们发布的是羽衣甘蓝。」

**关于 SWE-bench 的背景知识**

SWE-bench 测试集中的每个示例都是根据 GitHub 上 12 个开源 Python 代码库中一个已解决的 GitHub issue 创建的。每个样本都有一个相关的拉取请求（PR），其中包括解决方案代码和用于验证代码正确性的单元测试。这些单元测试被称为 FAIL_TO_PASS 测试，因为在 PR 中的解决方案代码添加之前它们会失败，添加之后则会通过。每个样本还包括 PASS_TO_PASS 测试，这些测试在 PR 合并前后都会通过，用于检查 PR 是否破坏了代码库中与问题无关的其他功能。

在 SWE-bench 中，AI 智能体会获得来自 GitHub issue 的原始文本，即问题陈述，并可以访问代码库。给定这些信息，智能体必须编辑代码库中的文件以解决问题。

AI 智能体给出的编辑将通过运行 FAIL_TO_PASS 和 PASS_TO_PASS 测试来评估。如果 FAIL_TO_PASS 测试通过，这意味着编辑解决了问题。如果 PASS_TO_PASS 测试通过，则意味着编辑没有破坏代码库中无关的部分。要完全解决原始的 GitHub 问题，两组测试都必须通过。

...

参考链接：[OpenAI「草莓」模型再次跳票，凌晨发布的SWE-bench Verified是个啥？](https://openai.com/index/introducing-swe-bench-verified/)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。