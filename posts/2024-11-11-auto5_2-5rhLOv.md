---
title: 'LoRA、完全微调到底有何不同？MIT 21页论文讲明白了'
date: 2024-11-12
author: ByteAILab

---

本文旨在了解两种微调大型语言模型方法之间的差异：完全微调和低秩自适应 (LoRA)。这两种方法都用于将预训练模型适应特定的下游任务，但它们却有所不同。

---


微调（Fine-tuning）是将经过预训练的大语言模型应用于下游任务的关键范例。最近，低秩自适应 (LoRA) 等方法已被证明可以在各种任务上达到完全微调模型的性能，同时可训练参数的数量却大大减少。

这就提出一个问题，即它们学到的解决方案真的等效吗？

带着这一疑问，来自 MIT 的研究者在论文《 LORA VS FULL FINE-TUNING: AN ILLUSION OF EQUIVALENCE 》中进行了深入探讨。

论文地址：[https://arxiv.org/pdf/2410.21228v1](https://arxiv.org/pdf/2410.21228v1)

作者通过分析预训练模型权重矩阵的光谱特性来研究不同的微调方法如何改变模型。

研究发现，完全微调与 LoRA 产生的权重矩阵奇异值分解结构有显著不同，并且经过微调后的模型在面对超出适应任务分布的测试时也显示出不同的泛化行为。

特别是，LoRA 训练的权重矩阵中出现了称为「侵入维度（intruder dimensions）」的新的高秩奇异向量，而在完全微调中则不会出现这种情况。

这些结果表明，即使在微调分布上表现相同，但使用 LoRA 和完全微调更新的模型访问参数空间的不同部分。

作者通过研究 LoRA 微调模型中出现侵入维度的原因、它们为什么不受欢迎，以及如何最小化这些效果来展开研究。

最后，作者给出了以下几点观察：

首先，LoRA 和完全微调在结构上产生不同的参数更新，这种差异由侵入维度的存在产生的。这些侵入维度是奇异向量，具有较大的奇异值，并且与预训练权重矩阵中的奇异向量近似正交。相比之下，完全微调模型在光谱上与预训练模型保持相似，不包含侵入维度。

其次， 从行为上看，与完全微调相比，具有侵入维度的 LoRA 微调模型会忘记更多的预训练分布，并且表现出较差的稳健连续学习能力：具有侵入维度的 LoRA 微调模型在适应任务分布之外不如完全微调模型，尽管分布准确度相当。

最后， 即使在目标任务上低秩 LoRA 表现良好，但更高秩的参数化可能仍然是可取的。低秩 LoRA（r ≤ 8）适合下游任务分布，完全微调和高秩 LoRA（r = 64）让模型泛化能力更强、自适应能力更加鲁棒。然而，为了利用更高的秩，LoRA 更新模型必须是秩稳定的。

沃顿商学院副教授 Ethan Mollick 对此评论道：事实证明，使用 LoRA 定制通用 LLM（Apple 调优其设备内置模型的方式），对 LLM 的限制远大于微调，因为它们失去了一些泛化能力。原因是 LoRA 增加了不祥的侵入维度。

LoRA 和完全微调模型的差异

本文采用神经网络参数的奇异值分解 SVD 来理解微调对预训练权值的变化。

特别是，本文测量了用 LoRA 微调过的权重矩阵中的奇异向量或完全微调过的权重矩阵中奇异向量映射到预训练权重中的奇异向量的程度，使用它们的余弦相似性。这些关系如图 1 和图 3 所示，颜色表示预训练和微调奇异向量之间的余弦相似度。

图 2 (b) 中观察到，LoRA 和完全微调的奇异向量与预训练奇异向量的相似度非常不同：与完全微调相比，使用 LoRA 微调的模型的奇异向量与预训练奇异向量的平均余弦相似度似乎要低得多。

图 2 (b) 中左下角有一个唯一的红点，作者将这些新维度命名为侵入维度，其正式定义如下：

LoRA 微调模型包含高秩侵入维度，而完全微调的模型则不包含。为了量化特定权重矩阵的侵入维度集的大小，作者使用图 4 所示的算法。

即使在 LoRA 微调模型学习效果不如完全微调的任务中，侵入维度也存在。

观察图 5b、5c 和 5d，我们可以清楚地看到，即使 LoRA 的 r=256，高秩奇异向量集中仍出现侵入维度。重要的是，当 r=2048 时没有侵入维度，而是展示了与完全微调非常相似的曲线。这支持了早先的发现：随着秩增加超过一个阈值，侵入维度会消失，LoRA 开始趋向于与完全微调相似。

了解更多内容，请参考原论文。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。