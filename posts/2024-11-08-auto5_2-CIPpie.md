---
title: '无问芯穹提出混合稀疏注意力方案MoA，加速长文本生成，实现最高8倍吞吐率提升'
date: 2024-11-09
author: ByteAILab

---

随着大语言模型在长文本场景下的需求不断涌现，其核心的注意力机制（Attention Mechanism）也获得了非常多的关注。注意力机制会计算一定跨度内输入文本（令牌，Token）之间的交互，从而实现对上下文的理解。

---
随着应用的发展，高效处理更长输入的需求也随之增长，这带来了计算代价的挑战：注意力高昂的计算成本和不断增长的键值缓存（KV-Cache）代价。稀疏注意力机制可以有效缓解内存和吞吐量的挑战。

然而，现有稀疏注意力通常采用统一的稀疏注意力模式，即对不同的注意力头和输入长度应用相同的稀疏模式。这种统一的方法难以捕捉到大语言模型中多样的注意力模式，导致不同注意力头的不同的精度 - 代价权衡被忽略。

最近，来自清华大学、无问芯穹和上海交通大学的研究团队发表了《MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression》，提出通过混合不同稀疏度的注意力头，使用 25% 的注意力稠密度，就可以记忆几乎 100% 的上下文。

本工作现已开源，欢迎交流讨论。

代码：https://github.com/thu-nics/MoA
主页：https://nics-effalg.com/MoA
arXiv：https://arxiv.org/abs/2406.14909

总览

在大语言模型中，不同的注意力头表现出各异的注意力模式和扩展规则：有的关注全局信息，有的则聚焦局部；有的注意力范围随输入长度增加而扩展，有的则保持不变。然而，现有的统一稀疏注意力机制破坏了这些固有的特性。

为应对这一挑战，研究团队提出了混合稀疏注意力（Mixture of Sparse Attention, MoA）方法，它能够为不同的头和层定制独特的稀疏注意力配置。MoA 构建了一个包含多种注意力模式及其扩展规则的搜索空间。通过分析模型，评估潜在配置，MoA 可以为每个注意力头找到最优的稀疏注意力模式和扩展规则。

实验结果显示，无需任何训练，MoA 就可以在保持平均注意力跨度不变的情况下，将有效...
...
---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。