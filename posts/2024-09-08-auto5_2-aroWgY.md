---
title: '大模型边推理边纠错，有可能做到吗？这是ICML爆火的演讲'
date: 2024-09-09
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

即便是最强大的语言模型（LLM），仍会偶尔出现推理错误。除了通过提示词让模型进行不太可靠的多轮自我纠错外，有没有更系统的方法解决这一问题呢？

来自 Meta FAIR、CMU 和 MBZUAI 的叶添、徐子诚、李远志、朱泽园团队在最新的 arXiv 论文《语言模型物理学 Part 2.2：如何从错误中学习》中，通过可控实验，探索了让模型**「边推理边纠错」**的可能性。

他们在预训练中加入大量「错误的推理」和「错误的纠正」，展示了这类数据可以提高语言模型的推理准确性（无需提示词，无需多轮对话）。文章还深入探讨了许多细节，例如（1）这种方法与 beam search 的区别，（2）如何准备此类数据，（3）是否需要对错误进行掩码，（4）所需的错误数量，（5）此类数据是否可用于微调等。

作者首先展示了一个 GPT-4o 通过提示词和多轮对话进行纠错的示例（图 2），可以看到成功率不高，而且需要很长的对话才能完成纠错。那么，如果模型最终能纠错，为什么不在第一次犯错时「立即收回并改正」呢？

为此，作者使用探针（probing）方法研究模型的内部工作机制。通过 Part 2.1 建立的 iGSM 数据集，作者发现当模型犯错后，内部参数常常表现出「很后悔」的状态，也就是说，模型可能已经知道自己犯了错，但「覆水难收」。

那么，**能否简单地让模型「后悔即重试（retry upon regret）」**？即，通过额外训练（如微调）得到一个检测错误的模型，只要该模型判定当前步骤有错，就立即退格回到上一步骤的末尾，再重新生成呢？

如图 3 所示，作者进行了横向对比。即便错误识别率超过 99%，这种重试方法在 iGSM 数据集上也只能将推理正确率提高 2%（虽然比 beam search 好）。作者总结了此方法的三个不足。

首先，对正确率提高有限，毕竟退格后，模型依然是随机生成，并没有用高级的方法改错。其次，对错误识别率的要求很高（同等条件下，需要 100% 错误识别率才能将推理正确率提高 8%，但这太不现实）。最重要的是，这并不能降低模型生成文本的时间复杂度，因为依然需要一次次地重新生成。

接下来，作者更换方法，在预训练数据中**加入大量的错误和纠正，例如「A=&gt;B，哦我说错了，应该是 A=&gt;C」**。那么，这能否提升模型的推理正确率呢？乍一看，这似乎不合理，因为增加错误的同时，模型岂不是被迫学习说错误的话（即 A=&gt;B）？是否需要将错误部分（譬如「A=&gt;B，哦我说错了，应该是」这几个字）通过掩码（label masking）从训练标签中删除？

答案是不需要。依然通过 iGSM 数据集，作者用控制变量法，横向对比了诸多参数后得出若干结论（图 4）。

例如，即便预训练数据中的每道题目有 50% 的步骤包含错误，模型在测试阶段并不会刻意犯错（如使用 temp=0 生成时）。背后的原因与语言模型对语法的纠错能力有关，具体可参见作者的另一篇 Part 1 论文，因此不需要对错误进行掩码。更神奇的是，在合理范围内，训练集里的错误其实越多越好，例如包含 50% 错误的数据，比 10% 错误的数据在 iGSM 数据集上还能再提升推理正确率 4 个百分点。

接下来，作者研究了包含**「错误和纠正」的数据能否作为微调数据使用**。这是个重要问题，因为现有的开源大模型可能并不具备很好的纠错能力。如果我们制备了完美的错误纠正数据集，能否通过少量参数微调（如使用 LoRA 方法）让现有模型学会纠错？

答案是否定的。如图 5 所示，作者尝试了多种 LoRA 参数，发现最多只能将推理正确率从 78% 提高到 83%—— 甚至在大多数情况下，如 LoRA 的 rank 较小时，模型的正确率远低于 78%。这说明**「纠正错误」是一个高级能力**，与模型的正常推理不同，需要大量参数变化才能实现。（这也合理，毕竟如果修改少量参数就能完成纠错，那么让模型「后悔即重试（图 3）」恐怕早就能提高推理正确率了。）

相对而言，**「错误识别」并不是高级能力**，可以通过微量的 LoRA 微调学会。此外，通过 beam search 模型也能进行一定程度的重试，但对正确率的提升几乎为零。综合以上，作者认为，如果能制备优质的「错误和纠正」数据，应将此类数据放入预训练数据集中，而不是等到微调时再使用。

最后，作者研究了在实际生活中如何制备「错误和纠正」数据。目前为止，文章都在 iGSM 数据集上进行可控实验，由于此数据集中的数学题满足统一格式，可以随意删减拼接，制作无限量的错误和纠正数据。这太理想化了。现实生活中，有没有办法在**不要求理解题目的基础上生成一些「假错误」**？

作者对此做了一些初步尝试。例如，通过将解题步骤中靠后的第 Y 步骤挪到前面作为第 X 步的假错误，然后用原本的第 X 步作为纠正。这一方法在 iGSM 数据集上也能显著提升正确率（从 78% 到 91%），如图 6 所示。

据此，作者大胆预测，尽管未来的 LLM 可能不会直接在 iGSM 数据上进行训练，但本文通过可控的对比试验，研究了在通向 AGI 的道路上，我们需要对数据进行哪些修改和制备。

例如，利用像 Llama3-405B 这样的模型来改写数学题，在正确的解题步骤中插入许多错误 —— 甚至是简单的假错误，也有望改变模型的答题方式。让模型**「边推理边纠错」**，而不是通过额外的提示词被动纠错，或许是一个新的思路。作者限于 GPU 限制，无法对如此方向做真实数据的大规模研究，但欢迎读者沿着这一思路试试看。

最后，这篇 arXiv 论文是《语言模型物理学》系列作品中的 Part 2.2。此系列目前共 6 篇论文，在 ICML 2024 大会上做了 2 小时的演讲，收获诸多好评（图 7）。
有兴趣了解整个系列作品的小伙伴，可以移步 https://www.bilibili.com/video/BV1Yw4m1k7nH

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。