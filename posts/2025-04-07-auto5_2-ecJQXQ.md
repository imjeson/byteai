---

title: '反向传播、前向传播都不要，这种无梯度学习方法是Hinton想要的吗？'
date: 2025-04-08
author: ByteAILab

---

Noprop：没有反向传播或前向传播，也能训练神经网络。

「我们应该抛弃反向传播并重新开始。

---
」早在几年前，使反向传播成为深度学习核心技术之一的 Geoffrey Hinton 就发表过这样一个观点。

![image.png](https://image.jiqizhixin.com/uploads/editor/51727d01-cabe-45ea-b8c0-3041d2f58f24/640.png)

而一直对反向传播持怀疑态度的也是 Hinton。因为这种方法既不符合生物学机理，与大规模模型的并行性也不兼容。所以，Hinton 等人一直在寻找替代反向传播的新方法，比如 2022 年的前向 - 前向算法。但由于性能、可泛化性等方面仍然存在问题，这一方向的探索一直没有太大起色。

最近，来自牛津大学和 Mila 实验室的研究者向这一问题发起了挑战。他们开发了一种名为 NoProp 的新型学习方法，该方法既不依赖前向传播也不依赖反向传播。相反，NoProp 从扩散和流匹配（flow matching）方法中汲取灵感，每一层独立地学习对噪声目标进行去噪。

论文标题：NOPROP: TRAINING NEURAL NETWORKS WITHOUT BACK-PROPAGATION OR FORWARD-PROPAGATION  
论文链接：[链接](https://arxiv.org/pdf/2503.24322v1)

研究人员认为这项工作迈出了引入一种新型无梯度学习方法的第一步。这种方法不学习分层表示 —— 至少不是通常意义上的分层表示。NoProp 需要预先将每一层的表示固定为目标的带噪声版本，学习一个局部去噪过程，然后可以在推理时利用这一过程。

他们在 MNIST、CIFAR-10 和 CIFAR-100 图像分类基准测试上展示了该方法的有效性。研究结果表明，NoProp 是一种可行的学习算法，与其他现有的无反向传播方法相比，它实现了更高的准确率，更易于使用且计算效率更高。通过摆脱传统的基于梯度的学习范式，NoProp 改变了网络内部的贡献分配（credit assignment）方式，实现了更高效的分布式学习，并可能影响学习过程的其他特性。

在看了论文之后，有人表示，「NoProp 用独立的、无梯度的、基于去噪的层训练取代了传统的反向传播，以实现高效且非层次化的贡献分配。这是一项具有开创性意义的工作，可能会对分布式学习系统产生重大影响，因为它从根本上改变了贡献分配机制。」

其数学公式中涉及每层特定的噪声模型和优化目标，这使得无需梯度链即可进行独立学习。其优势在于通过让每一层独立地对一个固定的噪声目标进行去噪，从而绕过了反向传播中基于顺序梯度的贡献分配方式。这种方式能够实现更高效、可并行化的更新，避免了梯度消失等问题，尽管它并未构建传统的层次化表示。

![image.png](https://image.jiqizhixin.com/uploads/editor/d82dfbac-e6c6-4e06-8c85-fc950afd85e3/640.png)

还有人表示，「我在查看扩散模型架构时也产生过这样的想法…… 然而，我认为这可能是一种非最优的方法，所以它现在表现得如此出色让我感到很神秘。显而易见的是其并行化优势。」

![image.png](https://image.jiqizhixin.com/uploads/editor/4008cdf6-2ada-4806-aeda-0ac6a3a85f64/640.png)

### 为什么要寻找反向传播的替代方案？

反向传播虽是训练神经网络的主流方法，但研究人员一直在寻找替代方案，原因有三：

1. 生物学合理性不足：反向传播需要前向传递和后向传递严格交替，与生物神经系统运作方式不符。
2. 内存消耗大：必须存储中间激活值以计算梯度，造成显著内存开销。
3. 并行计算受限：梯度的顺序传播限制了并行处理能力，影响大规模分布式学习，并导致学习过程中的干扰和灾难性遗忘问题。

目前为止，反向传播的替代优化方法包括：
- 无梯度方法：如直接搜索方法和基于模型的方法
- 零阶梯度方法：使用有限差分近似梯度
- 进化策略
- 基于局部损失的方法：如差异目标传播（difference target propagation）和前向 - 前向算法

但这些方法因在准确性、计算效率、可靠性和可扩展性方面的限制，尚未在神经网络学习中广泛应用。

### 方法解析

#### NoProp

设 x 和 y 是分类数据集中的一个输入 - 标签样本对，假设从数据分布 q₀(x,y) 中抽取，z₀,z₁,...,zₜ ∈ Rᵈ 是神经网络中 T 个模块的对应随机中间激活值，目标是训练该网络以估计 q₀(y|x)。

定义两个分布 p 和 q，按以下方式分解：

![image.png](https://image.jiqizhixin.com/uploads/editor/123e5c59-653d-4d04-a871-64cca4b7c380/640.png)

p 分布可以被解释为一个随机前向传播过程，它迭代地计算下一个激活值 zₜ，给定前一个激活值 zₜ₋₁ 和输入 x。实际上，可以看到它可以被明确表示为一个添加了高斯噪声的残差网络：

![image.png](https://image.jiqizhixin.com/uploads/editor/80066aa7-c880-4cca-bf9d-6eaee1aa8b37/640.png)

其中 Nᵈ(・|0,1) 是一个 d 维高斯密度函数，均值向量为 0，协方差矩阵为单位矩阵，aₜ,bₜ,cₜ 是标量（如下所示），bₜzₜ₋₁ 是一个加权跳跃连接，而 ûθₜ(zₜ₋₁,x) 是由参数 θₜ 参数化的残差块。注意，这种计算结构不同于标准深度神经网络，后者没有从输入 x 到每个模块的直接连接。遵循变分扩散模型方法，也可以将 p 解释为给定 x 条件下 y 的条件隐变量模型，其中 zₜ 是一系列隐变量。可以使用变分公式学习前向过程 p，其中 q 分布作为变分后验。关注的目标是 ELBO，这是对数似然 log p (y|x)（即证据）的下界：

![image.png](https://image.jiqizhixin.com/uploads/editor/6ef5904e-d37a-4fb4-9e75-ccfe62c99762/640.png)

遵循 Sohl-Dickstein 和 Kingma 等人的方法，将变分后验 q 固定为一个易于处理的高斯分布。在这里使用方差保持的 Ornstein-Uhlenbeck 过程：

![image.png](https://image.jiqizhixin.com/uploads/editor/b5963337-f8bb-4660-82a5-e52ebde90948/640.png)

其中 uᵧ 是类别标签 y 在 Rᵈ 中的嵌入，由可训练的嵌入矩阵 W (Embed) ∈ Rᵐˣᵈ 定义，m 是类别数量。嵌入由 uᵧ = {W (Embed)}ᵧ 给出。利用高斯分布的标准性质，我们可以得到：

![image.png](https://image.jiqizhixin.com/uploads/editor/10a1b1ad-c36e-43ea-94f7-9bae6c019cd4/640.png)

其中 ᾱₜ = ∏ₛ₌ₜᵀαₛ，μₜ(zₜ₋₁,uᵧ) = aₜuᵧ + bₜzₜ₋₁，aₜ = √(ᾱₜ(1-αₜ₋₁))/(1-ᾱₜ₋₁)，bₜ = √(αₜ₋₁(1-ᾱₜ))/(1-ᾱₜ₋₁)，以及 cₜ = (1-ᾱₜ)(1-αₜ₋₁)/(1-ᾱₜ₋₁)。为了优化 ELBO，将 p 参数化以匹配 q 的形式：

![image.png](https://image.jiqizhixin.com/uploads/editor/b735da69-b0f7-4bdf-8fb1-8501d7c59430/640.png)

其中 p (z₀) 被选为 Ornstein-Uhlenbeck 过程的平稳分布，ûθₜ(zₜ₋₁,x) 是由参数 θₜ 参数化的神经网络模块。给定 zₜ₋₁ 和 x 对 zₜ 进行采样的结果计算如残差架构（方程 3）所示，其中 aₜ,bₜ,cₜ 如上所述。最后，将此参数化代入 ELBO（方程 4）并简化，得到 NoProp 目标函数：

![image.png](https://image.jiqizhixin.com/uploads/editor/6ef5904e-d37a-4fb4-9e75-ccfe62c99762/640.png)

其中 SNR (t) = ᾱₜ/(1-ᾱₜ) 是信噪比，η 是一个超参数，U {1,T} 是在整数 1,...,T 上的均匀分布。我们看到每个 ûθₜ(zₜ₋₁,x) 都被训练为直接预测 uᵧ，给定 zₜ₋₁ 和 x，使用 L2 损失，而 p̂θout (y|zₜ) 被训练为最小化交叉熵损失。每个模块 ûθₜ(zₜ₋₁,x) 都是独立训练的，这是在没有通过网络进行前向或反向传播的情况下实现的。

### 实现细节

NoProp 架构如图 1 所示。

![image.png](https://image.jiqizhixin.com/uploads/editor/bd66200c-734d-4353-b48f-376491002c6f/640.png)

在推理阶段，NoProp 架构从高斯噪声 z₀开始，通过一系列扩散步骤转换潜变量。每个步骤中，潜变量 zₜ通过扩散动态块 uₜ演化，形成序列 z₁→z₂→...→zₜ，其中每个 uₜ都以前一状态 zₜ₋₁和输入图像 x 为条件。最终，zₜ通过线性层和 softmax 函数映射为预测标签ŷ。

训练时，各时间步骤被采样，每个扩散块 uₜ独立训练，同时线性层和嵌入矩阵与扩散块共同优化以防止类别嵌入崩溃。对于流匹配变体，uₜ表示 ODE 动态，标签预测通过寻找与 zₜ在欧几里得距离上最接近的类别嵌入获得。

训练所用的模型如图 6 所示，其中左边为离散时间情况的模型，右边为连续时间情况的模型。

![image.png](https://image.jiqizhixin.com/uploads/editor/2661be0c-7525-4ae9-aa73-ff0394cd842d/640.png)

作者在三种情况下构建了相似但有区别的神经网络模型：
1. 离散时间扩散：神经网络 ûθt 将图像 x 和潜变量 zt−1 通过不同嵌入路径处理后合并。图像用卷积模块处理，潜变量根据维度匹配情况用卷积或全连接网络处理。合并后的表示通过全连接层产生 logits，应用 softmax 后得到类别嵌入上的概率分布，最终输出为类别嵌入的加权和。
2. 连续时间扩散：在离散模型基础上增加时间戳 t 作为输入，使用位置嵌入编码并与其他特征合并，整体结构与离散情况相似。
3. 流匹配：架构与连续时间扩散相同，但不应用 softmax 限制，允许 v̂θ 表示嵌入空间中的任意方向，而非仅限于类别嵌入的 convex combination。

所有模型均使用线性层加 softmax 来参数化相应方程中的条件概率分布。

对于离散时间扩散，作者使用固定余弦噪声调度。对于连续时间扩散，作者将噪声调度与模型共同训练。

### 实验结果

作者对 NoProp 方法进行了评估，分别在离散时间设置下与反向传播方法进行比较，在连续时间设置下与伴随敏感性方法（adjoint sensitivity method）进行比较，场景是图像分类任务。

结果如表 1 所示，表明 NoProp-DT 在离散时间设置下在 MNIST、CIFAR-10 和 CIFAR-100 数据集上的性能与反向传播方法相当，甚至更好。此外，NoProp-DT 在性能上优于以往的无反向传播方法，包括 Forward-Forward 算法、Difference Target 传播以及一种称为 Local Greedy Forward Gradient Activity-Perturbed 的前向梯度方法。虽然这些方法使用了不同的架构，并且不像 NoProp 那样显式地对图像输入进行条件约束 —— 这使得直接比较变得困难 —— 但 NoProp 具有不依赖前向传播的独特优势。

![image.png](https://image.jiqizhixin.com/uploads/editor/88c64a56-ff38-4881-850c-5f01ff399b9d/640.png)

此外，如表 2 所示，NoProp 在训练过程中减少了 GPU 内存消耗。

![image.png](https://image.jiqizhixin.com/uploads/editor/cd18f848-8afd-46a6-a2c6-16728c682991/640.png)

为了说明学习到的类别嵌入，图 2 可视化了 CIFAR-10 数据集中类别嵌入的初始化和最终学习结果，其中嵌入维度与图像维度匹配。

![image.png](https://image.jiqizhixin.com/uploads/editor/0b6b3f42-77e4-4144-bc91-739b0932be38/640.png)

在连续时间设置下，NoProp-CT 和 NoProp-FM 的准确率低于 NoProp-DT，这可能是由于它们对时间变量 t 的额外条件约束。然而，它们在 CIFAR-10 和 CIFAR-100 数据集上通常优于伴随敏感性方法，无论是在准确率还是计算效率方面。虽然伴随方法在 MNIST 数据集上达到了与 NoProp-CT 和 NoProp-FM 相似的准确率，但其训练速度明显较慢，如图 3 所示。

![image.png](https://image.jiqizhixin.com/uploads/editor/88c64a56-ff38-4881-850c-5f01ff399b9d/640.png)

对于 CIFAR-100 数据集，当使用 one-hot 编码时，NoProp-FM 无法有效学习，导致准确率提升非常缓慢。相比之下，NoProp-CT 仍然优于伴随方法。然而，一旦类别嵌入与模型联合学习，NoProp-FM 的性能显著提高。

作者还对类别概率的参数化和类别嵌入矩阵 W_Embed 的初始化进行了消融研究，结果分别如图 4 和图 5 所示。消融结果表明，类别概率的参数化方法之间没有一致的优势，性能因数据集而异。对于类别嵌入的初始化，正交初始化和原型初始化通常与随机初始化相当，甚至优于随机初始化。

![image.png](https://image.jiqizhixin.com/uploads/editor/88c64a56-ff38-4881-850c-5f01ff399b9d/640.png)
![image.png](https://image.jiqizhixin.com/uploads/editor/88c64a56-ff38-4881-850c-5f01ff399b9d/640.png)

更多详细内容请参见原论文。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。