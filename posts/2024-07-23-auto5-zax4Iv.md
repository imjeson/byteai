---
title: '中科大联合华为诺亚提出Entropy Law，揭秘大模型性能、数据压缩率以及训练损失关系'
date: 2024-07-24
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com
数据是大语言模型（LLMs）成功的基石，但并非所有数据都有益于模型学习。直觉上，高质量的样本在教授 LLM 上预期会有更好的效率。因此，现有方法通常专注于基于质量的数据选择。然而，这些方法中的大多数独立地评估不同的数据样本，忽略了样本之间复杂的组合效应。如图 1 所示，即使每个样本质量完美，由于它们的互信息冗余或不一致性，它们的组合可能仍然次优。尽管基于质量的子集由所有三个优质样本组成，但它们编码的知识实际上是冗余和冲突的。相比之下，另一个由几个相对较低质量但多样化的样本组成的数据子集在教授 LLM 方面可能传达更多信息。因此，基于质量的数据选择并未完全符合最大化 LLM 知识掌握的目标。

本文旨在揭示 LLM 性能与数据选择之间的内在关系。受 LLM 信息压缩本质的启发，我们发现了一条 entropy law，它将 LLM 性能与数据压缩率和前几步模型训练的损失加以联系，分别反映了数据集的信息冗余程度和 LLM 对数据集中固有知识的掌握程度。通过理论推导和实证评估，我们发现模型性能与训练数据的压缩率呈负相关，而这通常会产生较低的训练损失。基于 entropy law 的发现，我们提出了一种非常高效且通用的数据选择方法用于训练 LLM，名为 ZIP，其旨在优先选择低压缩率的数据子集。ZIP 分多阶段、贪心地选择多样化的数据，最终获得一个具有良好多样性的数据子集。

团队：中科大认知智能全国重点实验室陈恩红团队，华为诺亚方舟实验室
论文链接: https://arxiv.org/pdf/2407.06645
代码链接: https://github.com/USTC-StarTeam/ZIP

Entropy law
我们对数据压缩与 LLM 性能之间的关系进行理论分析。直觉上，训练数据的正确性和多样性会影响最终模型的性能。同时，如果数据存在严重的内在冲突...

ZIP：高度轻量化的数据选择算法
在 entropy law 的指导下，我们提出了 ZIP 这一数据选择方法，通过数据压缩率来选择数据样本，旨在在有限的训练数据预算下最大化有效信息量。出于效率考量...

实验结果
1.ZIP 选择算法对于不同 LLM、在不同 LLM 对齐阶段的有效性
对比不同的 SFT 数据选择算法，基于 ZIP 选择数据所训练得到的模型性能上展现出优势，并且在效率上也占优...

2.Entropy law 的实验验证
基于 SFT 数据选择实验，我们基于模型效果、数据压缩率以及模型在前几步训练的损失，分别拟合了多条关系曲线...

3.Entropy law 的实际应用
我们提供了一个 entropy law 在真实场景中指导 LLM 训练数据增量更新的应用。在该任务场景中，训练数据量保持相对稳定...

---
。注意：Title、Date、Body 三个部分的内容，放入到对应的位置。最后只需要按照格式标准输出为Makedown源文件格式内容。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。