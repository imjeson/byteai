---
title: 'Mixtral 8x22B sets new benchmark for open models'
date: 2024-04-20
author: Jeson

---

Mistral AI发布了Mixtral 8x22B，为开源模型的性能和效率设立了新的标杆。![图片](https://www.artificialintelligence-news.com/wp-content/uploads/sites/9/2024/04/mistral-mixtral-8x22b-open-source-ai-model-artificial-intelligence.jpg){ width=50% }

---
该模型拥有强大的多语言能力、卓越的数学和编码能力。
Mixtral 8x22B作为一种稀疏专家混合（SMoE）模型运行，仅在活动时利用其1410亿参数中的390亿。
除了其效率，Mixtral 8x22B还擅长多种主要语言，包括英语、法语、意大利语、德语和西班牙语。其熟练程度延伸到技术领域，具有强大的数学和编码能力。值得注意的是，该模型支持原生函数调用结合“约束输出模式”，有助于大规模应用开发和技术升级。

带有64K令牌上下文窗口的Mixtral 8x22B确保从大量文档中精确地获取信息，进一步吸引了处理大型数据集的企业级利用，处理大量数据集通常是例行任务。
为了促进协作和创新的AI研究环境，Mistral AI已将Mixtral 8x22B发布在Apache 2.0许可下。这个高度宽松的开源许可证确保了没有限制的使用，并促进了广泛的采用。
从统计数据来看，Mixtral 8x22B超越了许多现有模型。在标准产业基准测试中进行头对头比较-从常识、推理到特定主题知识- Mistral的新创新在各种语言环境下的关键推理和知识基准测试中表现出色。Mistral AI公布的数字显示，Mixtral 8x22B在通用语言语境中明显优于LLaMA 2 70B模型：

此外，在编码和数学领域，Mixtral继续在开源模型中保持优势。更新的结果显示，这个模型的指导版本在数学基准测试中表现出色：
鼓励潜在用户和开发人员在Mistral AI的互动平台La Plateforme上探索Mixtral 8x22B。在这里，他们可以直接与模型互动。

在AI的作用不断扩大的时代，Mixtral 8x22B在高性能、效率和开放性方面的融合标志着先进AI工具民主化进程中的一个重要里程碑。

![Photo by Joshua Golde](Photo by Joshua Golde)

查看更多：SAS旨在通过打包的AI模型使AI无论技能水平如何都能获得

想要从行业领袖那里了解更多关于AI和大数据的知识吗？请查看在阿姆斯特丹、加利福尼亚和伦敦举办的AI＆Big Data Expo。这一全面的活动与其他领先活动如BlockX、Digital Transformation Week和Cyber Security＆Cloud Expo同时举办。

探索由TechForge提供的其他即将举行的企业技术活动和网络研讨会。

tags: 8x22b, ai, artificial intelligence, development, mistral ai, mixtral, Model, open source
---
