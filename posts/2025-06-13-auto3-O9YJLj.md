---

title: 'KAYTUS推出升级版MotusAI，加速LLM部署'
date: 2025-06-14
author: ByteAILab

---

KAYTUS，一家领先的端到端AI和液冷解决方案提供商，今天在2025年ISC高性能大会上宣布发布其最新版本的MotusAI AI DevOps平台。![图片](https://ai-techpark.com/wp-content/uploads/KAYTUS-1.jpg){ width=60% }

---
升级后的MotusAI平台在大型模型推理性能方面提供了显著增强，并支持多个开源工具的广泛兼容性，覆盖大型模型的全生命周期。MotusAI旨在实现统一和动态的资源调度，极大地提高了大规模AI模型开发和部署中的资源利用率和操作效率。此次MotusAI的最新发布将在教育、金融、能源、汽车和制造等关键领域进一步加速AI的采用并推动业务创新。

随着大型AI模型越来越多地嵌入到实际应用中，企业正在大规模部署它们，以在广泛的行业中创造切实价值。然而，许多组织在AI采用过程中仍面临关键挑战，包括较长的部署周期、严格的稳定性要求、零散的开源工具管理以及较低的计算资源利用率。为了解决这些痛点，KAYTUS推出了最新版本的MotusAI AI DevOps平台，旨在简化AI部署，增强系统稳定性，并优化大规模模型操作的AI基础设施效率。

增强的推理性能以确保服务质量
部署AI推理服务是一项复杂的任务，涉及服务的部署、管理和持续健康监控。这些任务需要在模型和服务治理、高性能调优和长期服务稳定性等方面遵循严格的标准，这通常需要大量的人员、时间和技术专 expertise。

升级后的MotusAI提供强大的大模型部署能力，确保可见性和性能的完美结合。通过集成优化框架，如SGLang和vLLM，MotusAI确保企业可以快速并自信地部署高性能的分布式推理服务。MotusAI设计用于支持大参数模型，利用智能资源和网络亲和调度来加速上线时间，同时最大化硬件利用率。其内置监控能力覆盖全栈——从硬件和平台到Pod和服务——提供自动故障诊断和快速服务恢复。MotusAI还支持基于实时使用和资源监控的推理工作负载动态扩展，提供增强的服务稳定性。

全面的工具支持以加速AI采用
随着AI模型技术的快速发展，支持生态系统的开发工具愈加复杂。开发者需要一个简化的通用平台来高效选择、部署和操作这些工具。

升级后的MotusAI提供对大量领先开源工具的广泛支持，使企业用户能够按需配置和管理其模型开发环境。内置的工具如LabelStudio加速数据标注和不同类别之间的同步，提高数据处理效率，加速模型开发周期。MotusAI还为整个AI模型生命周期提供了一体化工具链。这包括用于数据标注和治理的LabelStudio和OpenRefine、用于大型模型微调的LLaMA-Factory、用于大型模型应用开发的Dify和Confluence以及用于文本到图像生成的Stable Diffusion。这些工具共同赋能用户快速采用大型模型并提升开发生产力。

混合训练-推理调度在同一节点上最大化资源效率
有效利用计算资源仍然是AI初创公司和中小型企业在AI采纳早期阶段的重要优先事项。传统的AI集群通常为训练和推理任务分别分配计算节点，限制了在两种工作负载之间的灵活性和资源调度的效率。

升级后的MotusAI通过实现混合调度，使训练和推理工作负载能够在单个节点上进行，克服了传统的限制，让多样任务类型的无缝集成和动态编排成为可能。配备先进的GPU调度能力，MotusAI支持按需资源分配，使用户能够根据工作负载需求有效管理GPU资源。MotusAI还具有多维GPU调度功能，包括细粒度分区和对多实例GPU（MIG）的支持，满足模型开发、调试和推理等广泛使用场景。

MotusAI的增强调度器显著超越了基于社区的版本，在大规模POD部署中实现了任务吞吐量提高5倍和延迟降低5倍。它实现了数百个POD的快速启动和环境准备，同时支持训练和推理的动态工作负载扩展和潮汐调度。这些能力为各种现实AI场景下的无缝任务编排提供了支持。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。