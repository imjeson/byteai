---
title: '现在的大模型榜单，真就没一个可信的'
date: 2024-09-11
author: ByteAILab

---

文章来源：数字生命卡兹克

![图片来源：由GPTNB生成](http://www.jesonc.com/upload/8FD7B96F5E34993C64020C0DB54F4C00/1725955154588/FvuWhfBmRNhu85lFvZWFn37XU9X5.jpg)

现在的大模型榜单上，真的都是水分。
全是作弊的考生，真的。

---

上周，AI圈有个很炸裂的大模型发布，在全网引起了山呼海啸，一众从业者和媒体尊称它为开源新王。
就是 Reflection 70B。

![Reflection 70B](http://www.jesonc.com/Fk6ndy1ELm8AVjm3sb8s2YtM_UFs)

在每项基准测试上都超过了 GPT-4o，还只用70B的参数，就击败了405B的Llama 3.1，模型中还有一个叫「Reflection-Tuning」的技术，能让模型能够在最终回复之前，先识别自己有没有错误，如果有，纠正以后再回答。

![Reflection-Tuning](http://www.jesonc.com/Frk2WT9910DEm4H0FlQhQAQu9CQh)

其实这个东西当时我就很存疑，因为在我的理解里，这玩意，就是个CoT，就是个纯Prompt，一个Prompt把70B模型直接带的螺旋升天？
你这玩意，真要是能做到，奥特曼就真的直接原地给你磕头了。。。

最关键的是，还有一个很离谱的点，这个模型就两个人做，而且，从一拍即合、到找数据集、到模型微调完成并正式发布，一共就花了3周。
这效率，这速度，直接卷的螺旋升天，国内大厂速度没卷到这个地步...

直到昨天，发现这模型底都快被人扒掉了。
模型结果造假，提供给开发者的API，还是造假。

先是跑分评测上面，这是他们老板Matt自己发出来的跑分结果，勇夺第一。

![Matt跑分结果](http://www.jesonc.com/Fmq-olWneaiLtvP4eb66gtBu9gT5)

看这个结果，你就说屌不屌吧，拳打Claude3.5，脚踢GPT-4o，还把Gemini1.5 Pro和Llama3.1 405B给摁在地上摩擦。
你很难想象这只是一个两个人花三周训的70B的模型能干出来的事。

直到7号，Artificial Analysis用他们自己的标准评测集跑了一通，发现这事不对啊，你这么多项评测集都登顶了，你应该很牛逼才对啊，这得分什么情况？？？

![Artificial Analysis评测结果](http://www.jesonc.com/Fi2hgdlajznfWQS48gTym_65k-qC)

他们是这么说的：

“哥们，我们测完了咋感觉你比Llama3.1 70B更拉了呢？老实说，你是不是在骗兄弟们。”

Matt看到了以后，开始说卧槽不对劲啊，我们内部是好的啊，怎么你们测试结果这么烂？

![内部结果正常](http://www.jesonc.com/FtnjfOAoxnTFDbxTerwcENypJZiV)

花了好半天，Matt终于说，哦是 Hugging Face 权重出现了问题，我也不知道咋回事，你们等一等。

说完还不忘凡尔赛一下，说：

![乞求等待](http://www.jesonc.com/FvocEPLQxL_lWRmspuItddl7OPgr)

翻译一下就是：我们是在太太太太火啦，你们再等等啊，乖。

直到今天凌晨，最骚的事情来了，Matt说，我们终于解决了问题，开放了新的API。

![新的API](http://www.jesonc.com/FlvaiVwFkWA9ehY1twco73BD5AC3)

他们提供了一个私有接口，说这个才是Reflection 70B完全体。

大家一测，卧槽，果然牛逼，牛逼炸了。

真的好像比GPT4o还有那些大厂的模型强哎。

就差点直接给Reflection 70B开香槟了。

2个人，3周时间，创了AI行业的奇迹。

但是大家香槟刚开一半，就被生生的摁回去了。

大家发现，这个所谓的“Reflection 70B”的API，怎么跟Claude3.5回复的东西，一模一样。。。

![API回复](http://www.jesonc.com/Fu0BBtU5UxM--femWn46ifdeoKfb)

于是有人，又做了一个验证测试，他把所有API的参数全部设为10个Token、0温度、top_k 1，然后让大模型，重复entsprechend这个词20次，因为大模型对token的计算都不太一样，所以其实10个token限制输出的内容也不太一样，你既然说你是基于Llama3.1微调的，那你肯定得跟Llama3.1输出内容一样对吧。

但是，结果直接让人大跌眼镜。

![验证测试结果](http://www.jesonc.com/FmuMLQSzh1OhB82ixRE-bQLiuZ0R)

好兄弟，你怎么，跟Claude的长度一样，你到底是Llama3.1生出的Reflection 70B，还是披着狗皮的Claude啊？？？

更狗的是，他们居然，还把Claude设成了屏蔽词，在用户的对话中，一旦你发Claude，就直接剔除。

![屏蔽Claude](http://www.jesonc.com/FjVCsVkT7ibhCtLdq4B-NXH_Mrlf)

骚啊，实在是太骚了。。。

这下，所有AI行业的人，都知道，Reflection 70B就是一场彻头彻尾的闹剧。

这个闹剧背后，我觉得反应出了现在整个AI领域，一个非常诡异的现象。

刷榜。
回到整个事情的起点，就是模型能力的强度，和榜单。
正是因为Reflection 70B在评测集上屠榜了，秒杀Claude3.5和GPT4o，才让大家如此兴奋。

但是结果大家发现，卧槽你怎么就做那些特定评测集的题目那么牛逼？换个别的题就直接变废物了？不是细狗你行不行啊？
直接对着答案抄，还不行，那不扯淡吗。

大模型目前的评测体系，从来就不复杂，就是考试，纯纯的考试。
评测数据集相当于试卷，模型就是正在考试的学生，最后交卷，看谁的分高。
听...
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。