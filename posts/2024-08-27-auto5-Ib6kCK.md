---
title: 'RTX3090可跑，360AI团队开源最新视频模型FancyVideo，红衣大叔都说好'
date: 2024-08-28
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

论文作者之一 Ao Ma，硕士毕业于中科院计算所，曾在 MSRA 视觉计算组和阿里通义实验室进行学术研究和算法落地工作。目前是奇虎 360-AIGC 团队-视频生成方向负责人，长期致力于视觉生成方向研究和落地，以及开源社区建设。

近日，开源社区又迎来一款强力的「视频生成」工作，可以在消费级显卡 (如 GeForce RTX 3090) 上生成任意分辨率、任意宽高比、不同风格、不同运动幅度的视频，其衍生模型还能够完成视频扩展、视频回溯的功能…… 这便是 360AI 团队和中山大学联合研发的 FancyVideo，一种基于 UNet 架构的视频生成模型。

作者基于已经开源的 61 帧模型，实测效果如下。

首先适配不同分辨率、宽高比：[图片链接](https://mp.weixin.qq.com/s/_Njlo7D1YogSpr8nK_p_Jg)其次支持不同风格： [图片链接](https://mp.weixin.qq.com/s/_Njlo7D1YogSpr8nK_p_Jg)最后生成不同运动性： [图片链接](https://mp.weixin.qq.com/s/_Njlo7D1YogSpr8nK_p_Jg)

- 论文地址：https://arxiv.org/abs/2408.08189
- 项目主页：https://fancyvideo.github.io/
- 代码仓库：https://github.com/360CVGroup/FancyVideo
- 论文标题：FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance

**跨帧文本引导模块**

作者在进行视频生成研究过程中，发现现有的文本到视频（T2V）工作通常会采用空间交叉注意力（Spatial Cross Attention），将文本等价地引导至不同帧的生成过程中，缺乏对不同帧灵活性的文本引导（如下图左）。这会导致模型理解提示词所传达的时间逻辑和生成具有连续运动视频的能力受到限制。

FancyVideo 正是从这一角度切入，特殊设计了跨帧文本引导模块（Cross-frame Textual Guidance Module, CTGM, 如下图右）改进了现有文本控制机制。

具体来说，CTGM 包含 3 个子模块：

- 时序信息注入器（Temporal Information Injector, TII）-- 将来自潜在特征的帧特定信息注入文本条件中，从而获得跨帧文本条件；
- 时序特征提取器（Temporal Affinity Refiner, TAR）-- 沿时间维度细化跨帧文本条件与潜在特征之间的相关矩阵；
- 时序特征增强器（Temporal Feature Booster, TFB）-- 增强了潜在特征的时间一致性。

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKLHz8BxpxoIuwr5sjhcsJaobKgfNnzcGAMm5vOs1y5vQ0NSHibbLVRWMg/640?wx_fmt=png&from=appmsg)

**FancyVideo 训练流程**

FancyVideo 整体训练 Pipeline 如下所示。其中在模型结构方面，FancyVideo 选择在 2D T2I 模型基础上插入时序层和基于 CTGM 的运动性模块的方式构造 T2V 模型。在生成视频时，先进行 T2I 操作生成首帧，再进行 I2V。这既保存了 T2I 模型的能力，使视频整体画质变高，又大大减少了训练代价。

此外，为实现运动控制的能力，FancyVideo 在训练阶段将基于 RAFT 提取视频运动信息和 time embedding 一起注入到网络中。

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKLZ3ljcA60xkIz8LgYfG53TKc8bRzbMqZtjhH9Iso9XCSnaRYGwo6Z1w/640?wx_fmt=png&from=appmsg)

**实验结果**

作者通过定量和定性两个方面对模型效果进行评估。他们首先在 EvalCrafter Benchmark 上比较了 FancyVideo 和其他 T2V 模型，可以看到 FancyVideo 在视频生成质量、文本一致性、运动性和时序一致性方面均处于领先位置。

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKLFbKHkicbSyYVGLU4bSy0PXBZZPDgkSLWXdno1HYMtzibkwwG1uFwIjtg/640?wx_fmt=png&from=appmsg)

论文还在 UCF-101 和 MSR-VTT Benchmark 上进行了 Zero-shot 的评测，在衡量生成视频丰富性的 IS 指标和文本一致性的 CLIPSIM 指标均取得了 SOTA 结果。

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKLxlZ3dFhbxpllamDVuJA8szJoeWm69EKDjian1kPaF6FqrIkCJcCsYuQ/640?wx_fmt=png&from=appmsg)

...

**应用场景**

基于这种训练 pipline 和策略，FancyVideo 可以同时完成 T2V 和 I2V 功能，还可以在生成关键关键帧的基础上进行插帧操作：

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKL6vX4ic31qqH3lIQyPuXeBCK7Myibz9iblENqhhCiczZC1bkK07O3icwX3rw/640?wx_fmt=png)

视频扩展、视频回溯操作：

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKLYa2vyRdhs0QVqOGHqvR3kwpzAMbE1bAVGn2TFpyUxV0SOibic29LzVCQ/640?wx_fmt=png)

FancyVideo 上线开源社区不到一周，已经有手快的同学自发搭建了 FancyVideo 的 ComfyUI 插件，让大家可以在自己的机器上玩的开心。

[图片链接](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW91tNQoFtwaOuGvpvjhnoKLib3f4AjTI8BVWorguevVM21TdoU2xC9V1JcsOACsLqvtQ0LU425w0jA/640?wx_fmt=png)

此外，据作者了解，后续 FancyVideo 团队除了会放出更长、效果更好的模型到开源社区，还计划上线网页版本供大家【免费】使用。在 AIGC 时代，人人都是「能诗会画」的艺术家。

**结论**

相比于 SORA 类视频生成「产品」的发展，开源社区中视频生成模型的更新和迭代显得略微缓慢，FancyVideo 的发布也给了普通用户更多选择。相信在社区小伙伴共同的努力下，视频生成这一目前看上去费时费力的任务，能够成为更多普通小伙伴日常生活、工作中的工具。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。