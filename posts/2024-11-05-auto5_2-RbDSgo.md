---
title: 'LLM超越人类时该如何对齐？谷歌用新RLHF框架解决了这个问题'
date: 2024-11-06
author: ByteAILab

---

让 LLM 在自我进化时也能保持对齐。

我们这个世界是不断变化的开放世界。

---
人工智能要在这个世界长久立足，就需要突破许多限制，包括可用数据和规模和质量以及有用新信息的增长率。

对基于 LLM 的 AI 来说，高质量的人类数据非常关键，但已有研究预计这些高质量数据将在未来几年耗尽。
如果 LLM 保持现在的发展势头，预计在 2028 年（中位数）左右，已有的数据储量将被全部利用完，来自论文《Will we run out of data? Limits of LLM scaling based on human-generated data》

此后，这类数据的质量也将停滞不前：随着 LLM 能力越来越强，它们将能解决越来越复杂和越来越多的难题，而这些难题所需的训练数据已经超出了人类的能力。

因此，我们就需要为 LLM 构建一种能使其实现自我提升的基本机制，让模型可以持续地自我生成和自我求解更困难的问题。

于是，问题就来了：语言模型能否自我创建可学习的新任务，从而实现自我改进以更好地泛化用于人类偏好对齐？

为了提升语言模型的对齐能力，人们已经提出了许多偏好优化算法，但它们都默认使用固定的提示词训练分布。这种固定的训练范式缺乏可扩展性，并不可避免地导致泛化问题和效率问题。

基于这些考虑，谷歌 DeepMind 和芝加哥大学一个研究团队开发了**一种可扩展的开放式 RLHF 框架 eva，即 Evolving Alignment via Asymmetric Self-Play，也就是「通过非对称自博弈实现的演进式对齐」**。

eva 能让自我提升式语言模型的训练分布自动演进，如图 1 所示。

**eva 的核心方法**

在介绍 eva 的核心方法之前，我们需要先了解一些前提设置，这里截图如下：

概述地讲，eva 可通过一个创建器（creator）将经典 RLHF 扩展成开放式 RLHF，该创建器使用易于实现的估计、采样、进化程序来调整提示词的分布，模仿不对称自博弈的最小最大遗憾（minimax-regret）策略。

...
---

实际的算法

下面说明如何实际实现算法 1 中的 eva。

1. 创建器步骤：估计，采样，然后演进

显然，创建器会找到最有用的提示词并生成它们的变体，并将这些变体用于偏好优化。创建器的实现分为 3 步。

- 第 1 步：info (・)—— 估计信息量。对于提示集 X) t 中的每个 x，生成响应、注释奖励并通过式估计 x 的信息量指标。
- 第 2 步：sample (・)—— 对富含信息的子集进行加权采样。使用信息量指标作为权重，对富含信息的提示词子集 X^info_t 进行采样，以便稍后执行演进。
- 第 3 步：evolve (・)—— 为高优势提示词执行近端区域演进。具体来说，迭代 X^info_t 中的每个提示词，让它们各自都演化为多个变体，然后（可选）将新生成的提示词与对 X_t 的均匀采样的缓存混合以创建 X′_t。

2. 求解器步骤：求解然后优化

此步骤是经典的偏好优化，其中生成响应并执行梯度下降。以逐点奖励模型设置为例，对于每个提示，采样 n 个响应，每个响应都带有奖励注释；这里采用最大和最小奖励的响应来构建偏好对，然后进行优化。

总之，eva 可以使用新的创建器模块统一现有的迭代优化工作流程，该模块可以与求解器策略共享相同的网络，也可独立运行。

实验结果

这里我们仅关注实验的主要结果，实验设置请参看原论文。

总体而言，eva 在对齐方面取得了显著的进步，同时无需依赖任何人工数据，因此更具效率。
是基础设置，即一次迭代微调后的模型，eva 则会在此基础上添加一个创建器，以实现初始迭代的提示词集的自我演进，并使用一个偏好优化算法进行额外的开放式 RLHF 迭代，这会得到。

eva 能实现自我提升

如表 1 红色标记所示，eva 在不同优化算法中的表现显著优于基础设置，尤其是在更难的 Arena-Hard 基准上，该基准由于其提示词的复杂性和更公平的评分系统而被认为更具挑战性。

具体来说，eva 使用 SimPO 作为求解器时增益为 8.4%，使用 DPO 作为求解器时增益为 8.5%，超越了其 27B 版本并与 Arena-Hard 排行榜上报告的 claude-3-opus-240229 相当，同时还使用了全自动的提示词生成进行对齐。

eva 可以超越人工编写的提示词

实验进一步表明，使用 eva 提示词训练的模型的表现能够比肩甚至超越那些使用了来自 UltraFeedback 的额外新提示词训练的模型，这可被视为是人类提示词。同时，前者还能做到成本更低，速度更快。

此外，在 MT-Bench 上，使用新的人类提示词进行训练通常会在第一轮中表现出性能下降，在第二轮中也只会有适度的提升。相比之下，eva 能显著提高第二轮的表现。

针对此现象，该团队给出了自己的假设：eva 可演化出全新的可学习的提示词，并且其中包含第二轮问题的特征，这表明 eva 涌现出了处理后续互动等新技能。

消融研究

为了验证 eva 各组件的有效性，该团队也执行了消融研究，下面我们简单给出其发现，详细实验过程请访问原论文：

- 信息量指标：新提出的基于后悔值的指标优于其它替代指标；
- 采样之后执行演化的流程：新方法优于贪婪选择方法；
- 使用奖励模型进行扩展：eva 的对齐增益会随奖励模型而扩展；
- 持续训练：新提出的方法可通过增量训练获得单调增益；eva 演化得到的数据和调度可用作隐式正则化器，从而实现更好的局部最小值。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。