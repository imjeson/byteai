---
title: '当AI取代真相，大模型如何一步步诱骗了人类的文明？'
date: 2024-11-02
author: ByteAILab

---

如今，人们选择餐厅，多半会打开app搜索一番，再看看排名。然而美国奥斯汀的一家餐厅Ethos的存在证实这种选择机制多么不可靠。

---
Ethos在社交媒体instagram宣称是当地排名第一的餐厅，拥有7万余粉丝。

实际上，这家看起来很靠谱的餐厅根本不存在，食物和场地的照片都由人工智能生成。可它家发布在社媒上的帖子却得到了数千名不知情者的点赞和评论。大模型通过视觉形式误导公众认知，引发了人们对其潜在影响的深刻思考。

![图片来源：由GPTNB生成](http://www.jesonc.com/upload/8FD7B96F5E34993C64020C0DB54F4C00/1730426781892/Fv7s-fZCEso4Hn0rqWiwTr9Jj2f_.png)

**AI生成的虚假信息影响深远**

类似开篇提到的虚假餐厅的例子，现实生活中发生了不止一次。2023年11月，搜索引擎Bing曾因为爬取了错误信息，而针对“澳大利亚是否存在”的问题，给出了荒谬的回复。

![Bing截图；图源：Bing](http://www.jesonc.com/Fs-7l6ur6wevh_E-jy7wd1TY0Wi6)

上面的例子，还可视为程序的bug，而普林斯顿的一项研究则系统性说明了AI生成数据的影响不止如此。该研究发现，截止24年8月，至少5%的英文维基百科页面是由AI生成的，相对而言德语、法语和意大利语文章的占比较低。

维基百科（Wikipedia）是人工智能训练数据的重要来源，且被普遍视为权威的信息来源。尽管AI生成的内容并不一定都是虚假信息，但该研究指出，被标记为AI生成的维基百科文章通常质量较低，并具有较明显的目的性，往往是自我推广或对有争议话题持特定观点。

**AI生成的虚假信息利用判断真假的启发式弱点**

虽然虚假信息被发现后很快会被纠正，但如同小时候听过的“狼来了”的故事，一次次的接触虚假信息，会让磨损我们彼此间的信任。

我们判断一件事情是否为真时，有两种不同的思考方式，一是启发式，另一种则是费力的思考。启发式思维所需的认知资源更少，依赖直觉，属于丹尼尔·卡尼曼所说的系统一。对个体来说，判断是否是虚假信息，启发式的判断标准包括是否声明清晰，是否吞吐犹豫，是否有熟悉感；而费力的思考则多基于逻辑：“不应该只相信互联网来源，我是否在学校或是书本中见过类似的？”

![大模型会如何利用人类事实监控机制的弱点，图源：参考文献1](http://www.jesonc.com/FuDJ5GbU3Ij7Gmvoa-sutkz1XT5P)

在日常生活中，我们常用到两种启发式方法判断信息真实与否：一种是观察发言是否流畅自信，另一种是言论是否熟悉。然而事实证明，这两种判断基准在人工智能面前都会败下阵来。

然而，大模型生成的文章，往往会显得自信且流畅。一项研究对比人工智能生成的和人类撰写的大学入学论文，发现人工智能生成的论文与来自特权背景的男性学生的论文相似。人工智能论文倾向于使用更长的词汇，并且在写作风格上与私立学校申请人的论文尤其相似，相比真实的申请论文缺乏多样性。这意味着当我们面对大模型生成的信息时，之前用来判断真假的第一种启发式——“表述是否自信”——失效了。

而对于第二个判断机制——是否熟悉，由于大模型产生信息的速度远远大于人类，它可以通过高度重复同质化的信息，人工创造出一种真实感。当大模型不加区分地向“思想市场”输出真伪参半的同质化信息时，判断信息真伪的第二个启发式机制“熟悉与否”也失效了。

---

继续阅读完整内容，请访问[原文链接](https://www.aixinzhijie.com/article/6847118)。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。