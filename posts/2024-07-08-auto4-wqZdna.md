---

title: '大模型训练成本飙升，千亿模型将亮相'
date: 2024-07-09
author: ByteAILab

---

Anthropic 首席执行官 Dario Amodei 在In Good Company 播客中表示目前正在开发的人工智能模型的训练成本高达 10 亿美元。目前的模型（如 ChatGPT-4o）仅花费约 1 亿美元，但他预计，训练这些模型的成本将在短短三年内上升至 100 亿美元甚至 1000 亿美元。

---


“目前是 1 亿。如今训练的模型数量可能超过 10 亿。” Amodei 还补充道，“我认为如果我们达到 100 亿或 1000 亿，我认为这将在 2025 年、2026 年或 2027 年实现，并且算法改进和芯片改进将继续保持一定速度，那么我认为到那时我们很有可能能够获得在大多数事情上都比大多数人类更好的模型。”

Anthropic 首席执行官在讨论人工智能从生成式人工智能（如 ChatGPT）发展到通用人工智能 (AGI) 时提到了这些数字。他说，我们不会突然达到通用人工智能。相反，这将是一个渐进的发展过程，模型建立在过去模型的发展之上，就像人类儿童的学习方式一样。

因此，如果 AI 模型每年的性能提高十倍，我们可以合理地预期训练它们所需的硬件的性能也会至少提高十倍。因此，硬件可能是 AI 训练中最大的成本驱动因素。早在 2023 年，就有报道称ChatGPT 需要超过 30,000 个 GPU，Sam Altman 证实 ChatGPT-4 的训练成本为 1 亿美元。

去年，有超过 380 万个 GPU 被交付到数据中心。鉴于 Nvidia 最新的 B200 AI 芯片售价约为 30,000 至 40,000 美元，我们可以推测 Dario 的十亿美元估值将在 2024 年实现。如果模型/量化研究的进步以当前的指数级增长，那么我们预计硬件需求将保持同步，除非像搜狐 AI 芯片这样的更高效的技术变得更加普及。

我们已经看到这种指数级增长正在发生。Elon Musk希望购买 30 万块 B200 AI 芯片，而据报道，OpenAI 和微软正在计划一个价值 1000 亿美元的 AI 数据中心。有了这么多的需求，如果 Nvidia 和其他供应商能够跟上市场步伐，我们明年的 GPU 数据中心交付量可能会激增至 3800 万。

然而，除了实际芯片硬件的供应，这些人工智能公司还需要关注电力供应和相关基础设施。仅去年一年售出的所有数据中心 GPU 的总耗电量就足以为 130 万户家庭供电。如果数据中心的电力需求继续呈指数级增长，那么我们可能会耗尽足够的经济实惠的电力。此外，虽然这些数据中心需要发电厂，但它们还需要一个完全升级的电网，以处理耗电的人工智能芯片运行所需的所有电子。出于这个原因，包括微软在内的许多科技公司现在正在考虑为其数据中心配备模块化核电。

人工智能正在迅速发展，硬件创新似乎也在跟进。因此，Anthropic 的 1000 亿美元预测似乎符合预期，尤其是如果 Nvidia、AMD 和 Intel 等制造商能够实现这一目标的话。然而，随着我们的人工智能技术每一代都以指数级的速度进步，一个大问题仍然存在：它将如何影响我们社会的未来？

参考链接：

https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-models-that-cost-dollar1-billion-to-train-are-in-development-dollar100-billion-models-coming-soon-largest-current-models-take-only-dollar100-million-to-train-anthropic-ceo

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。