---
title: 'Anthropic安全负责人：在超级AI「毁灭」人类之前，我们可以做这些准备'
date: 2024-09-10
author: ByteAILab

---

2023 年，Anthropic 发布了负责任扩展策略（Responsible Scaling Policy，RSP），这是一系列技术和组织协议，Anthropic 将采用这些协议来帮助他们管理开发功能日益强大的 AI 系统。
![图片](https://image.jiqizhixin.com/uploads/editor/1d19d966-15c5-46bb-9fd7-259e06ed9907/640.png)
Anthropic 认为，AI 模型一方面变得越来越强大，创造巨大的经济和社会价值，另一方面也带来了严重的风险。

---
RSP 将专注于灾难性风险 —— 即人工智能模型直接造成大规模破坏的风险。此类风险可能来自故意滥用模型（例如恐怖分子用它来制造生物武器），也可能来自模型以违背其设计者意图的方式自主行动而造成破坏。

RSP 还定义了一个称为 AI 安全等级 (ASL，AI Safety Levels) 的框架，ASL 等级越高，其安全性证明就越严格。
![图片](https://image.jiqizhixin.com/uploads/editor/4ae4d718-3a4e-41f8-9b1c-96ca074ab613/640.png)
- ASL-1 指的是不构成重大灾难风险的系统，例如 2018 年的 LLM 或只会下棋的 AI 系统。
- ASL-2 指的是显示出危险能力早期迹象的系统（例如能够给出如何制造生物武器的指令），但这些信息由于可靠性不足或无法超越搜索引擎能提供的信息而没有太多用处。包括 Claude 在内的当前 LLM 似乎是 ASL-2。
- ASL-3 指的是与非 AI 基线（例如搜索引擎或教科书）相比，大大增加了灾难性滥用风险的系统或显示出低级自主能力的系统。
- ASL-4 及更高版本（ASL-5+）尚未定义，因为它与现有系统相差太远，但可能会涉及灾难性滥用潜力和自主性的质的升级。

一直以来，Anthropic 在为 AI 安全做着各种努力，「要做哪些技术工作才能使非常强大的人工智能的开发顺利进行？」近日，Anthropic 安全研究部门的负责人 Sam Bowman 在一篇博客中分享了他的观点。
![图片](https://image.jiqizhixin.com/uploads/editor/763de0c4-7120-4514-8c02-c7fe8b3f0383/640.png)

在开始讨论超级人工智能的风险之前，我有一些前提需要声明：
人工智能有望达到与人类相当的水平。这个阶段，我称之为变革性人工智能（TAI）。TAI 将有能力在所有适合远程工作的职业中替代人类，包括 AI 研发。
TAI 并不是人工智能能力的上限，未来可能会出现远超人类能力的系统，它们将对世界产生深远影响。在未来十年内，我们很有可能见证 TAI 的诞生，而那时的商业、政策和文化背景预计与当前相比不会有太大变化。
TAI 一旦实现，它将极大地加速人工智能的研发进程，可能在 TAI 出现后的几个月或几年内，我们就能看到...

[阅读原文](https://sleepinyourhat.github.io/checklist/)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。