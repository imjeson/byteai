---

title: 'Meta又给OpenAI一记重击，视频生成Movie Gen震撼登场，甚至可以配音、编辑'
date: 2024-10-06
author: ByteAILab

---

视频生成领域真是越来越卷且越来越迈向实用性！

在 OpenAI Sora 难产的时候，Meta 首次公开展示了自家的「用于媒体的突破性生成式 AI 研究」：Meta Movie Gen。

Meta 在相应博客中使用了「premiere」一词，也就是初次展示，因此手痒的用户可能还得再等上一段时间。

---


Meta 表示：「无论是希望在好莱坞大展身手的电影制作人，还是爱好为观众制作视频的创作者，我们相信每个人都应该有机会使用有助于提高创造力的工具。」

根据 Meta 的描述，Movie Gen 的功能包括：文本生成视频和音频、编辑已有视频、图片生视频。并且人类评估表明，Movie Gen 在这些任务上的表现均优于行业内类似模型。

具体的效果如何，我们先来看几个示例。

可以看到，小女孩在奔跑的过程中衣服的褶皱就已经吊打很多视频生成应用了。

在转头、正视前方、微笑的几个动作中，人物面部依然可以保持稳定状态，克服了形变。怪不得 Meta 在博客中还谈到，这是能够用来分享日常生活的可贵技术。

生成动物对 Movie Gen 来说也是小菜一碟。动物的毛发、动作都十分逼真。仔细看这只猴子的尾巴，在遮挡后依然能够遵循遮挡前的运动轨迹。背景生成结果也十分贴合 prompt。水面的波动、倒映都栩栩如生。不过水下折射的生成效果看起来还有些进步空间。

视频时间长一些，Movie Gen 也能有稳定的表现。人物大幅度动作的展现也比较逼真。但每一帧定格下来，还会有些瑕疵。不过这是视频生成一贯的难题，Meta 或许会在未来进行改进。

Sora 刚刚问世时，往往还需要 Elevenlabs 这样的音频生成应用来辅助。而 Movie Gen 显然更加便捷，除了视频生成，配备相应的音乐、音效也是拿手好戏。

面对一整个视频的场景，合适的背景音乐能够贯穿全程。不仅如此，音效的适配度也很高。这个瀑布倾泻的水声就十分逼真。

更让人惊讶的是，音效还能够精准地与视频内容匹配。它能够通过视频中的动作节点来把握音效出现的时机，让画面和声音相辅相成，给我们呈现出完整的生成效果。

如果 Meta 所言非虚，那么 Movie Gen 也真算得上是目前最先进和最沉浸式的「讲故事模型套件（storytelling suite of models）」。

Meta 表示训练使用的数据集都是公开数据集或已获得授权的数据集。下面将简要介绍各项能力以及背后的技术，更多详情请参阅原论文。

Meta 在博客中简单回顾了自己的视频生成之旅。他们的第一波生成式 AI 研究始于 Make-A-Scene 系列模型，这些模型可以生成图像、音频、视频和 3D 动画。

随着扩散模型的出现，他们又基于 Llama 基础模型做出了第二波研究，成功实现了更高质量的图像和视频生成以及图像编辑。

Movie Gen 则是 Meta 的第三波研究。他们将以上所有模态都组合到了一起，并能以前所未有的方式为用户提供进一步的细粒度控制。

下面详细介绍 Movie Gen 的各项能力。

**视频生成**

给定文本提示词，Movie Gen 会使用一个针对文生图和文生视频任务优化过的联合模型来创建高质量...

---
```
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。