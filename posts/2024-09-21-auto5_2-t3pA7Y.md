---
title: '首个Mamba+Transformer混合架构多模态大模型来了，实现单卡千图推理'
date: 2024-09-22
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

本文作者来自于香港中文大学深圳和深圳大数据研究院。其中第一作者为香港中文大学深圳博士生王熙栋和研究助理宋定杰，主要研究方向分别为医疗AGI和多模态学习；博士生陈舒年研究方向为多模态学习，博士生张辰研究方向为高效语言模型。通讯作者为香港中文大学深圳数据科学学院王本友教授。

扩展多模态大语言模型（MLLMs）的长上下文能力对于视频理解、高分辨率图像理解以及多模态智能体至关重要。这涉及一系列系统性的优化，包括模型架构、数据构建和训练策略，尤其要解决诸如随着图像增多性能下降以及高计算成本等挑战。

该团队将模型架构调整为 Mamba 和 Transformer 块的混合体，在数据构建中考虑多个图像之间的时间和空间依赖性，并采用渐进式训练策略。提出了首个混合架构多模态大语言模型 LongLLaVA，在效率和性能之间实现了更好的平衡。

LongLLaVA 不仅在各种基准测试中取得了有竞争力的结果，还保持了高吞吐量和低显存消耗，其可以在单个 A100 80GB GPU 上处理近千张图像，展现出了广阔的应用前景。

[图片1](https://image.jiqizhixin.com/uploads/editor/46c439f7-5c0c-43f8-be70-2464f74c8982/640.png)

[图片2](https://image.jiqizhixin.com/uploads/editor/013b2750-9cea-4d54-9b3e-5bcbd52bcf9c/640.png)

论文地址：[https://arxiv.org/abs/2409.02889](https://arxiv.org/abs/2409.02889)

项目地址：[https://github.com/FreedomIntelligence/LongLLaVA](https://github.com/FreedomIntelligence/LongLLaVA)

1. 简介

多模态大语言模型（MLLMs）的快速进步展示了它们在各个应用领域中的显著能力。然而，多图像理解场景仍然是一个重要但尚未充分探索的方面。特别是，将 MLLMs 的应用场景扩展到理解更长的视频、更高分辨率的图像以及基于更多历史信息的决策，对于提升用户体验和进一步拓展 MLLMs 的应用范围至关重要。

...

[图片3](https://image.jiqizhixin.com/uploads/editor/8d9d5050-290a-4956-8242-04b6fa6e5d31/640.png)

...

更多内容请阅读原文。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。