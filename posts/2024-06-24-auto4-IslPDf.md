---
title: 'OpenAI正引发全球AI安全热战，我们将如何应对？'
date: 2024-06-25
author: ByteAILab

---

最近AI领域发生了一件重磅事件，引发行业内的关注。

北京时间6月20日，OpenAI联合创始人、前首席科学家伊利亚（Ilya Sutskever）在社交平台上发文宣布，将创办一家“安全超级智能公司”（Safe Superintelligence Inc.，简称SSI）。

---


“SSI是我们的使命、我们的名字、我们的整个产品路线图，因为它是我们唯一的焦点。我们的团队、投资者和商业模式都是以实现SSI为目标。”伊利亚、科技企业家和投资者Daniel Gross、OpenAI前科学家Daniel Levy在该公司官网发表的联合署名文章中称，“构建安全的超级智能，是我们这个时代最重要的技术问题。我们计划尽快提升能力，同时确保我们的安全始终处于领先地位。”

伊利亚透露，该公司已经启动了世界上第一个直接的安全的超级智能实验室，只有一个目标和一个产品：一个安全的超级智能。但目前，SSI并未公布公司的股东、科研团队和盈利模式。

实际上，伊利亚离开OpenAI，很大程度上就是因为与OpenAI CEO奥尔特曼（Sam Altman）为核心的管理层存在分歧，尤其是在如何驾驭超级AI、AGI（通用人工智能）安全发展等问题上存在相反的观点。

其中，奥尔特曼和OpenAI总裁Greg Brockman倾向于加速商业化，以获得更多资金来支持AI模型的算力需求，力求快速增强AI的力量；而伊利亚等人则希望AI更安全。去年11月，双方矛盾激化，OpenAI上演“宫斗”大戏，结果奥尔特曼和Greg Brockman在短暂离职后重归OpenAI，原董事会中的多位董事离开，伊利亚则在今年5月宣布离任。

对此，国内AI安全公司瑞莱智慧（RealAI）CEO田天对钛媒体AGI等表示，奥尔特曼和伊利亚之间的分歧在于对AI安全的“路线之争”，而伊利亚的新公司就是为了 AI 安全目标而设立的。

田天指出，包括伊利亚、图灵奖得主Geoffrey Hinton等人认为，AI安全问题现在已经到了“非常迫切”去解决的程度，如果现在不去做，很有可能就会错过这个机会，未来再想亡羊补牢是“没有可能性”的。

“大模型领域也是一样，虽然我们对于大模型预期非常高，认为它在很多领域都能去应用，但其实现在，真正在严肃场景下的大模型应用典型案例还是非常少的，主要问题在于 AI 安全上。如果不解决安全可控问题，对于一些严肃场景，是没有人敢去信任AI，没有人敢去用它（AI）。只有说解决安全、可信问题，AI才有可能去落地和应用。”田天表示，如果一些商业化公司对于安全问题不够重视、并毫无边界快速往前跑的话，可能会造成一系列安全危害，甚至可能对于整个全人类有一些安全风险和影响。

早在聊天机器人ChatGPT发布之前，伊利亚接受英国《卫报》采访时便提到AGI对人类社会可能的威胁。他把AGI与人类的关系，类比人类与动物的关系，称“人类喜欢许多动物，但当人类要造一条高速路时，是不会向动物征求意见的，因为高速路对人类很重要。人类和通用人工智能的关系也将会这样，通用人工智能完全按照自己的意愿行事。”

AGI，即人工智能已具备和人类同等甚至超越人类的智能，简单而...

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。