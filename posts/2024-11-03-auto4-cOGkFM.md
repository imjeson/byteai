---
title: '当AI取代真相，大模型如何一步步诱骗了人类的文明？'
date: 2024-11-04
author: ByteAILab

---

文章来源：追问nestquestion

![图片来源：由GPTNB生成](http://www.jesonc.com/upload/8FD7B96F5E34993C64020C0DB54F4C00/1730426781892/Fv7s-fZCEso4Hn0rqWiwTr9Jj2f_.png)

如今，人们选择餐厅，多半会打开app搜索一番，再看看排名。然而美国奥斯汀的一家餐厅Ethos的存在证实这种选择机制多么不可靠。

---
Ethos在社交媒体instagram宣称是当地排名第一的餐厅，拥有7万余粉丝。

实际上，这家看起来很靠谱的餐厅根本不存在，食物和场地的照片都由人工智能生成。可它家发布在社媒上的帖子却得到了数千名不知情者的点赞和评论。大模型通过视觉形式误导公众认知，引发了人们对其潜在影响的深刻思考。

![图1：图源：X](http://www.jesonc.com/FlB4irAzphvhDRNl9ZopcYv14bVF)

大型语言模型（LLMs），由于其幻觉及涌现特性，总让人们忧虑其传播虚假信息的可能。然而这一现象背后的机理我们却不甚了解。大模型究竟如何改变人类的心理机制，让人们越来越难以判断一件事情真实与否，并失去对专业机构和彼此的信任？

[▷ Garry, Maryanne, et al. "Large language models (LLMs) and the institutionalization of misinformation." Trends in Cognitive Sciences (2024).](http://www.jesonc.com/Fs_Lu-54Tehr8ylVlIXfCAJcGcbC)

### AI生成的虚假信息影响深远

类似开篇提到的虚假餐厅的例子，现实生活中发生了不止一次。2023年11月，搜索引擎Bing曾因为爬取了错误信息，而针对“澳大利亚是否存在”的问题，给出了如下图所示荒谬的回复。（事后官方很快对该问题进行了修复。）

![图2：Bing截图；图源：Bing](http://www.jesonc.com/Fs-7l6ur6wevh_E-jy7wd1TY0Wi6)

上面的例子，还可视为程序的bug，而普林斯顿的一项研究则系统性说明了AI生成数据的影响不止如此。该研究发现，截止24年8月，至少5%的英文维基百科页面是由AI生成的，相对而言德语、法语和意大利语文章的占比较低。

维基百科（Wikipedia）是人工智能训练数据的重要来源，且被普遍视为权威的信息来源。尽管AI生成的内容并不一定都是虚假信息，但该研究指出，被标记为AI生成的维基百科文章通常质量较低，并具有较明显的目的性，往往是自我推广或对有争议话题持特定观点。

### AI生成的虚假信息如何利用了判断真假的启发式弱点

虽然虚假信息被发现后很快会被纠正，但如同小时候听过的“狼来了”的故事，一次次的接触虚假信息，会让磨损我们彼此间的信任。

我们判断一件事情是否为真时，有两种不同的思考方式，一是启发式，另一种则是费力的思考。启发式思维所需的认知资源更少，依赖直觉，属于丹尼尔·卡尼曼所说的系统一。对个体来说，判断是否是虚假信息，启发式的判断...

![图3：大模型会如何利用人类事实监控机制的弱点，图源：参考文献1](http://www.jesonc.com/FuDJ5GbU3Ij7Gmvoa-sutkz1XT5P)

在在日常生活中，我们常用到两种启发式方法判断信息真实与否：一种是观察发言是否流畅自信，另一种是言论是否熟悉。然而事实证明，这两种判断基准在人工智能面前都会败下阵来。

然而，大模型生成的文章，往往会显得自信且流畅。一项研究对比人工智能生成的和人类撰写的...

### 小结

人类应对虚假信息的方式，与自身的历史一样悠久。只是大模型的出现，让我们传统的启发式应对机制失效了。要应对大模型生成的虚假信息，需要多方合作，可以通过大模型智能体以及众包协作来进行事实审核，也需要向公众科普大模型的运行机制，使其不再拟人化大模型，并习惯采取非启发式的方式去判断信息真假。

我们需要创立优化的虚假信息监控制度，并重建大众对机构的信任。我们需要加深对真伪信息判断机制的理解，无论是个体层面、人际层面，以及制度层面。我们需要对每个解决方案的有效性进行心理学研究。缺少这些，迎接我们的，不是后真相时代，而是不可避免的虚假信息制度化。
---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。