---
title: '光芯片能否代替电子芯片？破解 AI 「算力荒」'
date: 2024-05-22
author: ByteAILab

---

编辑 | ScienceAI

摩尔定律的描述已经非常快了——计算机芯片每两年左右就会安装两倍数量的晶体管，从而在速度和效率上产生重大飞跃。但深度学习时代的计算需求增长速度更快——这种速度可能不可持续。

---


论文链接：https://arxiv.org/abs/2202.05924

国际能源署预测，2026 年人工智能消耗的电力将是 2023 年的 10 倍，而当年的数据中心消耗的能源将相当于日本一个国家一年的能源消耗。

报告链接：https://www.iea.org/reports/electricity-2024

「人工智能所需的[计算能力]每三个月就会翻一番，速度远远快于摩尔定律的预测。」 计算硬件公司 Lightmatter 的创始人兼首席执行官 Nick Harris 表示，「这会破坏公司和经济。」

最有前景的方法之一是不使用电子来处理信息（电子在计算领域占据了 50 多年的主导地位），而是使用光子流（即微小的光包）。近期的研究表明，对于现代人工智能的某些基础计算任务，基于光的「光学计算机」可能具有优势。

剑桥大学物理学家 Natalia Berloff 表示，光计算的发展「为人工智能等需要高速、高效处理的领域的突破铺平了道路」。

### 最佳光学

理论上，光有许多潜在好处。其一，光信号比电信号可以携带更多信息——它们有更多的带宽。其二，光频率也比电频率高得多，因此光系统可以在更短的时间内以更少的延迟运行更多的计算步骤。

还有效率问题。除了相对浪费的电子芯片造成的环境和经济成本之外，它们的运行温度也非常高，以至于只有一小部分晶体管（所有计算机核心的微小开关）可以随时处于活动状态。理论上，光学计算机可以同时进行更多操作，在消耗更少能源的同时处理更多数据。斯坦福大学电气工程师 Gordon Wetzstein 说：「如果我们能够利用」这些优势，「这将带来许多新的可能性。」

### 一些简单的数学

矩阵或数字数组相乘的过程是大量重型计算的基础。具体来说，在神经网络中，矩阵乘法是如何在旧数据上训练网络以及如何在经过训练的网络中处理新数据的基本步骤。光可能是比电更好的矩阵乘法媒介。

这种人工智能计算方法在 2017 年爆发，当时麻省理工学院的 Dirk Englund 和 Marin Soljačić 领导的团队描述了如何在硅芯片上构建光学神经网络。

论文链接：https://www.nature.com/articles/nphoton.2017.93

研究人员将他们想要相乘的各种量编码成光束，然后将光束发送通过一系列改变光束相位（光波振荡方式）的组件，每个相位改变代表一个乘法步骤。通过反复分裂光束、改变相位、重新组合，可以使...
---

Related article: [AI Needs Enormous Computing Power. Could Light-based Chips Help?](https://www.quantamagazine.org/ai-needs-enormous-computing-power-could-light-based-chips-help-20240520/)
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。