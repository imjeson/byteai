---

title: 'CVPR 2025 | 2D 大模型赋能3D Affordance 预测，GEAL助力可泛化的3D场景可交互区域识别'
date: 2025-04-11
author: ByteAILab

---

**GEAL 由新加坡国立大学的研究团队开展，第一作者为博士生鲁东岳，通讯作者为该校副教授 Gim Hee Lee，团队其他成员还包括孔令东与黄田鑫博士。**

主页：[https://dylanorange.github.io/projects/geal/](https://dylanorange.github.io/projects/geal/)

论文：[https://arxiv.org/abs/2412.09511](https://arxiv.org/abs/2412.09511)

代码：[https://github.com/DylanOrange/geal](https://github.com/DylanOrange/geal)

在现实世界中，如何让智能体理解并挖掘 3D 场景中可交互的部位（Affordance）对于机器人操作与人机交互至关重要。

---
所谓 **3D Affordance Learning**，就是希望模型能够根据视觉和语言线索，自动推理出物体**可供哪些操作、以及可交互区域的空间位置**，从而为机器人或人工智能系统提供对物体潜在操作方式的理解。

与 2D 任务相比，3D 数据的获取与高精度标注通常更为困难且成本高昂，这使得大规模高质量的 3D 标注数据**十分稀缺**，也阻碍了模型在新物体或场景中的泛化。与此同时，现有 3D 多依赖几何与位置编码来表征空间结构，难以从外观语义中充分汲取上下文信息，因而在传感器不准、场景复杂或处理误差等情形下更易受到噪声影响，导致其**鲁棒性不足**，难以稳定应对真实环境中的多变挑战。

为克服标注与数据分布限制，一些工作尝试将 2D 视觉模型或大语言模型融入 3D 场景理解。但由于 3D 与 2D 的特征存在**显著模态差异**，以及受限于对空间几何关系与纹理细节的保留，直接对接往往导致可交互区域定位不准确或易受点云噪声的干扰，难以在真实复杂场景中保持**鲁棒性和通用性**。因此，如何充分利用**大规模预训练的 2D 模型**所蕴含的强大表征能力，同时兼顾 3D 模态下**细节和结构信息的准确对齐**，成为提升 3D Affordance Learning 效果的关键挑战。

针对上述问题，新加坡国立大学的研究团队提出了 **GEAL（Generalizable 3D Affordance Learning），无需额外收集与标注大规模 3D 数据，便可借助 2D 基础模型实现对 3D 场景中可交互区域的精确预测。**具体而言，GEAL 首先利用 3D Gaussian Splatting 将稀疏点云转换为可在 2D 模型中处理的真实感渲染图，并通过跨模态一致性对齐（Cross-Modal Consistency Alignment）有效融合 2D 视觉与 3D 空间特征，显著提升模型对多种物体与场景的泛化能力。此外，团队还构建了**针对真实场景扰动的全新基准数据集**，全面评估模型的稳健性。实验结果表明，GEAL 无论在公共数据集还是各种噪声环境下，都显著优于现有方法，为通用且鲁棒的 3D Affordance Learning 提供了新思路。

目前，GEAL 已被 CVPR 2025 接收，论文、代码和模型权重均已公开。

**2D-3D 跨模态对齐  完成 3D 可交互区域预测**

如图所示，在 GEAL 的整体框架中，我们通过**2D 分支 与 3D 分支**的协同合作，将预训练 2D 模型的强语义表达能力注入到稀疏点云的三维世界中，并通过跨模态特征对齐来获得更强的鲁棒性与泛化能力。下面对各个关键步骤加以说明。

**利用 3D Gaussian Splatting 渲染稀疏点云，高效引入 2D 分支**

考虑到三维数据通常存在采样稀疏、标注昂贵、遮挡严重等问题，我们在网络结构中单独设置了一个 2D 分支，借助在海量 2D 数据上预训练的视觉 backbone（如 DINOV2），获取包含丰富语义上下文与外观信息的多粒度图像特征，从而为后续的三维功能区域预测提供更具鲁棒性与泛化力的先验。由于该分支与 3D 分支并行存在，我们可在后期设计中灵活地融合并对齐 2D/3D 特征，避免简单拼接带来的模态失配。为了让预训练的 2D 模型充分 “看见” 三维场景的纹理与遮挡信息，GEAL 采用了 3D Gaussian Splatting 技术来渲染点云。具体而言，我们用可学习的高斯基元对每个三维点进行表示，并通过光栅化与 α-混合在 2D 图像中生成具有深度、透明度与颜色信息的像素，从而获得更为平滑、逼真的二维视图。这些视图不仅能够为 2D 模型提供足以辨别纹理和轮廓的语义特征，还能在后续跨模态步骤中与点云的几何结构建立一一对应关系，为特征对齐打下基础。

**跨模态特征对齐**

在两条分支分别获得多尺度 2D/3D 特征后，GEAL 通过**颗粒度自适应融合模块（Granularity-Adaptive Fusion Module, GAFM） 与一致性对齐模块（Consistency Alignment Module，CAM）**实现语义与几何间的双向对齐。

- 颗粒度自适应融合模块  
针对 2D 与 3D 在不同层级上所捕获的细节与全局信息，通过自适应权重聚合和文本引导的视觉对齐，将最相关的多粒度特征与用户指令相互融合。这样既能突出与功能需求紧密关联的目标局部，又确保对全局场景保持整体把握。

- 一致性对齐模块  
基于 Gaussian Splatting 所构建的像素-点云映射，将 3D 分支提取的点云特征再度渲染至二维平面，与 2D 分支形成逐像素对应，然后通过一致性损失（如 L2 距离）使两者在同一空间区域的表征尽可能相似。这种策略能让 2D 分支的通用语义向 3D 分支扩散，同时也让 3D 分支在几何维度上对 2D 特征形成有益补充，最终实现更准确的可交互区域定位。

**Corrupt Data Benchmark 评估鲁棒性**

为了更全面地测试 GEAL 在真实干扰环境中的表现，我们基于常见的 PIAD 与 LASO 数据集，构建了包含多种扰动形式的 **Corrupt Data Benchmark**。它涵盖局部或全局的随机丢失、噪声注入、尺度变化、抖动及旋转等多种干扰场景，模拟复杂感知条件下的真实挑战。实验结果表明，GEAL 在该基准上依然能够保持高精度与鲁棒性，印证了跨模态对齐对于三维功能区域预测在噪声环境中的关键价值。

通过以上几个核心环节，GEAL 成功将 2D 模型的强大语义理解与 3D 数据的空间几何细节有机结合，不仅免去了大规模 3D 标注数据的依赖，还显著提升了可交互区域预测的可靠性与泛化水平，为 3D Affordance Learning 迈向真实应用场景提供了新的技术思路。

**实验结果**

为评估 GEAL 在 3D 场景可交互区域预测上的整体表现，作者在主流数据集 PIAD 与 LASO 上进行了系统实验。结果显示，GEAL 相较现有最优方法均取得了更高分数，尤其在 unseen 类别测试中依然保持高准确率，证明其对未见过的物体形状与类别具备良好适应能力。这一优势主要得益于 2D 语义先验的充分利用，以及跨模态一致性带来的 2D-3D 特征对齐，使得模型能在几何细节与语义信息之间保持平衡。

为了模拟实际感知场景中的各种干扰，如传感器噪声、局部丢失或随机旋转等，作者还在新提出的 Corrupt Data Benchmark 上对 GEAL 进行了测试。结果表明，即便在高度不确定的环境下，GEAL 依然能够稳定预测可交互区域，展现出优异的鲁棒性。这主要归功于 2D 分支在大规模预训练模型中的抗干扰特性，以及与 3D 分支通过一致性约束实现的高效信息传递。

相比仅使用 2D 分支或 3D 分支的基础版本，融合双分支并加入 CAM 后，在未见类别和高噪声条件下的准确率均显著提升；进一步引入 GAFM 后，则在见类与未见类任务中同时提高精度与 IoU，说明多粒度特征融合对于捕捉局部细节和全局语义至关重要。

综上所述，多项实验结果与消融研究均验证了 GEAL 的有效性：该方法不仅在常规数据集上表现出卓越的精度与泛化能力，还能在真实干扰环境中保持稳健，展现出跨模态对齐与双分支架构在 3D 场景可交互区域预测中的强大潜力。

**结论**

综上所述，GEAL 通过双分支架构与 3D Gaussian Splatting 的巧妙结合，在不依赖大规模 3D 标注的情况下，充分挖掘了大规模 2D 预训练模型蕴含的丰富语义信息，实现了对 3D 场景可交互区域的精确预测。该成果为在机器人操作、增强现实和智能家居等领域中灵活、高效地获取三维可交互区域提供了新思路，对构建通用、稳健的 3D Affordance Learning 系统具有重要意义。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。