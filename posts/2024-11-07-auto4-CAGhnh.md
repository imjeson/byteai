---
title: '研究实锤：别让大模型「想」太多，OpenAI o1准确率竟下降36.3%'
date: 2024-11-08
author: ByteAILab

---

思维链（CoT）已被证明可以在许多任务（如多步骤推理）上显著提升大模型的性能。然而，在哪些情况下，CoT 会系统性地降低大模型的性能，这仍然是一个有待进一步讨论的问题。

---
如今，来自普林斯顿大学和纽约大学的研究团队，参照思考对“人类性能”的影响，提出了新的见解。他们认为，虽然模型的认知过程与人类的认知过程并不完全相同，但可以参照思考对人类“性能”产生负面影响的情况，假定思考会对模型产生负面影响的环境。他们从心理学中选择了 6 项已被充分研究的任务类型来探讨 CoT 对 LLM 性能的影响，并验证了 CoT 在一些任务中甚至可能导致模型准确率下降。这一发现不仅为未来优化 LLM 的提示策略提供了新思路，还为理解人类与模型在推理过程中的相似性与差异性带来了新见解。

论文链接：[https://arxiv.org/abs/2410.21333](https://arxiv.org/abs/2410.21333)

研究表明，CoT 并非在所有任务中都能提高模型性能，在隐性统计学习、面部识别、含例外模式的数据分类三种情况下，各种 SOTA 模型的性能都会明显下降。此外，研究本身进一步揭示了通过人类心理学研究大模型的可行性。

## 研究方法
为分析 CoT 对大语言模型（LLM）与多模态大模型（LMM）性能的影响，该研究的方法框架基于以下两个关键条件：
1. 言语思考或深思熟虑会损害人类“性能”的情况。
2. 将制约人类“性能”的因素推广到语言模型的情况。

之后，为验证“CoT 在一些任务中会导致模型表现下降”的假设，研究团队在上述两个条件的指导下基于人类心理学设计了以下 6 种任务场景：
- 隐性统计学习
- 面部识别
- 含例外模式的数据分类
- 解释逻辑不一致
- 空间直觉
- 特征聚合决策

图｜对 6 项任务进行评估，以确定 CoT 提示是否会降低任务的绩效。（来源：该论文）

针对每个任务场景，研究团队分别构建了零样本（zero-shot）和 CoT 提示条件，并在多个主流 LLM 和 LMM 上进行测试，包括 GPT-4o、Claude 3.5、Llama 等，通过对比不同条件下模型的准确率，量化 CoT 提示的效果，从而验证他们的假设。

## 实验结果
研究团队首先对满足上述两个关键条件的 3 类任务场景进行实验验证。

### 隐性统计学习
针对该情境，该研究考察了模型在分类基于特定语法结构的序列时的表现。任务包含 4400 个分类问题，基于 100 种有限状态语法（FSG）结构，每个测试提供 15 个样例，再要求模型对新序列进行分类。

实验结果显示，使用 CoT 提示的模型表现显著下降，尤其是 OpenAI o1-preview 模型的准确率下降了 36.3%。

图 | 人工语法学习中 zero-shot 和 CoT 对比结果。（来源：该论文）

### 面部识别
在该任务情境中，该研究测试了 CoT 是否会影响模型的面部识别能力，这是基于心理学中“语词遮蔽”现象进行的任务情境设计。模型需要在 500 项任务中从 5 个候选中匹配初始人脸。

结果表明，当被要求执行 CoT 时，每个被测试的 LMM 都显示出性能下降，与假设一致。

图｜面部识别中 zero-shot 和 CoT 提示的对比。（来源：该论文）

### 含例外模式的数据分类
该任务通过包含多个主次特征的分类任务来测试模型在处理含例外情况时的表现，任务要求模型在多次分类中逐步学习，目标是尽可能减少迭代次数。

实验在 GPT-4o、Claude 3.5 Sonnet 和 Claude 3 Opus 上进行，结果表明，CoT 显著增加了学习轮次。平均来看，GPT-4o 在 CoT 条件下完成正确分类所需的轮次为直接提示的四倍，而 Claude 3.5 Sonnet 和 Claude 3 Opus 的轮次需求也分别增加至直接提示的两倍多。

图｜使用直接或 CoT 提示，模型学习标签的平均轮数。（来源：该论文）

在 GPT-4o 的进一步分析中发现，直接提示使模型在第二或第三轮就能达到完美分类，而使用 CoT 时模型在第四到第五轮仅能正确分类 8/10 的对象。这表明 CoT 提示会引导模型偏向基于规则的推理方式，而忽视了已知的正确答案，导致分类效率大幅下降。

之后，研究团队又对满足条件（1）但不满足条件（2）的三类任务情境开展实验。

### 解释逻辑不一致
在该任务中，模型需要识别句子对中的逻辑矛盾性。该任务基于 SNLI 和 MNLI 数据集以及合成数据集。

研究发现，CoT 增加了模型忽视矛盾的可能性，模型在逐步推理时更倾向于关注复杂的逻辑结构，从而忽视了直接矛盾判定。这表明在需要精确逻辑验证的任务中，CoT 提示存在局限性。

图｜逻辑不一致任务中比较 zero-shot 和 CoT 的结果。（来源：该论文）

### 空间直觉
在该情境中，模型需要通过“倾斜杯子”的问题来推断水面的位置。这类任务依赖于人类的空间或运动直觉，而人类通常在非言语思维下表现更好。

模型接收了视觉提示和多项选择答案，实验结果显示，使用 CoT 提示对模型表现无明显影响。这说明在依赖空间或运动直觉的任务中，模型的推理方式与人类的直觉差异较大，因而 CoT 提示的负面影响较小。

图｜空间直觉任务中 zero-shot 和 CoT 的比较结果。（来源：该论文）

### 特征聚合决策
此任务模拟了基于多项特征的决策过程（如选房），用于测试信息超载对决策的影响。人类在类似任务中由于记忆限制，往往在 CoT 模式下表现较差。相对地，模型保留了所有上下文信息，能够无损地聚合和评估每项特征。

结果显示，CoT 提示在高上下文记忆任务中提高了模型表现，说明在信息保留至关重要的场景下，CoT 提示能够发挥正向作用。

图｜四种模型和三种范围内的公寓选择任务结果。（来源：该论文）

## 不足与展望
当然，该研究也存在一些局限性，如下：
- inference-time 推理的类型。
- 应用范围。
- 关于 CoT 未能复制人类结果的替代解释。

研究团队表示，虽然该研究聚焦于 CoT 推理，但所提出的框架为利用人类心理学研究评估和改进模型表现提供了一种通用策略。他们认为，未来还需要更多的跨学科合作，通过将自然语言处理方法、心理学见解与人类和模型表现比较的相关研究相结合，可以形成更全面的 AI 评估和改进策略。

---

来源：[https://www.aixinzhijie.com/article/6847172](https://www.aixinzhijie.com/article/6847172)
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。