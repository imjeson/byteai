---
title: 'ICML 2024 Oral | DPO是否比PPO更适合LLM，清华吴翼团队最新揭秘'
date: 2024-07-23
author: ByteAILab

---
AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

吴翼，清华大学交叉信息院助理教授，曾任 OpenAI 全职研究员，研究领域为强化学习，大模型对齐，人机交互，机器人学习等。2019 年在美国加州大学伯克利分校获得博士学位，师从 Stuart Russell 教授；2014 年本科毕业于清华大学交叉信息院（姚班）。其代表作包括：NIPS2016 最佳论文，Value Iteration Network；多智能体深度强化学习领域最高引用论文，MADDPG 算法；OpenAI hide-and-seek 项目等。

如何让大模型更好的遵从人类指令和意图？如何让大模型有更好的推理能力？如何让大模型避免幻觉？能否解决这些问题，是让大模型真正广泛可用，甚至实现超级智能（Super Intelligence）最为关键的技术挑战。这些最困难的挑战也是吴翼团队长期以来的研究重点，大模型对齐技术（Alignment）所要攻克的难题。

对齐技术中，最重要的算法框架就是根据人类反馈的强化学习（RLHF, Reinforcement Learning from Human Feedback）。RLHF 根据人类对大模型输出的偏好反馈，来学习基于人类反馈的奖励函数（Reward Model），并进一步对大模型进行强化学习训练，让大模型在反复迭代中学会辨别回复的好坏，并实现模型能力提升。目前世界上最强的语言模型，比如 OpenAI 的 GPT 模型和 Anthropic 的 Claude 模型，都极其强调 RLHF 训练的重要性。OpenAI 和 Anthropic 内部也都开发了基于大规模 PPO 算法的 RLHF 训练系统进行大模型对齐。

...

整体内容转化为以上Makedown源文件格式。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。