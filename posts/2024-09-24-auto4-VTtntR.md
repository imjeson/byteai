---

title: '带你认识微信多模态大模型 POINTS'
date: 2024-09-25
author: ByteAILab

---

近来，随着大型语言模型的发展，视觉语言大型模型的能力也在逐步增强，GPT-4、Gemini Pro 1.5和Claude 3等著名的闭源模型成功将 LLM 扩展到视觉语言模型领域。LLaVA，InternVL等开源模型也在迅速发展。

---
目前，视觉语言模型领域存在一些关键问题亟待解决：1）闭源模型很少公开关于其架构的详细信息。相比之下，开源模型虽公开了其训练策略，但这些策略的详细消融并没有完全披露。2）在目前的开源工作中，对于预训练阶段，大多都是凭经验添加不同来源的数据集，这使得预训练过程难以得到深入的探索。3）在微调阶段，绝大多数工作关注的重点通常是添加和消融更多的数据集，这样性能会较快触及瓶颈。我们针对以上几点给出了我们的方案，并进行了清晰充分的实验论证。

POINTS 主要包含 3 个部分:
1. 强 baseline: 通过融入当前主流的前沿技术，我们为进一步探索创造了一个较强的 baseline
2. pre-train 数据集过滤策略：借鉴 LLM 中常用的利用 ppl 来过滤数据的思想，我们利用一个离线模型使用 ppl 的方式来过滤 pre-train 数据，最终得到 1M 高效的 pre-train 数据
3. model soup: 在指令微调阶段，过滤大部分工作都集中在消融更多的数据集来增强模型性能，但这种方式很可能达到一个阶段后带来的增益就比较有限，我们率先提出在不同指令微调数据集上训练得到的模型进行 model soup，实验结果表明，模型的性能可以得到进一步较大的提升

### 强 baseline

我们首先将现在开源工作中的各种技术（动态高分辨率，CapFusion，双视觉编码器，Individual Select）进行了整合，并提出一种新的动态分辨率分割方法，以搭建一个更加稳健和高效的 baseline。下图是POINTS的模型结构。对于每个模块（例如 OCR ViT、General ViT、MLP Projector 和 Large Language Model），虚线左侧标签表示预训练时的状态，右侧标签表示指令调优时的状态。

**Consistent Aspect Ratio Dynamic High Resolution (CATTY)**

将高分辨率图像提供给视觉语言模型有利于捕捉细粒度细节和减少幻觉。与 LLaVA-Next 和 InternVL-1.5 中的动态高分辨率不同的是，我们在对高分辨率图像进行切分时保持了图块的纵横比。在InternVL-1.5中，在分割图像之前，图像大小会被调整为预定义分辨率中与其最接近的目标分辨率，这种切图方式容易导致失真。而我们通过在切图时引入overlap的方式在把图像切割为固定大小图块的同时，保持了图块纵横比。

CATTY的具体步骤如下：i) 首先，预定义图像可以分割成的最大图块数（在我们的实验中设置为 8）。ii) 列出图块数不超过最大图块数的所有整数分辨率组合。iii) 给定一个高度为H和宽度为W的图像，我们寻找与其分辨率最近的整数分辨率，乘视觉编码器的图像尺寸，得到目标图像的高度Hr和宽度Wr，并通过以下方式将图像大小调整为目标大小 （Ht × Wt ）：

给定视觉编码器的输入分辨率 Hv×Wv，目标图像应分为 Hr/Hv × Wr/Wv 图块。接下来，我们使用一个滑动窗口分割目标图像 Ht×Wt ，该窗口的步幅分别为 Height 和 width。步幅（Sh, Sw） 的计算方式如下：（若 Hr/Hv = 1，Sh为0，若 Wr/Wv = 1，Sw 为 0）

除了使用CATTY分割的图块，我们还加入了整体图像的缩略图来捕获整体上下文信息。在将视觉编码器输出的特征对齐并传入到大型语言模型之前，我们InternLM-XComposer2-4KHD中的pixel-shuffile，设置下采样因子为 0.25，减少图像特征的序列长度以提高效率。

**CapFusion**

预训练数据集中原始 caption 通常存在很多噪声和结缺陷。为了解决这个问题，LAION-COCO 和 BLIP-LAION等提出了通过 image captioning model 生成 synthetic caption。但合成字幕中较为简单的句法和语义结构可能会导致缺失可扩展性和缺乏世界知识。CapFusion利用大语言模型整合原始 caption 和 synthetic caption，在丰富的世界知识和结构化且语法简单之间取得了较好的平衡。我们使用 InternLM-XComposer2 为图像生成 synthetic caption，使用 InternLM2 来整合原始 caption 和 synthetic caption。

**Dual Vision Encoder**

相关工作如 SPHINX 和 Cambrian1，已经证明不同的视觉编码器会在不同的领域表现优势，结合多个视觉编码器可以在性能上有更大的提升。文本密集型图像所需的视觉编码能力在一定程度上有别于自然图像。为了增强光学字符识别（OCR）能力，我们训练了一个单独的视觉编码 OCR ViT，遵循 Vary 的方法从图像中提取文本特征。与其不同的是，我们没有从零构建图文对的训练样本（如图表），我们使用PaddleOCR提取的文本结果构建数据集，并在数据集中加入描述自然图像的caption组成完整的预训练数据集。我们将完成训练的 OCR ViT 与通用视觉编码器（通用 ViT）用过加权平均值合并，然后将输入到大型语言模型中。

**Individual Select**

Individual Select 通过贪心选择的方式从一个数据池中确定最有效的指令调优数据集。我们收集了其中所提到的开源数据集，并整合了DeepSeekVL、Cambrian-1 和 Cauldron 使用的数据以及额外的16个数据集。此外，鉴于学术数据集的提示风格趋于同质，为了增强提示的多样性，我们从 LAION-5B 中随机选取了一些图像使用 GPT-4o 生成了一系列问答对。最终我们得到了可视化指令调优数据集 Base Set。

**预训练数据筛选策略**

perplexity(PPL) 常在大语言模型中被用作评估预训练数据集质量的指标。受其启发，我们利用已被训练过的视觉语言模型 P 来进一步过滤掉CapFusion获得预训练数据集中的低质量部分。对于预训练数据集中的每个数据样本s，我们使用以下公式计算所有文本的 PPL,（{w1，...，wN } 表示s中的文本序列）：

我们对所有样本进行升序排序，并选择前 20% 用于预训练。与大型语言模型中的筛选结果不同，对比 PPL 前 20% 和后 20% 的样本，我们发现两者的区别并不是数据的质量。PPL 后 20% 的物品通常包含晦涩难懂的世界知识，例如游戏版本号和计算机出厂序列号，这类世界知识极为罕见，且信息量非常少，因此对模型的学习不太有益。

**Model Soup**

视觉指令调优数据对于现有视觉语言模型的卓越性能至关重要。但现有工作大多通过迭代消融来选择更有效的数据集，这种方法很快会达到了一个瓶颈期，后续的数据选择带来的提升微乎其微，甚至会降低模型性能。在数据选择遇到瓶颈后，我们使用 model soup 整合使用不同指令调优数据集进行微调的模型优势。model soup 的原理是通过使用不同的超参数（学习率、数据增强、初始化种子等）对模型进行微调以收敛到不同的局部最优值，对这些模型的权重进行插值，以得到一个更强的模型。为了将 model soup 的思路应用在数据集的优势融合上，我们在性能饱和的指令调优数据集base set上单次加入一个 Individual Select 阶段未能带来显著性能提升的数据集，构成新的数据集Di* ，模型在微调后收敛到不同的局部最优值f(Dk*;θk)，再将这些模型权重进行插值。我们提出了Maximum Soup、Average Soup 和 Greedy Soup 三种方法来选择微调模型的最佳组合。

**Maximum Soup**

给定评估分数 Acc，我们使用以下公式获得一个更强模型 f(θs)：

**Average Soup**

通过取所有微调模型的权重平均值，我们可以获得更强的模型 f(θs)：

**Greedy Soup**

首先根据评估分数对微调后的模型进行降序排序。接着遍历排序后的模型。对于每个模型，我们计算其权重与模型池中当前所有模型的权重的平均值。如果评估分数提高，则模型将添加到池中。最后，我们对池中所有模型的权重进行平均，以获得更强的模型，表示为 f(θs)。下表概述了 Greedy Soup 的步骤。

**实验**

我们使用 OpenCompass中 的 8 个基准，从不同角度均衡地对模型进行评估。这些指标包括：MMBench 和 MMStar：用于评估通用能力；MMMU：用于测试STEM（科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics））能力；MathVista：用于数学相关能力；AI2D：用于图表相关能力；OCRBench：用于OCR功能；MMVet：用于主观评估。我们使用 OpenCompass 提供的 VLMEvalKit 进行标准化评估，在 MMBench 上选择 dev-en split。

**数据配置**

在预训练阶段，训练 OCR ViT 时，我们没有额外抓取PDF文件并转换为图像构建数据集，为了提高复杂背景下的 OCR 能力，我们从 LAION-5B-en、LAION-5B-cn、WuKong 和 Zero 中随机选择了 2000 万个数据样本。使用 PaddleOCR 从图像中提取文本，替换原始 caption 以形成新的数据对。此外我们加入 LAION-5B 的 1000 万个原始数据样本，构成最终数据集进行训练。

在指令调优阶段，基于 中确定的数据，我们继续使用 Individual Select 的方式在其他开源数据工作中进行了选择，确定为基础数据集。对于未能通过 Individual Select 得到增益的其余数据集，我们将其组成数据池，以待通过 model soup 的方式加以选择和利用。

**训练配置**

在 OCR ViT 的预训练阶段，我们使用 LLaVA 架构，视觉编码器从 OpenAI 的 CLIP-ViT-Large-336 初始化，大型语言模型是从 Yi-1.5-9B-Chat初始化。冻结大型语言模型权重，设置视觉编码器和MLP是可训练的。视觉编码器和MLP的学习率分别设置为 2×10-4 和 2×10-5 ，学习率的schedule在训练过程的前3%采用预热，剩余步骤采用余弦衰减。

在整体视觉语言模型的预训练阶段，通用 ViT 从 OpenAI 的 CLIP-ViT-Large-336 初始化的，OCR ViT 则继承上个阶段的权重。参考 Vary 的设置，我们的通用 ViT 只有最后三层是可训练的，OCR ViT 则在整个阶段保持冻结状态。我们选择通用 ViT 和 OCR ViT 倒数第二层的特征通过 MLP 与大预言模型连接。起到特征对齐作用的 MLP，在预训练阶段保持可训练状态。通用 ViT 和 MLP 的学习率分别设置为 2×10-4 和 2×10-5 ，学习率的schedule在训练过程的前3%采用预热，剩余步骤采用余弦衰减。

在指令调优阶段，我们冻结通用 ViT 和 OCR ViT。MLP 和大预言模型保持可训练状态，学习率设置为 2 × 10−5 。学习率的 schedule 在训练过程的前3%采用预热，剩余步骤采用余弦衰减。

**与目前先进的方法对比**

除了 OpenCompass 的8个基准外，我们增加了ScienceQA、MME、LLaVA-Wild 和 ReadWorldQA 更详细地与目前领域内最先进的模型进行比较。POINTS的性能可与现有类似尺寸的模型媲美，甚至超越更大参数量的模型性能，如 Cambrian-34B。此外，与其他模型相比，POINTS 使用的预训练数据集和指令调优数据集更少且是公开可用的，我们下面将从各角度对
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。