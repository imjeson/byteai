---
title: '开源视频版GPT-4o？快速记忆，实时问答，拿下CVPR'24长视频问答竞赛冠军'
date: 2024-07-10
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

张颢继：清华大学本科生，师从唐彦嵩老师，主要研究方向为视频理解。
王逸钦：清华大学深圳国际研究生院硕士生，师从唐彦嵩老师，主要研究方向为多模态学习。
唐彦嵩博士：清华大学深圳国际研究生院助理教授，主要研究方向为计算机视觉与模式识别。
刘镛：清华大学深圳国际研究生院博士生，师从唐彦嵩老师，主要研究方向为视觉分割、视频理解。
冯佳时博士：字节跳动视觉研究的负责人，他的研究领域包括深度学习及其在计算机视觉中的应用。
代季峰博士：清华大学电子工程系副教授，主要研究方向为视觉基础模型与自动驾驶。
项目 Leader，靳潇杰博士：现就职于字节跳动美国研究院，研究方向包括多模态基础模型、生成式学习、视频编辑等。

基于 ChatGPT、LLAMA、Vicuna 等大语言模型（Large Language Models，LLMs）的强大理解、生成和推理能力，多模态大模型（Large Multimodal Models，LMMs）在图片视觉理解任务上取得了成功，如 MiniGPT-4、LLAVA 等等。更进一步地，一些工作将 LMM 强大的图片理解能力迁移到视频领域，使得视频内容理解和推理成为可能，例如 Video-ChatGPT、Vista-LLaMA 等。

然而，大多数多模态模型仅能对较短的离线视频数据进行文本描述或问答，对于长视频和在线视频流的理解能力比较有限。让模型具有理解长视频的能力是通往更智能的模型甚至达到 AGI 的路径。这一研究空白限制了多模态大模型在许多在线场景中的实际应用，如具身人工智能、智能监控系统等。

针对这点，一些工作开始研究如何增强对长视频的理解能力，大多基于帧采样和特征融合的方法。然而，现有的方法存在以下缺点：1) 显存开销和回答延迟随输入帧数量增长，这为长视频理解带来困难，只能使用稀疏采样等方式，而这会显著影响模型性能。2) 无法处理在线视频流，只能将在线视频流进行分段处理，难以处理新输入的视频片段与旧视频片段之间的信息交互，阻碍了 LMM 对长视频流整体的理解能力。

为了解决此问题，字节跳动联合清华大学的研究人员仿照人类的感知和记忆机制，提出了首个针对长视频流的在线理解多模态大模型 Flash-VStream。

在具体介绍它之前，先来体验一下 Flash-VStream 的实时问答能力：[实时问答链接]。

我们可以看到...
...（具体内容略）

Flash-VStream 不仅在多个长视频理解 benchmark 上表现优秀，还获得了 CVPR'24 长视频问答竞赛 Long-Term Video Question Answering Challenge @ CVPR 2024 Workshop 的冠军。

地址：[比赛地址链接]

更进一步...
...（内容较长，详细信息略）

---

```相关文献```:

- OpenAI. Chatgpt: A language model for conversational AI.
- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ́ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.
- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt qualit
- Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models.
- Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
- Liu, Haotian, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.
- Maaz, Muhammad, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. 
- Ma, Fan, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. VISTA-LLAMA: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens.
- Song, Enxin, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi et al. Moviechat: From dense token to sparse memory for long video understanding.
- Jin, Peng, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding.
- Grauman, Kristen, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger et al. Ego4d: Around the world in 3,000 hours of egocentric video.
- Huang, Qingqiu, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic dataset for movie understanding.

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。