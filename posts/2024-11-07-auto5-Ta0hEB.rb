```
---

title: '腾讯混元又来开源，一出手就是最大MoE大模型'
date: 2024-11-08
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

随着人工智能技术的快速发展，大型语言模型（LLMs）在自然语言处理、计算机视觉和科学任务等领域取得了显著进展。然而，随着模型规模的扩大，如何在保持高性能的同时优化资源消耗成为关键挑战。为了应对这一挑战，腾讯混元团队率先采用混合专家（MoE）模型架构，最新发布的Hunyuan-Large（Hunyuan-MoE-A52B）模型，是目前业界已经开源的基于Transformer的最大MoE模型，拥有389B总参数和52B激活参数。

本次腾讯混元 - Large共计开源三款模型：Hunyuan-A52B-Pretrain，Hunyuan-A52B-Instruct和Hunyuan-A52B-FP8，可支持企业及开发者精调、部署等...

[点击查看完整内容](https://llm.hunyuan.tencent.com/) [GitHub](https://github.com/Tencent/Hunyuan-Large) [HuggingFace](https://huggingface.co/tencent/Hunyuan-Large/tree/main) [技术报告](https://arxiv.org/abs/2411.02265)

...

---
```
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。