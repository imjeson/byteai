---
title: '穹彻智能-上交大最新Nature子刊速递：解析深度学习驱动的视触觉动态重建方案'
date: 2024-11-15
author: ByteAILab

---

随着人形机器人技术的迅猛发展，如何有效获取高质量的操作数据成为核心挑战。鉴于人类操作行为的复杂性和多样性，如何从真实世界中精准捕捉手与物体交互的完整状态，成为推动人形机器人操作技能学习的关键所在。

---
面对这一挑战，穹彻智能携手上海交通大学卢策吾和刘景全团队，创新性地提出了名为 ViTaM 的视觉-触觉联合记录和追踪系统。该系统包括高密度可伸缩触觉手套和基于视觉-触觉的联合学习框架，不仅在触觉手套的设计和制造上实现了技术突破，更通过视觉与触觉的深度融合，为理解手物交互过程状态提供了全新的视角和强大工具。

在人形机器人操作领域，有一个极具价值的问题：鉴于操作数据在人形操作技能学习中的重要性，如何有效地从现实世界中获取操作数据的完整状态？

如果可以，那考虑到人类庞大规模的人口和进行复杂操作的简单直观性与可扩展性，人形机器人再也不用担心没有高质量的操作数据资源了。

穹彻智能携手上海交通大学卢策吾和刘景全团队意识到，分布式触觉技术对于重建完整人类操作至关重要，当操作被遮挡时，触觉可以作为视觉的有效补充，从而一同还原出操作区域的形变状态、接触力位点和大小。因此，该团队提出了一种全新的视觉 - 触觉联合记录和追踪系统 ViTaM（为 Visual-Tactile recording and tracking system for Manipulation 的缩写），包括一个可伸缩的触觉手套，与一个基于视觉 - 触觉的联合学习框架。文章在 24 个物体样本中进行实验，涵盖了 6 个类别，包含刚性物体和可形变物体，重建误差均值仅为 1.8 厘米。

ViTaM 系统在未来发展中，有望被深度集成至机器人的电子皮肤之中，从而赋予机器人与周围环境进行无缝互动的能力。这不仅能够使机器人实时感知并精准响应多样化的环境刺激，更将极大提升其在复杂场景下的灵巧操作水平，推动智能机器人技术迈向更加先进和实用的新阶段。

论文名称：Capturing forceful interaction with deformable objects using a deep learning-powered stretchable tactile array
论文链接：https://www.nature.com/articles/s41467-024-53654-y
项目地址：https://github.com/jeffsonyu/ViTaM

演示视频：[视频链接](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650942987&amp;idx=2&amp;sn=3d32dfd94578c347c15bb89bb3a47dd6&amp;chksm=84e7ea75b3906363c613ac948b5661b628d3569f9af5dcfc62a4a0b45ea10177de9a6b43afcd)

可以看到，对于刚体和可形变物体，系统都能进行高水准的重建，也同时适用于不同类型的物体，如纸杯，橡皮泥，剪刀等日常生活中常见的物体。

ViTaM 方法详解

![图 1: A 人机交互中涉及人类操作的（i）无力交互和（ii）有力交互的任务及其响应结果。B ViTaM 系统概述：(i) 受人类启发的联合感知方法，在操作过程中同时处理跨模态的视觉和触觉信号，以实现状态跟踪；(ii) 可拉伸界面的应变导致的传感误差，它降低了力测量的精度和触觉传感器的应用效果；(iii) 触觉记录方案，包括具有主动应变干扰抑制功能的高密度可拉伸触觉手套，以及用于显式分布式力检测结果的 VR 界面；(iv) 由深度学习驱动的物体状态估计应用，能够重建物体的整体几何形状和接触区域的细粒度表面形变，特别是对于可形变物体。](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9a5B5mYrg2w0SujXJkH7RBZWLgKQTAzd6ZlqDQHCsnFIEojfk8GLx7PlZkptEzCibvT346uo8lYzg/640?wx_fmt=png&amp;from=appmsg)

ViTaM 系统核心挑战是要解决在与可形变物体进行带力交互时如何捕捉细粒度信息，当可形变物体能被正确捕捉时，刚性部件的交互就自然迎刃而解了。

![图 1: A 人机交互中涉及人类操作的（i）无力交互和（ii）有力交互的任务及其响应结果。B ViTaM 系统概述：(i) 受人类启发的联合感知方法，在操作过程中同时处理跨模态的视觉和触觉信号，以实现状态跟踪；(ii) 可拉伸界面的应变导致的传感误差，它降低了力测量的精度和触觉传感器的应用效果；(iii) 触觉记录方案，包括具有主动应变干扰抑制功能的高密度可拉伸触觉手套，以及用于显式分布式力检测结果的 VR 界面；(iv) 由深度学习驱动的物体状态估计应用，能够重建物体的整体几何形状和接触区域的细粒度表面形变，特别是对于可形变物体。](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9a5B5mYrg2w0SujXJkH7RBZWLgKQTAzd6ZlqDQHCsnFIEojfk8GLx7PlZkptEzCibvT346uo8lYzg/640?wx_fmt=png&amp;from=appmsg)

ViTaM 方法详解

![图 1: A 人机交互中涉及人类操作的（i）无力交互和（ii）有力交互的任务及其响应结果。B ViTaM 系统概述：(i) 受人类启发的联合感知方法，在操作过程中同时处理跨模态的视觉和触觉信号，以实现状态跟踪；(ii) 可拉伸界面的应变导致的传感误差，它降低了力测量的精度和触觉传感器的应用效果；(iii) 触觉记录方案，包括具有主动应变干扰抑制功能的高密度可拉伸触觉手套，以及用于显式分布式力检测结果的 VR 界面；(iv) 由深度学习驱动的物体状态估计应用，能够重建物体的整体几何形状和接触区域的细粒度表面形变，特别是对于可形变物体。](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9a5B5mYrg2w0SujXJkH7RBZWLgKQTAzd6ZlqDQHCsnFIEojfk8GLx7PlZkptEzCibvT346uo8lYzg/640?wx_fmt=png&amp;from=appmsg)

ViTaM 方法详解

![图 1: A 人机交互中涉及人类操作的（i）无力交互和（ii）有力交互的任务及其响应结果。B ViTaM 系统概述：(i) 受人类启发的联合感知方法，在操作过程中同时处理跨模态的视觉和触觉信号，以实现状态跟踪；(ii) 可拉伸界面的应变导致的传感误差，它降低了力测量的精度和触觉传感器的应用效果；(iii) 触觉记录方案，包括具有主动应变干扰抑制功能的高密度可拉伸触觉手套，以及用于显式分布式力检测结果的 VR 界面；(iv) 由深度学习驱动的物体状态估计应用，能够重建物体的整体几何形状和接触区域的细粒度表面形变，特别是对于可形变物体。](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9a5B5mYrg2w0SujXJkH7RBZWLgKQTAzd6ZlqDQHCsnFIEojfk8GLx7PlZkptEzCibvT346uo8lYzg/640?wx_fmt=png&amp;from=appmsg)

ViTaM 系统核心挑战是要解决在与可形变物体进行带力交互时如何捕捉细粒度信息，当可形变物体能被正确捕捉时，刚性部件的交互就自然迎刃而解了。

该系统利用一个高密度、可拉伸触觉手套和一个 3D 相机记录操作过程，并利用一个视觉 - 触觉联合学习框架在几何层面上估计手 - 物体的状态。高密度触觉手套最多有 1152 个触觉传感通道分布在手掌上，当与物体交互时，会记录接触区域的手部物体状态，并以 13Hz 的帧速率准确捕捉手物交互过程中可拉伸界面上的力分布和动态。同时，非接触区域的手与物体状态可以由高精度深度摄像头记录。

捕捉到的力测量和点云序列，经过视觉 - 触觉学习模型处理，融合跨模态数据特征，最终实现对不同形变材料的被操作物体的跟踪和几何三维重建。

A. 硬件设计：触觉手套的设计与制造

在高精度触觉反馈系统...

![图 2：触觉手套的具体设计：A. 最大传感通道为 1152 的高密度可拉伸触觉手套的放大示意图；B. (i) 带有两对应变电极、行电极阵列和列电极阵列的触觉传感块的结构；(ii) 显示应变电极位置的放大图；(iii) 显示紧密装配的触觉传感块侧视图。](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9a5B5mYrg2w0SujXJkH7RB9bQJ20cvkialG00l1aLVGebqFRHoQNQ928TxMTew0q16h4apHUv8Frw/640?wx_fmt=png&amp;from=appmsg)

B. 视觉 - 触觉联合学习在人类操作中的应用

在操作可形变物体时，手部与物体接触的力分布能够帮助揭示因形变而发生的几何变化。然而，由于形变区域几乎具备无限的自由度，完全估算物体形变的几何形状一直是一个难题。尽管触觉手套能够测量接触区域的分布力并帮助感知形变，但其覆盖范围仅限于部分物体表面，且即便是高密度、分布式的传感器网络也难以全面捕捉物体的完整几何信息。因此，团队认为，还需要视觉观测来弥补这一不足，从而恢复完整的物体几何形态。此类视觉 - 触觉交互机制与人类的认知过程高度相似。

团队提出了一种视觉 - 触觉联合学习框架，旨在手 - 物体重建和跟踪中恢复物体几何信息，尤其是在高度非刚性形变的情况下。该框架通过结合触觉数据和视觉信息，能够有效重建被手部遮挡或形变的物体细节。为了评估这一框架，团队制作了一个视觉 - 触觉数据集，包括 7680 个样本，涵盖 24 种物体、6 个类别。数据集中包括海绵、橡皮泥、瓶子和杯子等可形变物体，以及折叠架和剪刀等刚性物体。每个物体都进行了 20 次触摸，并通过 16 个不同的摄像头视角进行了记录。训练数据来自 RFUniverse，它支持基于有限元方法（FEM）的仿真，测试数据则来源于实际操作。

![图 3: 该模型包含手部重建器、特征提取器、时间特征融合器和绕数场（WNF）预测器。全局和局部特征均从视觉和触觉输入中提取，并基
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。