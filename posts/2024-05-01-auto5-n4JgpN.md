---
title: 'AI知道苹果是什么吗？DeepMind语言模型科学家正把这些概念变得可量化、可测试'
date: 2024-05-02
author: ByteAILab
---

编辑 | 白菜叶

与计算机科学家 Ellie Pavlick 谈论她的工作——寻找大语言模型 (LLM) 中理解的证据——听起来可能像是在开玩笑。「hand-wavy」这个短语是她最喜欢的，如果她提到「意义」或「推理」，它通常会带有引号。

---


作为一名在布朗大学和 Google DeepMind 研究语言模型的计算机科学家，她知道接受自然语言固有的模糊性是认真对待自然语言的唯一方法。「这是一门科学学科——而且有点脆弱。」她说。

从青春期开始，精确性和细微差别就一直存在于 Pavlick 的世界里，当时她喜欢数学和科学。作为一名本科生，她获得了经济学和萨克斯演奏学位，然后攻读计算机科学博士学位，但她仍然觉得自己在这个领域是个局外人。

「很多人[认为]智能系统看起来很像计算机代码：整洁且方便，就像[我们]擅长理解的许多系统一样。」她说，「我只是相信答案很复杂。如果我有一个简单的解决方案，我很确定它是错误的。我不想犯错。」

一次偶然的机会，Pavlick 遇到了一位从事自然语言处理工作的计算机科学家，于是她开始了她的博士研究课题，研究计算机如何编码语义或语言中的意义。「我认为这很有趣。」她说，「它涉及哲学，这与我目前正在做的很多事情相符。」

现在，Pavlick 的主要研究领域之一集中在「基础」——单词的含义是否取决于独立于语言本身而存在的事物的问题，例如感官知觉、社交互动，甚至思想。

语言模型完全基于文本进行训练，因此它们为探索基础对意义的重要性提供了一个富有成效的平台。但这个问题本身几十年来一直困扰着语言学家和思想家们。

「这些不仅仅是『技术』问题。」Pavlick说，「语言是如此之大，对我来说，感觉它涵盖了一切。」

在这里，媒体与 Pavlick 讨论了这些问题。

**Q：从经验上来说，「理解」或「意义」意味着什么？具体来说，你在寻找什么？**

**A：** 当我在布朗大学开始我的研究项目时，我们认为意义在某种程度上涉及概念。我意识到这是一个理论上的承诺，并不是每个人都会做出这样的承诺，但它看起来很直观。

如果你用「apple」这个词来表示苹果，你就需要一个苹果的概念。无论你是否使用这个词来指代它，它都必须是一件事。这就是「有意义」的含义：需要有一个概念，即你正在用语言表达的东西。

我想在模型中找到概念。我想要一些我可以在神经网络中获取的东西，证明有一个东西在内部代表「苹果」，这使得它可以被同一个词一致地引用。因为似乎确实存在这种内部结构，它不是随机的、任意的。你可以找到这些定义明确的函数的小块，可以可靠地执行某些操作。

我一直专注于描述这种内部结构。它有什么形式？它可以是神经网络内权重的某个子集，或者是对这些权重的某种线性代数运算，某种几何抽象。但它必须在[模型的行为中]发挥因果作用：它与这些输入相关，但与那些输出无关，与这些输出相关，但与那些输出无关。

这感觉就像你可以开始称之为「意义」的东西。这是关于弄清楚如何找到这种结构并建立关系，以便一旦我们将其全部到位，我们就可以将其应用于诸如「它知道『苹果』意味着什么吗？」之类的问题。

**Q：你找到过这种结构的例子吗？**

**A**：是的，有一个研究结果涉及语言模型何时检索一条信息。

![图片](https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmZ4DLrbianTX5yPP3uwFfGVzWImPvXMeSiclWQAkPxHe05UnGvrbRqLDrfUpwbPyRsINIfY9Mn5kYw/640?wx_fmt=png&from=appmsg)

论文链接：[https://arxiv.org/abs/2305.16130](https://arxiv.org/abs/2305.16130)

如果你询问模型「法国的首都是什么」，它需要说「巴黎」，而「波兰的首都是什么」应该回复「华沙」。它很容易记住所有这些答案，并且它们可以分散在[模型内]各处 - 没有真正的理由让它在这些事物之间建立联系。

相反，我们在模型中发现了一个有趣的小地方，它基本上将连接简化为一个小向量。如果将其添加到「法国的首都是什么」，它将检索「巴黎」；如果你问「波兰的首都是什么」，同一个向量将检索「华沙」。就像这个系统的「检索首都城市」向量。

这是一个非常令人兴奋的发现，因为[该模型]似乎是在总结这些小概念，然后对它们应用通用算法。尽管我们正在研究这些非常 [简单] 的问题，但它是为了寻找模型正在使用的这些原始成分的证据。

在这种情况下，摆脱记忆会更容易——在很多方面，这就是这些网络的设计目的。相反，它将[信息]分解为碎片和相关的「原因」。我们希望，当我们提出更好的实验设计时，我们可能会为更复杂的概念找到类似的东西。

**Q：「基础」与这些表述有何关系？**

**A：** 人类学习语言的方式基于大量的非语言输入：你的身体感觉、你的情绪、你是否饿了等等。这被认为对于意义来说非常重要。

但还有其他一些与内部表征更多相关的基础概念。有些词与物质世界没有明显的联系，但它们仍然有意义。像「民主」这样的词就是一个最喜欢的例子。这是你脑子里的一件事：我可以在不谈论民主的情况下思考它。所以基础可能是从语言到那个东西，那个内部表征。

**Q：但你认为，即使是更外在的事物，比如颜色，也可能仍然锚定于内部「概念」表征，而不依赖于感知。那会如何运作呢？**

**A：** 嗯，语言模型没有眼睛，对吧？它对颜色一无所知。所以也许[它捕获]了一些更普遍的东西，比如理解它们之间的关系。我知道当我将蓝色和红色混合起来时，我会得到紫色；这些类型的关系可以定义这种内部[基础]结构。

我们可以使用 RGB 代码 [代表颜色的数字字符串] 向 LLM 提供颜色示例。如果你说「好的，这里是红色」，并给出红色的 RGB 代码，「这是蓝色」，给出蓝色的 RGB 代码，然后说「告诉我紫色是什么」，它应该生成紫色的 RGB 代码。这种映射应该很好地表明模型的内部结构是健全——它缺少[颜色]的感知，但概念结构就在那里。

棘手的是，[模型]只能记住 RGB 代码，这些代码遍布其训练数据。因此，我们「倒转」了所有颜色[远离其真实的 RGB 值]：我们会告诉 LLM，「黄色」一词与代表绿色的 RGB 代码相关联，依此类推。该模型表现良好：当你要求绿色时，它会给你 RGB 代码的倒转版本。这表明其内部颜色表示存在某种一致性。它是应用他们之间关系的知识，而不仅仅是记忆。

这就是「基础」的全部要点。将名称映射到颜色是任意的。更多的是关于他们之间的关系。所以这很令人兴奋。

**Q：这些听起来很哲学的问题怎么可能是科学的呢？**

**A：** 我最近看到了一个思想实验：如果海洋冲到沙子上并且[当它]退潮时，留下的图案会生成一首诗，会怎么样？这首诗有意义吗？这看起来非常抽象，你可以进行很长的哲学辩论。

语言模型的好处是我们不需要思想实验。这不像是「从理论上讲，这样那样的东西会有智能吗？」 只是：这东西有智能吗？它变得科学和可实践。

有时人们会不屑一顾；有一种「随机鹦鹉学舌」方法。我认为这是因为有人担心人们会过度关注这些东西——我们确实看到了这一点。为了纠正这一点，人们会说：「不，这都是骗局。这都是雾里看花。」

这有点帮倒忙。我们发现了一些非常令人兴奋和新颖的东西，值得深入理解它。这是一个巨大的机会，不应该因为我们担心过度解释模型而被忽视。

**Q：当然，你也做出了研究来澄清这种过度解释。**

![图片](https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmZ4DLrbianTX5yPP3uwFfGVMBkTb9PeCkBvMicBYQ38ZaoSMWtbTic1KEGicuWofDLvZxk8XzCm5dMMg/640?wx_fmt=png&from=appmsg)

论文链接：[https://www.semanticscholar.org/paper/Right-for-the-Wrong-Reasons:-Diagnosing-Syntactic-McCoy-Pavlick/42ed4a9994e6121a9f325f5b901c5b3d7ce104f5](https://www.semanticscholar.org/paper/Right-for-the-Wrong-Reasons:-Diagnosing-Syntactic-McCoy-Pavlick/42ed4a9994e6121a9f325f5b901c5b3d7ce104f5)

**A：** 在这项工作中，人们发现了模型所利用的所有「浅层启发法」（以模仿理解）——这些对于我作为一名科学家的成长来说是非常基础的。

但这很复杂。就像，不要太早宣称胜利。[我内心]对评估是否正确有一点怀疑或偏执，即使是我知道我设计得非常仔细的评估！这就是其中的一方面：不要过度宣称。

另一方面是，如果你处理这些[语言模型]系统，你就会知道它们不是人类水平的——它们解决问题的方式并不像看起来那么智能。

**Q：当这个领域有如此多的基本方法和术语存在争议时，你如何衡量成功呢？**

**A：** 我认为，作为科学家，我们正在寻找的是对我们所关心的事物（在本例中为智力）的精确、人类可以理解的描述。然后我们附上文字来帮助我们到达那里。我们需要某种工作词汇。

**A：** 但这很难，因为这样你就可能陷入这场语义之战。当人们问「它有意义吗：是或否？」 我不知道。我们把对话引向了错误的方向。

我试图提供的是对我们关心解释的行为的精确描述。在这一点上，无论你想称之为「意义」还是「表征」，或者任何这些负载词，都没有什么意义。关键是，有一个理论或提议的模型摆在桌面上——让我们对其进行评估。

**Q：那么，语言模型的研究如何才能转向更直接的方法呢？**

**A：** 我真正希望能够回答的深层问题——智力的组成部分是什么？人类的智慧是什么样的？模型智能是什么样的？——真的很重要。但我认为未来 10 年会发生的事情并不是很迷人。

如果我们想要处理这些[内部]表征，我们需要找到它们的方法——科学上合理的方法。如果以正确的方式完成，这种低级的、超级杂乱的方法论的东西就不会成为头条新闻。但这是真
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。