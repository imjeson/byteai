---

title: '专用于法律的两个开源大模型，最高1410亿参数'
date: 2024-09-12
author: ByteAILab

---

法国国家高等教育计算中心、巴黎萨克雷大学的研究人员联合开源了专用于法律领域的大模型——SaulLM。SaulLM一共有540亿、1410亿两种参数，以及基础模型和指令微调两种版本。

---
SaulLM的最大特色是使用了5400亿token的专业法律数据进行了预训练，包括美国、欧洲、澳大利亚等地的法律文本，输出内容的准确率高于很多同类模型。

开源地址：https://huggingface.co/Equall/SaulLM-54-Base
指令微调：https://huggingface.co/Equall/SaulLM-141B-Instruct

![图片来源：由GPTNB生成](http://www.jesonc.com/upload/8FD7B96F5E34993C64020C0DB54F4C00/1726022384851/lnj5jJJrrSmaAH_Jl6guufey7ZVb.png)

SaulLM-54B和SaulLM-141B是基于Mixtral系列模型开发而成，通过引入专家混合（MoE）机制，显著提升了模型处理大量数据的能力。MoE架构的核心思想是将大型模型分解为多个小型专家网络，这些专家可以根据输入数据的不同特点被动态地激活。这种方法不仅提高了模型的计算效率，还增强了模型处理复杂法律文本的能力。

![图片来源：由GPTNB生成](http://www.jesonc.com/Fq1yfGs5YD3Vp0bORjtb2x2nB4Bh)

SaulLM-54B由32层组成，模型维度为4096，隐藏维度为14336；而SaulLM-141B则由56层构成，模型维度增至6144，隐藏维度达到16384。使得两个模型最多能支持长达32768和65536个token的上下文长度。

研究人员使用了分段策略来训练SaulLM模型，包括持续预训练、专业法律指令遵循协议的实施，以及模型输出与人类偏好的对齐。

第一步使用了超过5400亿token的专业法律语料库对模型进行预训练，盖了来自不同法律体系的广泛文本，包括美国、欧洲、澳大利亚等地的法律文献。

在预训练过程中，研究人员采用了AdamW优化器，并设置了特定的学习速率和梯度累积策略，以优化模型的学习效率和稳定性。此外，为了应对模型在训练过程中可能出现的灾难性遗忘等问题，研究团队还引入了重放策略，重新引入早期训练分布中的数据，以增强模型的记忆能力。

法律领域对大模型输出内容的准确性和专业性要求极高。为了提升模型在法律任务上的表现，研究人员使用了专业法律指令遵循协议，训练模型理解和执行法律场景中的指令。

在这一阶段，模型接受了包括法律分析、案件总结、法规解读等多种法律相关任务的训练。通过这种方式，模型学会了如何根据法律专家的需求，提供准确和相关的信息。

![图片来源：由GPTNB生成](http://www.jesonc.com/FigKK0h4U6xxhBEtNu44vpv_iB53)

为了使模型的输出更加符合法律专业人士的期望和偏好，使用了模型输出与人类偏好的对齐方法。主要使用了合成数据和人类反馈来调整模型的输出。合成数据的生成是基于模型的自我对话，模拟法律专家在分析案件时可能提出的问题和答案。通过这种方式，模型能够学习到法律推理的深层逻辑和结构。

同时，研究人员还引入了人类反馈机制，通过评估模型输出的准确性、相关性和逻辑一致性，进一步优化模型的性能。

研究人员在专业法律基准测试平台LegalBench - Instruct 和多基准平台MMLU上对模型进行了综合评估。

![图片来源：由GPTNB生成](http://www.jesonc.com/Fj0AQJIrD1HnpkB-guc5Fv4NZk6x)

实验结果显示， SaulLM – 54B优于 Mixtral - 54B，SaulLM -141B也优于Mixtral - 141B，比GPT-4、Llama-3也更加出色。此外，继续预训练显著增强了模型在法律领域的性能，在 IFT和 DPO阶段都有大约 7% 的显著提升。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。