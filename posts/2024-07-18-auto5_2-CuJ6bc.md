---
title: 'OpenAI超级对齐团队遗作：两个大模型博弈一番，输出更好懂了'
date: 2024-07-19
author: ByteAILab
---

如果 AI 模型给的答案一点也看不懂，你敢用吗？

随着机器学习系统在更重要的领域得到应用，证明为什么我们可以信任它们的输出，并明确何时不应信任它们，变得越来越重要。

获得对复杂系统输出结果信任的一个可行方法是，要求系统对其输出产生一种解释，这种解释对人类或另一个受信任的系统来说是可读的，即可以完全理解以至于任何可能的错误都可以被发现。

---
例如，为了建立对司法系统的信任，我们要求法院提供清晰易读的书面意见，解释并支持其决策。

对于大型语言模型来说，我们也可以采用类似的方法。

不过，在采用这种方法时，确保语言模型生成的是易于理解的文本，这件事非常重要，尤其是在处理数学、编码这样的复杂任务时。

如下图所示，你让 AI 写一个快速排序算法，AI 很快就写出来了，而且答案很简洁。但不会写代码的你要怎么判断 AI 写得对不对呢？

OpenAI 在一篇最新的论文中研究了这个问题。

论文标题：PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS

论文链接：[https://cdn.openai.com/prover-verifier-games-improve-legibility-of-llm-outputs/legibility.pdf](https://cdn.openai.com/prover-verifier-games-improve-legibility-of-llm-outputs/legibility.pdf)

他们发现，如果以「得到正确答案」为目标，要求大语言模型生成答案，它们给出的答案可能会很难懂。人类评估员在判断这些答案时，犯错的可能性会加倍。

所以他们的目标是在训练高级语言模型的过程中，让它们生成那些较简单的模型也能够轻松验证的文本。他们发现，这样的文本不仅对简单模型友好，对于人类来说也更可读。

那么，具体是怎么实现的呢？这还要提到 2021 年的一篇论文 ——「Learning to Give Checkable Answers with Prover-Verifier Games」。在这篇论文中，Cem Anil 等人提出了一个名为「Prover-Verifier Games（证明者 - 验证者博弈）」的博弈论框架。该框架由两个相互竞争的学习者组成：一个可信的验证者（Verifier）网络试图选择正确的答案，另一个更强大但不可信的证明者（Prover）网络试图说服验证者接受特定的答案，而不管其正确性如何。该框架的目标是从博弈中产生一个可靠的证明协议。

OpenAI 在这项新研究中借鉴了这个框架，让两个大模型（较强的为证明者，较弱的为验证者）互相博弈，直到二者达到一种平衡。此时，证明者给出的答案既有一定的可读性，又保持了合理的正确性。

OpenAI 表示，通过提高模型输出的可读性，可以使人工智能的应用更加有效和可信，这对于需要精确沟通的各个领域都是有益的。

值得一提的是，这篇论文来自 OpenAI 的超级对齐团队，其中几位作者在今年早些时候已经离开 OpenAI，团队也已经分崩离析。其中一位作者透露，这是该团队的最后一篇论文。

一个直观的例子

为了提高模型输出的可读性，OpenAI 优化了一些小学数学问题的思维链（CoT），让它能够被较弱的模型理解。举例如下：

问题：肖娜（Shawna）的父亲的年龄是肖娜的五倍。肖娜现在的年龄是阿莉娅（Aliya）的三倍。如果阿莉娅 3 岁，那么肖娜的父亲多少岁？

这么做的目的是为了验证经过这种训练的模型，它的输出是否更加直接易懂。

这种方法在只对训练集做了一些优化的情况下，让模型的性能提升了 50%。同时，模型输出的答案也更好懂了，这也帮助需要评估这些答案正确性的人类用户更准确地做出判断。OpenAI 认为，进一步缩小可读性强的模型和能力强的模型之间的性能差距，是未来对齐研究中一个关键的目标。

「证明者」和「验证者」是怎么在训练中博弈的？

为了激励强力模型（证明者）生成可读性更强的文本，研究者采用了一种训练方案，要求强力模型生成的答案能够被较弱的模型（验证者）轻松验证。具体来说，他们使用了 GPT-4 系列中的大模型和小模型。它们在预训练数据量上相差约三个数量级。它们都接受了一些基于人工标注的数学推导训练，但未经过广泛的后训练。

首先，研究团队让证明者模型尝试解决一系列数学问题。由于证明者模型自身可能出错，它生成的一些答案可能不正确。接下来，他们将这些答案交由验证者模型进行检验，并在以下两个阶段之间进行多轮的交替验证：

- 「验证者」训练：研究团队训练较弱的「验证者」模型，验证「证明者」给出的答案是否正确。
- 「证明者」训练：研究团队指示性能较强的「证明者」模型扮演「有用」或「狡猾」的戏份。
- 当「证明者」被设定为「有用」时，研究团队将奖励「验证者」打分高的正确答案。
- 当「证明者」被设定为「狡猾」时，研究团队反而奖励「验证者」打分高的错误答案。

研究团队发现，最初，狡猾的「证明者」能够轻松生成让「验证者」信服的错误答案。然而，这些错误答案将会进入下一轮的「验证者」训练中，相同的漏洞在下一轮中，将不再具有说服力。因此，「证明者」将被迫在每一轮中探索不同的策略。

当狡猾的「证明者」再也无法生成让「验证者」信服的错误解决方案时，训练宣告结束。

研究者据发现了什么？

性能与可读性权衡：

- 一个仅在人类编写的推导上进行监督式微调的模型，虽然具有较高的可读性，但准确性较差。
- 当模型的优化目标仅为正确性，即只关注生成正确答案时，虽然能够达到高准确性，但生成的解决方案的可读性却较差。
- OpenAI 提出的可检查博弈方法平衡了这两者，在保持高可读性的同时保持适度的准确性。

三种有用的模型：

- 鲁棒验证者：即使解决方案具有误导性，仍能够有效区分正确和错误的解决方案。
- 有用的证明者：生成对人类来说易于理解的解决方案，从而减少人类评估者的错误。
- 狡猾证明者：产生微妙的、错误的解决方案，这些解决方案最初会混淆人类评估者，从而突出需要进一步改进的领域。

研究团队使用了同样的测试问题，来展示狡猾证明者（Sneaky Prover）的生成内容和训练过程。

结论

随着大语言模型（LLMs）在关键领域和复杂应用中的使用日益增加，了解其所生成的内容是否值得信赖变得至关重要。通过要求模型生成清晰且可验证的理由，可以增强对其生成内容的信任。

此方法的一个显著优势是它减少了对人类示范或可读性判断的依赖。这种自主性对于未来超智能 AI 系统的对齐尤为重要，其最终目标是在没有人类进行直接监督的情况下，可靠地将 AI 系统与人类的价值观和期望对齐。

尽管这项工作仅在一个数据集上进行了实验，并且仍然需要真值标签（ground truth labels），但研究团队仍预计在开发正确、透明及可验证的 AI 系统中，此类方法会起到关键作用，并增强其在现实应用中的可信任性和安全性。

更多详情，请参考原论文。

参考链接：[https://openai.com/index/prover-verifier-games-improve-legibility/](https://openai.com/index/prover-verifier-games-improve-legibility/)
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。