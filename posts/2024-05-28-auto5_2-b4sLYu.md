---
title: '清华、华为等提出iVideoGPT：专攻交互式世界模型'
date: 2024-05-29
author: ByteAILab

---

近年来，生成模型取得了显著进展，其中视频生成正在成为一个新的前沿领域。这些生成视频模型的一个重要应用是，在多样化的互联网规模数据上以无监督方式学习，用于构建预测世界模型。

---
这些世界模型有望积累关于世界如何运作的常识性知识，从而能够基于智能体的行为预测潜在的未来结果。 

通过利用这些世界模型，采用基于强化学习的智能体可以在世界模型中进行想象、推理和规划，从而在现实世界中通过少量试验就能更安全、更有效地获得新技能。

尽管生成模型和世界模型有着基本的联系，但用于视频生成的生成模型和用于智能体学习的世界模型的发展之间仍然存在显著的差距。主要挑战之一是如何在交互性和可扩展性之间取得最佳平衡。

在基于模型的强化学习领域，世界模型主要使用循环网络架构。这种设计允许在每一步中基于动作传递观察或潜在状态，从而促进交互行为学习。然而，这些模型大多专注于游戏或模拟环境，数据简单，并且对大规模复杂的 in-the-wild 数据进行建模的能力有限。

相比之下，互联网规模的视频生成模型可以合成逼真的长视频，这些视频可以通过文本描述或未来动作序列进行控制。虽然这样的模型允许高层次的、长期的规划，但它们的轨迹级交互性并没有为智能体提供足够的粒度来有效地学习精确的行为作为基本技能。

来自清华大学、华为诺亚方舟实验室、天津大学的研究者提出了 iVideoGPT（Interactive VideoGPT），这是一个可扩展的自回归 Transformer 框架，它将多模态信号（视觉观察、动作和奖励）集成到一系列 token 中，通过预测下一个 Token 使智能体能够进行交互体验。

iVideoGPT 采用新颖的压缩 tokenization 技术，可有效离散高维视觉观察。利用其可扩展架构，研究者能够在数百万人类和机器人操作轨迹上对 iVideoGPT 进行预训练，从而建立一个多功能基础，可用作各种下游任务的交互式世界模型。该研究促进了交互式通用世界模型的发展。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1t1JmXC9GTbjiaZiaia7avW9hjcqs1awKmCpmxC5YIEXnKHibWS8jml3J8g/640?wx_fmt=png&from=appmsg)

- 论文地址：https://arxiv.org/pdf/2405.15223
- 论文标题：iVideoGPT: Interactive VideoGPTs are Scalable World Models

**方法**

在这一部分，研究团队介绍了一种可扩展的世界模型架构 ——iVideoGPT，其具有极高的灵活性，能够整合多模态信息，包括视觉观察、动作、奖励以及其他潜在的输入。

iVideoGPT 的核心包括一个压缩 tokenizer，用于离散化视频帧，以及一个自回归 transformer，用于预测后续 token。通过在多样化的视频数据上进行预训练，该模型可以获得广泛的世界知识，然后有效地迁移到下游任务中。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1oVGbM8OZxsFVZXsMPhW2KElrSFbbbJhrURibhRiaWIsJj1MTrBViaFxeg/640?wx_fmt=png&from=appmsg)

**架构**

压缩 tokenization。Transformer 在处理离散 token 序列方面表现特别出色。VQGAN 是一种常用的视觉 tokenizer，用于将原始像素转换为离散 token。研究者提出用一种由双编码器和解码器 {(E_c, D_c),(E_p, D_p)} 组成的新型条件 VQGAN 对视频进行 token 化。

如图3a所示，初始上下文帧包含丰富的上下文信息，通过N个token独立地进行token化和重构。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1R4UUawCibDicXyeJxrBnJWzvRHpUgicEsJJwdNiaVQ5RFnfQicvbm8QGTJw/640?wx_fmt=png&from=appmsg)

相比之下，由于上下文帧和未来帧之间存在时间冗余，只有必要的变化信息如移动对象的位置和姿态，才需要被编码。上述过程是通过使用条件编码器和解码器实现的。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1icVtYzFlqKc3aZkwbLD6WwBmibBcYMeUZYFWicp0XJ3MwogO7LTloRjtw/640?wx_fmt=png&from=appmsg)

研究人员通过在多尺度特征图之间使用交叉注意力来实现条件机制。总的来说，tokenizer 是通过以下目标进行训练的：

1. 首先，它显著减少了token化后视频的序列长度，该长度随帧数的增加而线性增长，但增长率n要小得多；
2. 其次，通过条件编码，预测后续token的transformer可以更容易地保持上下文的时间一致性，并专注于建模必要的动态信息。

Transformer 的可交互预测。Token化后，视频被展平成一系列token。

预训练

大语言模型可以通过next-word预测以自监督的方式从互联网文本中获得广泛的知识。同样，世界模型的无动作（action-free）视频预训练范式将视频预测作为预训练目标，为LLM缺乏的物理世界知识提供互联网规模的监督。

研究人员在这一通用目标上预训练iVideoGPT，应用交叉熵损失来预测后续视频token。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1jmLzeIcHtYupsM7hG8J55Xef2Ws1UHm6LKaWzib4mqKk2RFQTDQ1tYA/640?wx_fmt=png&from=appmsg)

预训练数据。虽然互联网上有大量视频可用，但由于计算限制，研究人员特别为机器人操作领域预训练了iVideoGPT。他们利用来自Open X-Embodiment（OXE）数据集和Something-Something v2（SSv2）数据集的35个数据集的混合，共计150万条轨迹。

**微调**

动作条件与奖励预测。该团队的架构被设计为灵活地整合额外模态以学习交互式世界模型。动作通过线性投影并添加到slot token嵌入中进行整合。对于奖励预测，他们没有学习独立的奖励预测器，而是在每个观察的最后一个token的隐藏状态上添加了一个线性头（linear head）。

这种多任务学习方法可以增强模型对任务相关信息的关注，从而提高控制任务的预测准确性。他们在方程式（3）的交叉熵损失之外，还使用了均方误差损失进行奖励预测。

Tokenizer 适应。研究团队选择更新完整模型，包括tokenizer，以适应下游任务，并发现这一策略比参数高效的微调方法更有效。

很少有文献探讨将VQGAN tokenizer用于特定领域的数据。在这项工作中，由于tokenization将动态信息与上下文条件解耦，并假设虽然此模型可能在下游任务中遇到未见过的对象，如不同类型的机器人，但transformer从多样化场景中学到的基本物理知识 —— 如运动和交互是共享的。

这一假设得到了实验的支持，他们将iVideoGPT从混合预训练数据迁移到未见过的BAIR数据集，其中预训练的transformer可以零样本泛化预测自然运动，仅需对未见过的机器人抓手的tokenizer进行微调。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1AWbRzNWX0apjUAgha9ImFMBBSsFwWS5LqX1ZXOZ9z6IRbxlSnUYASg/640?wx_fmt=png)

**实验**

如表1所示，与SOTA方法相比，iVideoGPT展现出了具有竞争力的性能，同时在其架构中实现了交互性和可扩展性。虽然初步实验是在64×64的低分辨率下进行的，但iVideoGPT可以轻松扩展到RoboNet的256×256。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB135vBnwEL66IIGFTZRpRfVqHsEW6OKZRXy1V1ViasajovvUUtA8ib9GRw/640?wx_fmt=png)

有关定性结果，请参阅图9。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1Jm865nnkweumETAyb7j74H4fSD8ldKzNdBvSgRzFBdibnKgN2HNU9VQ/640?wx_fmt=png)

图4显示了iVideoGPT与基准模型相比的成功率。iVideoGPT在两个RoboDesk任务中大幅优于所有基线，并实现了与最强模型SVG'相当的平均性能。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB135vBnwEL66IIGFTZRpRfVqHsEW6OKZRXy1V1ViasajovvUUtA8ib9GRw/640?wx_fmt=png)

图6显示基于模型的算法不仅比无模型算法提高了样本效率，而且达到或超过了DreamerV3的性能。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1Oxwm0Be31xQ0TO2VCeibss8soRhTchX3zs3Y5VjzialrUycksonqaL4A/640?wx_fmt=png)

接下来该研究分析了大规模预训练iVideoGPT在未见过的BAIR数据集上的零样本视频预测能力。有趣的是，研究者在图7的第二行观察到，iVideoGPT在没有微调的情况下，预测了一个机器人抓手的自然运动 —— 尽管与预训练数据集不同。这表明，尽管由于预训练数据的多样性不足，模型在完全未见过的机器人上的零样本泛化能力有限，但它有效地将场景上下文与运动动态分禦开来。相比之下，使用经过适应的tokenizer，未经过微调的Transformer成功地迁移了预训练知识，并在第三行预测了新型机器人的运动，提供了与第四行中完全微调的Transformer相似的感知质量。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8r7d6fJBwe7lP3o5eFkyB1wM3b7OqYAgjyWFQyjFsYNGSe9BiaoiaGf5icQ9XFicL9YpmsYadiaKKpenw/640?wx_fmt=png)

了解更多结果，请参考原论文。 

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。