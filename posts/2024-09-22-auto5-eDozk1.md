---
title: '首个Mamba+Transformer混合架构多模态大模型来了，实现单卡千图推理'
date: 2024-09-23
author: ByteAILab

---

本文作者来自于香港中文大学深圳和深圳大数据研究院。其中第一作者为香港中文大学深圳博士生王熙栋和研究助理宋定杰，主要研究方向分别为医疗AGI和多模态学习；博士生陈舒年研究方向为多模态学习，博士生张辰研究方向为高效语言模型。

---
通讯作者为香港中文大学深圳数据科学学院王本友教授。

扩展多模态大语言模型（MLLMs）的长上下文能力对于视频理解、高分辨率图像理解以及多模态智能体至关重要。这涉及一系列系统性的优化，包括模型架构、数据构建和训练策略，尤其要解决诸如随着图像增多性能下降以及高计算成本等挑战。

该团队将模型架构调整为 Mamba 和 Transformer 块的混合体，在数据构建中考虑多个图像之间的时间和空间依赖性，并采用渐进式训练策略。提出了首个混合架构多模态大语言模型 LongLLaVA，在效率和性能之间实现了更好的平衡。

LongLLaVA 不仅在各种基准测试中取得了有竞争力的结果，还保持了高吞吐量和低显存消耗，其可以在单个 A100 80GB GPU 上处理近千张图像，展现出了广阔的应用前景。

论文地址：https://arxiv.org/abs/2409.02889
项目地址：https://github.com/FreedomIntelligence/LongLLaVA

1. 简介

多模态大语言模型（MLLMs）的快速进步展示了它们在各个应用领域中的显著能力。然而，多图像理解场景仍然是一个重要但尚未充分探索的方面。特别是，将 MLLMs 的应用场景扩展到理解更长的视频、更高分辨率的图像以及基于更多历史信息的决策，对于提升用户体验和进一步拓展 MLLMs 的应用范围至关重要。

...

6. 结论

LongLLaVA（长上下文大型语言和视觉助手）这一创新性混合架构模型，在长上下文多模态理解方面表现出色。该模型集成了 Mamba 和 Transformer 模块，利用多个图像之间的时空依赖性构建数据，并采用渐进式训练策略。

LongLLaVA 在各种基准测试中表现出竞争性的性能，同时确保了效率，为长上下文多模态大型语言模型（MLLMs）设定了新的标准。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。