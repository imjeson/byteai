---
title: '大科技让人们忽略了人工智能存在的灾难性风险，一位顶尖科学家警告称'
date: 2024-05-26
author: ByteAILab

---

大科技已成功让世界对人工智能对人类存在的灾难性风险转移注意力，一位领先的科学家和人工智能倡导者警告称。

---
在首尔举行的人工智能峰会上，Max Tegmark在接受《卫报》采访时表示，从生命灭绝转向更广泛的人工智能安全概念的关注转移了对最强大程序的开发者实施严格监管的时间，这对人类构成了无法接受的延误。

“1942年，恩里科·费米在芝加哥的一个足球场下建造了第一座具有自持核链反应的反应堆，”作为一名受过物理训练的物理学家，Tegmark说道。“当那时的顶尖物理学家发现这一点时，他们真的很害怕，因为他们意识到建造核弹所剩余的最大障碍刚刚被克服。他们意识到距离只有几年时间 - 实际上，是三年，1945年的三一试验。“能够通过图灵测试（一个人无法在对话中辨别是否在与另一个人交谈）的人工智能模型是对那种你可能失控的人工智能的警告。这就是为什么现在出现像Geoffrey Hinton和Yoshua Bengio这样的人物 - 甚至是很多科技公司的CEO，至少在私下里 - 开始感到恐慌。”

Tegmark领导的非营利组织“未来生命研究所”去年呼吁在这些担忧背景下暂停六个月的高级人工智能研究。他说，去年三月OpenAI推出的GPT-4模型是地雷，证明了风险非常接近。尽管包括Hinton和Bengio在内的数千名专家签署了这封信件，这两位是当今机器学习领域的三位“创始人”之一，开创了现在该领域的方法。但没有人同意暂停。

相反，人工智能峰会，首尔是继去年十一月英国布莱切利公园后的第二次，成为AI监管的新兴领域。“我们希望这封信将对话合法化，并且我们对其效果感到非常高兴。一旦人们看到像Bengio这样的人担心，他们就会想，‘我担心也没关系。’ 就连我加油站的那个人在那之后对我说，他担心AI会取代我们。但现在，我们需要从口头支持转为实际行动。”

然而，自从最初宣布成为布莱切利公园峰会的焦点后，国际人工智能监管的注意力已经从灾难性风险转移开来。在首尔，只有三个“高层次”小组中的一个直接关注安全，它看到了“从隐私违规到工作市场扰乱和潜在的灾难性结果”的“完整风险光谱”。Tegmark认为，将严重风险淡化是不健康的 - 并且并非偶然。

“这正是我预测会因为行业游说而发生的事情，”他说。“1955年，第一篇关于吸烟导致肺癌的期刊文章出来了，你会认为很快就会有一些监管出台。但没有，直到1980年才有，因为行业对此进行了极大的分散。我感觉现在就是这样发生的。”

“当然，人工智能现在也会造成危害：存在偏见，它伤害边缘化群体... 但正如[英国科技与技术大臣]米歇尔·多内兰本人所说的，这并不意味着我们不能同时应对。这有点像说，‘让我们不要关注气候变化，因为今年可能会有飓风，所以我们应该专注于这场飓风。’”

Tegmark的批评者也针对他自己的主张提出了相同的观点：行业希望让每个人谈论未来的假设风险，以转移对现实中的具体危害的注意力，他对此指控进行了否认。“即使就其本身而言，这也是非常天马行空的：对于像OpenAI老板Sam Altman这样的人来说，为了避免监管，告诉所有人可能会有一场人人灯火管制的灾难，然后试图说服我们这样的人发出警报，这将是相当的四位元棋局。”相反，他认为，一些科技领导者的有限支持是因为“我认为他们都感到他们陷入了一个无法挣脱的困境，即使他们想要停下来，也无能为力。如果一个烟草公司的CEO一个早上醒来，感觉自己在做的事情不对，会发生什么？他们会换掉CEO。所以，唯一能够确保安全的方式是政府为所有人建立安全标准。”

探索更多关于这些主题的内容
- 人工智能（AI）
- 技术行业
- 计算
- 意识
- 新闻

分享此内容

重用这份内容。注意：Title、Date、Body 三个部分的内容，放入到对应的位置，Title部分内容需要翻译为中文。最后只需要输出为Makedown源文件格式内容。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。