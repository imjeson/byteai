---

title: 'KAYTUS发布升级版MotusAI以加速LLM部署'
date: 2025-06-15
author: ByteAILab

---

精简推理性能、工具兼容性、资源调度和系统稳定性，以快速推进大型AI模型的部署。![图片](https://ai-techpark.com/wp-content/uploads/KAYTUS-1.jpg){ width=60% }

---

KAYTUS，领先的端到端AI和液冷解决方案提供商，今天在ISC High Performance 2025上宣布发布其MotusAI AI DevOps平台的最新版本。升级版MotusAI平台在大型模型推理性能方面提供了显著的增强，并且与多个开源工具的广泛兼容，覆盖大型模型的整个生命周期。为了实现统一和动态的资源调度，该平台显著提高了大型AI模型开发和部署中的资源利用率和运营效率。MotusAI的最新版本将进一步加速AI的采用，并推动教育、金融、能源、汽车和制造等关键领域的商业创新。

随着大型AI模型在现实应用中的日益嵌入，企业正大规模部署这些模型，以在各种行业中产生实际价值。然而，许多组织在AI采用过程中仍面临重大挑战，包括延长的部署周期、严格的稳定性要求、分散的开源工具管理和低计算资源利用率。为了应对这些痛点，KAYTUS推出了最新版本的MotusAI AI DevOps平台，旨在简化AI部署、增强系统稳定性并优化大型模型操作的AI基础设施效率。

增强的推理性能以确保服务质量
部署AI推理服务是一项复杂的任务，涉及服务部署、管理和持续健康监测。这些任务需要在模型和服务治理、通过加速框架进行性能调优以及长期服务稳定性方面严格标准，这通常需要大量的人力、时间和技术专长的投资。升级版MotusAI提供了强大的大型模型部署能力，使可见性和性能完美对齐。通过整合优化框架，如SGLang和vLLM，MotusAI确保高性能、分布式推理服务，使企业能够快速、自信地部署。旨在支持大参数模型，MotusAI利用智能资源和网络亲和性调度，加速上市时间，同时最大化硬件利用。其内置的监控能力跨越整个栈——从硬件和平台到Pods和服务——提供自动故障诊断和快速服务恢复。MotusAI还支持基于实时使用和资源监控的推理工作负载动态扩展，从而增强服务稳定性。

全面的工具支持以加速AI采用
随着AI模型技术的快速演变，支持的开发工具生态系统也在不断复杂化。开发人员需要一个精简的、通用的平台，以高效选择、部署和操作这些工具。升级版MotusAI提供对多种领先开源工具的广泛支持，使企业用户能够按需配置和管理其模型开发环境。通过内置工具如LabelStudio，MotusAI加速数据标注和不同类别之间的同步，提高数据处理效率，加速模型开发周期。MotusAI还为整个AI模型生命周期提供了集成工具链。这包括用于数据标注和治理的LabelStudio和OpenRefine、用于微调大型模型的LLaMA-Factory、用于大型模型应用开发的Dify和Confluence，以及用于文本到图像生成的Stable Diffusion。这些工具结合在一起，赋予用户快速采用大型模型的能力，并在规模上提升开发生产力。

在同一个节点上进行混合训练-推理调度以最大化资源效率
高效利用计算资源依然是AI初创公司和中小型企业在AI采用初期的一个重要优先事项。传统的AI集群通常将计算节点分别分配给训练和推理任务，限制了两种工作负载之间资源调度的灵活性和效率。升级版MotusAI通过在单个节点上启用训练和推理工作负载的混合调度，克服了传统限制，允许无缝集成和动态协调不同任务类型。配备先进的GPU调度能力，MotusAI支持按需资源分配，使用户能够根据工作负载要求高效管理GPU资源。MotusAI还拥有多维GPU调度功能，包括细粒度的划分和对多实例GPU（MIG）的支持，满足模型开发、调试和推理的广泛用例。MotusAI的增强调度器显著优于社区版本，在大规模POD部署中实现了5倍的任务吞吐量提升和5倍的延迟减少，支持数百个POD的快速启动和环境准备，同时支持训练和推理的动态工作负载扩展和潮汐调度。这些能力使得在各种实际AI场景中实现无缝任务协调成为可能。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。