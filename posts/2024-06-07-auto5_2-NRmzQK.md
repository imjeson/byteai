---

title: 'Ilya参与，OpenAI给GPT-4搞可解释，提取了1600万个特征，还能看它怎么想'
date: 2024-06-08
author: ByteAILab

---

大模型都在想什么？OpenAI 找到了一种办法，能给 GPT-4 做「扫描」，告诉你 AI 的思路，而且还把这种方法开源了。

大语言模型（LLM）是当前 AI 领域最热门的探索方向，吸引了大量的关注和研究投入。

---
它们强大的语言理解能力和生成能力在各种应用场景中都表现出巨大潜力。虽然我们见证了大模型迭代后性能上的显著提升，但我们目前对模型中的神经活动仍然只是一知半解。

本周四，OpenAI 分享了一种查找大量「特征」的全新方法 —— 或许这会成为可解释的一种可用方向。OpenAI 表示，新方法比此前的一些思路更具可扩展性，研究团队使用它们在 GPT-4 中找到了 1600 万个特征。

有趣的是，从作者列表中，我们发现已经从 OpenAI 离职的 Ilya Sutskever、Jan Leike 等人也是作者之一。

可谓是一项重要的研究。

论文标题：Scaling and evaluating sparse autoencoders
论文地址：https://cdn.openai.com/papers/sparse-autoencoders.pdf
代码：https://github.com/openai/sparse_autoencoder
特征可视化：https://openaipublic.blob.core.windows.net/sparse-autoencoder/sae-viewer/index.html

**解释神经网络**

作为机器学习模型，神经网络通过使用模仿生物神经元协同工作的过程来识别现象并得出结论，然而长久以来，我们并不真正了解神经网络的内部运作原理。神经网络并不是直接设计的，研究人员设计了训练它们的算法。由此产生的神经网络还不能很好地被理解，并且不能轻易地分解为可识别的部分。这意味着我们不能像推理汽车安全那样推理人工智能安全。

为了理解和解释神经网络，首先需要找到用于神经计算的有用构建块。然而，语言模型内的神经激活是以不可...

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。