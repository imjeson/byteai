---
title: 'OpenAI和谷歌DeepMind员工在公开信中警告AI行业风险'
date: 2024-06-05
author: ByteAILab

---

一个由知名人工智能公司的现任和前任员工组成的团体在周二发布了一封公开信，警告该行业缺乏安全监督，并呼吁加强对吹哨人员的保护。

---
这封公开信呼吁"关于人工智能的警告权"，是一个由该行业内部员工发出的关于AI危险性的最公开声明之一。来自OpenAI的11名现任和前任员工签署了这封信，谷歌DeepMind的两名现任或前任员工也签署了这封信，其中有一位曾在Anthropic工作。

"AI公司拥有大量有关其系统能力和局限性、其保护措施的充分性以及不同类型伤害风险水平的非公开信息。然而，它们目前只有弱弱的义务将这些信息中的一些与政府分享，与民间社会则没有任何义务。我们认为它们无法指望自愿分享这些信息。" 公开信中写道。

OpenAI在一份声明中为其做法辩护，称公司拥有报告公司问题的渠道，直到有适当的保障措施才发布新技术。谷歌没有立即回应置评请求。

"我们为提供最先进和最安全的AI系统的良好纪录感到自豪，相信我们的科学方法可以解决风险问题。我们认为，鉴于这项技术的重要性，严格的辩论至关重要，我们将继续与全球各地的政府、民间社会和其他社区进行交流。" 一位OpenAI发言人说道。

对人工智能潜在危害的担忧已存在数十年，但近年来AI的繁荣加剧了这些恐惧，使监管机构不得不赶紧跟上技术进步的步伐。尽管AI公司公开表示承诺安全开发技术，研究人员和员工警告称，随着AI工具加剧现有社会危害或创造全新危害，缺乏监督。

本周二来自当前和前AI公司员工的公开信首次被《纽约时报》报道，呼吁增加对决定发表安全顾虑的先进AI公司员工的保护。该信呼吁对透明度和问责制的四项原则做出承诺，其中包括公司不会强迫员工签署任何禁止发表与风险有关的AI问题的贬低言论的非诋毁协议，以及为员工匿名向董事会分享顾虑建立机制。

"在这些公司没有有效政府监督的情况下，现任和前任员工是为数不多可以向公众追责的人。然而，广泛的保密协议阻止我们表达关切，除非是向那些可能未能解决这些问题的公司。" 该信称。

OpenAI等公司还采取了积极的手段，阻止员工自由谈论他们的工作，据Vox报道，上周OpenAI要求离职员工签署极具限制性的非诋毁和保密文件，否则将失去所有已归属股权。OpenAI的首席执行官Sam Altman在报道后道歉，表示将更改员工离职程序。

这封公开信发布之后，两名OpenAI的顶尖员工，联合创始人Ilya Sutskever和关键安全研究员Jan Leike，上个月辞职。在他离职后，Leike声称OpenAI放弃了"安全文化"，转而追求"闪亮产品"。

周二的公开信呼应了Leike之前的一些言论，表示公司并未展示透明运营的义务。

探索更多关于这些话题的内容：
人工智能（AI）
OpenAI
谷歌
ChatGPT
Alphabet
DeepMind
Sam Altman
新闻

分享
重新使用这份内容

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。