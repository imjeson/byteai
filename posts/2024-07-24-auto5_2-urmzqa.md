---
title: '击败GPT-4o的开源模型如何炼成？关于Llama 3.1 405B，Meta都写在这篇论文里了'
date: 2024-07-25
author: ByteAILab

---

经历了提前两天的「意外泄露」之后，Llama 3.1 终于在昨夜由官方正式发布了。Llama 3.1 将上下文长度扩展到了 128K，拥有 8B、70B 和 405B 三个版本，再次以一已之力抬高了大模型赛道的竞争标准。

---
对 AI 社区来说，Llama 3.1 405B 最重要的意义是刷新了开源基础模型的能力上限，Meta 官方称，在一系列任务中，其性能可与最好的闭源模型相媲美。下表展示了当前 Llama 3 系列模型在关键基准测试上的性能。可以看出，405B 模型的性能与 GPT-4o 十分接近。与此同时，Meta 公布了《The Llama 3 Herd of Models》论文，揭示了 Llama 3 系列模型迄今为止的研究细节。

[Llama3 论文亮点]
1、在使用 8K 上下文长度进行预训练后，Llama 3.1 405B 使用 128K 上下文长度进行连续训练，且支持多语言和工具使用。
2、与以前的 Llama 模型相比，Meta 加强了预处理和预训练数据的 Curation pipelines，以及后训练数据的质量保证和过滤方法。Meta 认为，高质量基础模型的开发有三个关键杠杆：数据、规模和复杂性管理。

...

更多技术细节，可参考原论文。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。