---
title: '腾讯混元又来开源，一出手就是最大MoE大模型'
date: 2024-11-07
author: ByteAILab
---

![](https://image.jiqizhixin.com/uploads/editor/93edddff-5754-4dcf-a2c2-092e0c9fb9bd/640.png)

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

随着人工智能技术的快速发展，大型语言模型（LLMs）在自然语言处理、计算机视觉和科学任务等领域取得了显著进展。然而，随着模型规模的扩大，如何在保持高性能的同时优化资源消耗成为关键挑战。为了应对这一挑战，腾讯混元团队率先采用混合专家（MoE）模型架构，最新发布的 Hunyuan-Large（Hunyuan-MoE-A52B）模型，是目前业界已经开源的基于 Transformer 的最大 MoE 模型，拥有 389B 总参数和 52B 激活参数。

本次腾讯混元 - Large 共计开源三款模型：Hunyuan-A52B-Pretrain，Hunyuan-A52B-Instruct 和 Hunyuan-A52B-FP8，可支持企业及开发者精调、部署等不同场景的使用需求，可在 HuggingFace、Github 等技术社区直接下载，免费可商用。通过技术优化，腾讯混元 Large 适配开源框架的精调和部署，具有较强的实用性。腾讯云 TI 平台和高性能应用服务 HAI 也同步开放接入，为模型的精调、API 调用及私有化部署提供一站式服务。

![](https://image.jiqizhixin.com/uploads/editor/723df067-05af-402b-8a66-ddabe0a9db60/640.png)

- 开源官网：[https://llm.hunyuan.tencent.com/](https://llm.hunyuan.tencent.com/)
- github（开源模型工具包）：[https://github.com/Tencent/Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large)
- huggingface（模型下载）：[https://huggingface.co/tencent/Hunyuan-Large/tree/main](https://huggingface.co/tencent/Hunyuan-Large)
- huggingface demo 地址：[https://huggingface.co/spaces/tencent/Hunyuan-Large](https://huggingface.co/spaces/tencent/Hunyuan-Large)
- 技术报告：[https://arxiv.org/abs/2411.02265](https://arxiv.org/abs/2411.02265)

**Hunyuan-Large 整体模型效果**

公开测评结果显示，腾讯混元 Large 在 CMMLU、MMLU、CEval、MATH 等多学科综合评测集以及中英文 NLP 任务、代码和数学等 9 大维度全面领先，超过 Llama3.1、Mixtral 等一流的开源大模型。

![](https://image.jiqizhixin.com/uploads/editor/ead20f45-3601-4485-ae45-06f999da5b6d/640.png)

**技术创新点**

MoE (Mixture of Experts)，也即混合专家模型，MoE 模型的每一层都包含多个并行的同构专家，一次 token 的前向计算只会激活部分专家。MoE 模型的每一层会采用路由算法，决定了 token 会被哪些专家处理。MoE 是一种稀疏的网络结构，具有比激活总参数量同等大小稠密模型更优越的性能，而推理成本却远低于总参数量相同的稠密模型。

得益于 MoE (Mixture of Experts) 结构的优越性，混元 Large 可以在保证模型推理速度的同时，显著提升模型的参数量进而提升模型性能。

---

Continue the rest of the text in the same format...


Please continue the same format for the remaining content...
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。