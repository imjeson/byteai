---

title: '为百亿参数LLM化学应用提供新范式，香港理工大学提出上下文分子微调'
date: 2024-05-31
author: ByteAILab

---

图片链接展示：
- ![图片](https://image.jiqizhixin.com/uploads/editor/341992eb-7949-4f1f-8974-0996ce5d2428/640.jpeg)

作者 | 香港理工大学李佳潼
编辑 | ScienceAI

去年，香港理工大学研究团队开发了一个基于检索的提示范式 MolReGPT，利用大型语言模型探索分子发现，在分子和分子文本描述之间进行翻译。

近日，香港理工大学、上海交通大学和上海人工智能实验室联合发表了题为《Large Language Models are In-Context Molecule Learners》的文章，也是MolReGPT的续作。

---


论文链接：https://arxiv.org/abs/2403.04197
模型链接：https://huggingface.co/phenixace/

介绍

近来，大语言模型在生物化学领域展现出了优异的性能，尤其是分子-描述翻译任务，这减小了自然语言描述与分子空间的差异。

但是，之前的方法要么是需要进行大量的领域预训练，要么是在分子与描述的对齐上存在过于粗糙的问题，要么是对大语言模型的能力和参数规模有着严苛的要求。

为了解决这些问题，我们提出了上下文分子微调（ICMA），作为一个让大语言模型学习分子-描述对齐的新方法。

具体来说，上下文分子微调由以下三个部分组成：混合模态上下文召回，召回后重排，和上下文分子微调。

- 首先，混合模态上下文召回沿用了BM25和分子图召回分别对分子描述和分子进行召回，以增强模型的输入。
- 此外，为了解决召回中存在的不准确、样本间过度重复等问题，我们设计了两个召回后处理方法：序列反转和随机游走，以提升召回结果的质量。
- 最后，上下文分子微调借助于大语言模型的上下文学习能力，生成最终的分子/分子描述。

我们在ChEBI-20和PubChem324K两个分子-分子描述数据集上展开实验，实验结果表明，上下文分子微调可以使得Mistral-7B在分子-分子描述上取得SOTA或接近的结果，无需领域预训练和复杂的模型结构。

我们的贡献在于：上下文分子微调不需要额外的领域预训练就可以发挥作用，为最新最先进的十亿甚至百亿参数大语言模型在化学任务上的运用提供了新思路。与此同时，上下文分子微调通过学习上下文例子中分子描述片段与分子SMILES结构的对应关系，精细化了分子描述和分子的对齐。此外，上下文分子微调不那么依赖于模型的上下文学习和推理能力，即便是稍小的模型也能获得良好的性能增益。

方法

分子的相似性原理指出，相似的分子一般会具有相似的性质，如图1所示。借助于分子SMILES和分子描述片段的对应，我们可以基于这些共现信息大致推理出，如果有另一个相似的分子，它的SMILES片段会对应哪些相应分子描述片段，即具有什么样的结构和化学性质。

图1: 三个相似分子以及他们的分子描述。分子可以被表示为SMILES表达式或者分子图，而分子描述说明了分子的特征。这里三个分子在图结构上是相似的，分子描述中重叠的地方被蓝色和粉色标出。

因此，在这篇文章...
...
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。