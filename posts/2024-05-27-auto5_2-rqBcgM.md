---
title: 'ICML 2024 | 脱离LoRA架构，训练参数大幅减少，新型傅立叶微调来了'
date: 2024-05-28
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

本文介绍了**香港科技大学（广州）**的一篇关于大模型高效微调（LLM PEFT Fine-tuning）的文章「Parameter-Efficient Fine-Tuning with Discrete Fourier Transform」，本文被 ICML 2024 接收，代码已开源。

论文地址：https://arxiv.org/abs/2405.03003

项目地址：https://github.com/Chaos96/fourierft

背景

大型基座模型在自然语言处理（NLP）和计算机视觉（CV）领域都获得了瞩目的成就。微调（Finetuning）大型基座模型，使其更加适应特殊的下游任务，成为了一项热门研究课题。然而，在模型越来越大，下游任务越来越多样的...

方法

傅立叶基底在各类数据压缩应用中广泛使用，例如一维向量信号和二维图像的压缩。在这些应用中，稠密的空域信号通过傅立叶变换被转化为稀疏的频域信号。基于这一原理，作者推测模型权重的增量也可以被视为一种空域信号，其对应的频域信号可以通过稀疏表示来实现。

在这一假设的基础上...

实验

1. 自然语言理解

作者在自然语言理解的 GLUE 基准测试上对傅立叶微调方法进行了评估。基线对比方法包括全量微调（FF，Full Finetuning）、Bitfit、适应器微调（Adapter Tuning）、LoRA、DyLoRA 和 AdaLoRA。下表展示了各种方法在 GLUE 各个任务上的表现及其所需的训练参数量。结果表明，傅立叶微调以最少的参数量达到了甚至超越了其他微调方法的性能。

2. 自然语言指令微调

大模型的自然语言生成是目前模型微调的重要应用领域。作者在 LLaMA 系列模型、MT-Bench 任务和 Vicuna 任务上评估了傅立叶微调的性能。结果显示，傅立叶微调以极低的训练参数量达到了与 LoRA 相似的效果，...

3. 图像分类

作者在 Vision Transformer 上测试了傅里叶微调的性能，涵盖了 8 个常见的图像分类数据集。实验结果表明，虽然在图像分类任务中傅立叶微调相较LoRA的压缩率提升并不比自然语言任务中显著，但其仍然以远小于 LoRA 的参数量超越了 LoRA 的效果。这进一步展示了傅立叶微调在不同应用领域中的有效性和优势。

4. 突破低秩

在 GLUE 基准的 RTE 数据集上，FourierFT 可以实现明显高于 LoRA (通常为 4 或 8) 的增量的秩。

5.GPU 资源消耗

微调过程中，FourierFT 可以实现比 LoRA 更少的 GPU 消耗。下图为采用单张 4090 显卡在 RoBERTa-Large 模型上的巅峰内存消耗。

结论

作者介绍了一种名为傅立叶微调的高效微调方法，通过利用傅里叶变换来减少大基础模型微调时的可训练参数数量。该方法通过学习少量的傅里叶谱系数来表示权重变化，显著降低了存储和计算需求。实验结果显示，傅立叶微调在自然语言理解、自然语言生成、指令调优和图像分类等任务上表现优异，与现有的低秩适应方法（如 LoRA）相比，傅立叶微调在保持或超过 LoRA 性能的同时，所需的可训练参数大幅减少。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。