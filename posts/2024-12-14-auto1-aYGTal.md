---
title: '因为一个AI生成的评分，她未能获得公寓——并起诉以帮助他人避免同样的命运'
date: 2024-12-15
author: ByteAILab

---

三百二十四。

---
这是玛丽·路易斯（Mary Louis）被一个AI驱动的租户筛选工具给出的评分。该软件SafeRent并没有解释其11页报告中评分的计算方式或各因素的权重。报告中也没有说明这个评分的具体含义，只是显示了路易斯的数字，并判断她的评分太低。在结果旁边的一个框中，报告写道：“评分建议：拒绝”。路易斯在马萨诸塞州东部的一个郊区申请了一间公寓。她在参观单位时，管理公司表示她的申请应该没问题。尽管她的信用评分较低且有一些信用卡债务，但是她有一份来自17年房东的杰出推荐，房东表示她始终按时支付租金。她还将使用低收入租户的补助金，这确保了管理公司将至少收到政府支付的部分月租金。她的儿子也在该补助金名单上，且信用评分较高，表明他能够作为支付缺失的后援。然而，在2021年5月，路易斯收到了一封电子邮件，告知她的申请被一款计算机程序拒绝。她需要至少获得443的评分，才能被接受申请。没有进一步解释，也没有上诉的机会。

“玛丽，我们遗憾地通知您，我们用来筛选所有潜在租户的第三方服务拒绝了您的租赁申请，”电子邮件写道。“不幸的是，该服务的SafeRent租赁评分低于我们租赁标准的许可范围。”

路易斯被迫租住一间更贵的公寓，那里没有算法评分。但是，她了解到，自己与SafeRent的经历并不是独一无二的。她是马萨诸塞州超过400名使用住房券的黑人和西班牙裔租户之一，他们的租赁申请因为SafeRent的评分而被拒绝。2022年，他们共同起诉该公司，依据《公平住房法》，声称SafeRent对他们的歧视。路易斯和另一位原告莫妮卡·道格拉斯（Monica Douglas）声称，该公司的算法不成比例地对使用住房券的黑人和西班牙裔租户评分低于白人申请者。他们声称，该软件对他们作为好租户的相关性权重误判，会不准确地考虑不相关的账户信息——如信用评分和非住房相关的债务——却没考虑到他们使用住房券的情况。研究表明，黑人和西班牙裔租赁申请者更可能拥有较低的信用评分和使用住房券。

“等待拒绝的过程完全是浪费时间，”路易斯说。“我知道我的信用不太好。但是AI并不了解我的行为——它知道我在支付信用卡方面落后，但并不知道我总是按时支付租金。”

两年过去了，自该小组首次起诉SafeRent以来，路易斯表示她已经继续生活，几乎忘记了这一诉讼，尽管她是唯一的两位原告之一。然而，她的行动仍可能保护其他使用类似住房计划的人（在美国联邦法规中被称为第8条住房券）免受算法决定的拒绝。SafeRent已与路易斯和道格拉斯达成和解。除了支付230万美元，SafeRent还同意在五年内停止使用任何评分系统或推荐与使用住房券的潜在租户有关的评级。尽管SafeRent在法律上没有承认错误，但这种公司接受对其核心产品进行变更的和解在科技公司中极为罕见；此类和解更常见的结果通常是财务协议。

“虽然SafeRent仍然相信SRS评分符合所有适用法律，但诉讼耗时且成本高昂，”该公司的发言人雅兹敏·洛佩斯（Yazmin Lopez）在一份声明中表示。“越来越明显的是，辩护SRS评分将会占用SafeRent可以更好地用于为住房提供者提供筛选申请工具的时间和资源。”

租户筛选系统如SafeRent通常被视为“避免直接与申请人接触”的一种方式，并将拒绝的责任推给计算机系统，唐德·卡普兰（Todd Kaplan），代表路易斯和起诉该公司的原告之一的律师表示。管理公司告诉路易斯，软件单独决定拒绝她的申请，而SafeRent的报告则表明是管理公司设定了申请人需要获得的最低评分门槛。

“他们输入了一堆信息，而SafeRent则根据自己的评分系统得出结论，”卡普兰说。“这使得人们更难预测SafeRent将如何看待他们。不仅仅是对申请的租户来说，对房东来说也不清楚SafeRent评分的细节。”

根据与路易斯达成的和解（于11月20日获得批准），该公司今后不得使用评分系统或在潜在租户使用住房券的情况中提供接受或拒绝的建议。如果该公司推出新的评分系统，必须由第三方公平住房组织独立验证。

“去掉这种赞成或拒绝的决定，确实允许租户说：‘我是一个优秀的租户’，”卡普兰说。“这使得判断变得更加个性化。”

几乎所有被认为是低收入的9200万人在美国的生活中都已接触到AI决策，涉及就业、住房、医疗、教育或政府援助等基本方面，法律援助协会的律师凯文·德·利班（Kevin de Liban）在一份关于AI危害的新报告中指出。作为一家名为TechTonic Justice的新AI公正组织的创始人，德·利班在2016年开始调查这些系统时，曾受到来自阿肯色州的残疾患者的求助，后者突然因为自动化决策减少了州资助的家庭护理小时数。德·利班提到的一个例子是，该州的医疗补助分配依赖的程序判断一名患者的脚没有任何问题，尽管他已被截肢。

“这让我意识到，我们不应该将AI系统视为一种超理性的方法来做决策，”德·利班说。他表示，这些系统基于“垃圾统计科学”做出各种假设，导致称之为“荒谬”的结果。

2018年，德·利班代表这些患者起诉阿肯色州人类服务部，因该部门的决策过程，州立法机构裁定该机构不得再自动化决定患者的家庭保健分配。德·利班的诉讼是对算法决策造成的危害的早期胜利，尽管它在其他领域，如就业，仍在全国范围内继续存在。

尽管有缺陷，限制AI使用的法律少之又少，而对受自动化决策影响的人的问责途径也有限。根据消费者报告在7月发布的一项调查，显示大多数美国人对AI和算法决策技术在住房、就业和医疗等重大生活事件中的应用感到“不安”。受访者表示，对于AI系统评估他们所使用的信息缺乏了解感到不安。

与路易斯的案件不同，许多人在算法用于做出生活决策时并未收到通知，这使得他们很难对这些决定进行上诉或质疑。

“我们现有的法律可能有用，但在能带来什么效果方面是有限的，”德·利班说。“市场力量在穷人身上无效。所有的动机都指向产生更多低劣技术，而缺乏刺激公司为低收入人群提供好的选择。”

在乔·拜登总统任内，联邦监管机构曾多次尝试赶上快速发展的AI行业。总统发布了一项行政令，其中包括一项框架，旨在解决与AI系统相关的国家安全和歧视风险。然而，唐纳德·特朗普承诺将推翻这些工作，并削减规制，包括拜登的AI行政令。这可能使像路易斯这样的诉讼成为AI问责的一个重要渠道。此案已经引起美国司法部和住房和城市发展部的关注——这两个机构处理影响受保护群体的歧视性住房政策。

“如果这是一个里程碑式的案件，它可能为如何看待这些案件提供道路图，并鼓励其他挑战，”卡普兰说。尽管如此，在缺乏监管的情况下，保持这些公司的问责将是困难的，德·利班表示。诉讼需要时间和金钱，公司可能会找到办法构建变通或类似的产品，以避免被集体诉讼覆盖。“你不能每天都提起这些案件，”他说。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。