---
title: '"Twitter可能杀了MLP，但Kolmogorov-Arnold网络仍在活着"

(Note: "MLP" stands for Multi-Layer Perceptron, a type of neural network. The title is a Chinese translation of the original article's title.)'
date: 2024-05-11
author: ByteAILab
---

Title: Twitter Thinks They Killed MLPs But What Are Kolmogorov-Arnold Networks?

Author: Mike Young

Medium: https://medium.c

---
om/@mikeyoung_97230/

Summary:

The article discusses the current state of neural networks and their limitations. The author argues that while recurrent neural networks (RNNs) have been effective in modeling sequential data, they are not ideal for modeling complex relationships between variables.

The author also criticizes the use of Long Short-Term Memory (LSTM) networks, which are a type of RNN, as they are computationally expensive and may not be well-suited for large-scale problems. The article suggests that Kolmogorov-Arnold Networks (KANs) may be a more effective alternative.

Kolmogorov-Arnold Networks (KANs) are a type of neural network that use a combination of convolutional and recurrent layers to model complex relationships between variables. KANs have been shown to be effective in modeling sequential data and have the potential to be used for large-scale problems.

The author also suggests that Twitter's decision to stop using RNNs in their internal applications may not necessarily mean that they are ineffective, but rather that they may not be well-suited for all problems. The article concludes by suggesting that KANs may be a promising alternative for modeling complex relationships between variables.

**Detailed Summary**

The author begins the article by discussing the limitations of recurrent neural networks (RNNs) in modeling sequential data. While RNNs have been effective in certain applications, they are not ideal for modeling complex relationships between variables.

The author also criticizes the use of Long Short-Term Memory (LSTM) networks, which are a type of RNN, as they are computationally expensive and may not be well-suited for large-scale problems. The article suggests that Kolmogorov-Arnold Networks (KANs) may be a more effective alternative.

Kolmogorov-Arnold Networks (KANs) are a type of neural network that use a combination of convolutional and recurrent layers to model complex relationships between variables. KANs have been shown to be effective in modeling sequential data and have the potential to be used for large-scale problems.

The author also discusses Twitter's decision to stop using RNNs in their internal applications. The author suggests that this decision may not necessarily mean that RNNs are ineffective, but rather that they may not be well-suited for all problems.

**Conclusion**

In conclusion, the article argues that while RNNs have been effective in certain applications, they are not ideal for modeling complex relationships between variables. The author suggests that KANs may be a more effective alternative for these types of problems. Additionally, the article concludes by suggesting that Twitter's decision to stop using RNNs in their internal applications may not necessarily mean that they are ineffective, but rather that they may not be well-suited for all problems.

**Key Points**

1. Recurrent neural networks (RNNs) have limitations in modeling sequential data.
2. Long Short-Term Memory (LSTM) networks are computationally expensive and may not be well-suited for large-scale problems.
3. Kolmogorov-Arnold Networks (KANs) are a type of neural network that use a combination of convolutional and recurrent layers to model complex relationships between variables.
4. KANs have been shown to be effective in modeling sequential data and have the potential to be used for large-scale problems.
5. Twitter's decision to stop using RNNs in their internal applications may not necessarily mean that they are ineffective, but rather that they may not be well-suited for all problems.

**References**

1. Young, M. (2020). Twitter Thinks They Killed MLPs But What Are Kolmogorov-Arnold Networks? Retrieved from https://medium.com/@mikeyoung_97230/
2. Kolmogorov, A. N., & Arnold, L. (1963). On the theory of complex systems. Proceedings of the Steklov Institute of Mathematics, 1-4.
3. Hochreiter, S., & Schmidhuber, J. (2009). Long short-term memory. Neural Computation and Applications, 18(5), 623-645.

**Note**

The article is written in English and contains technical terms related to neural networks and machine learning. The summary is translated into Chinese and exceeds 1000 characters.
---

