---

title: '协作研究人工智能安全至关重要'
date: 2025-01-10
author: ByteAILab

---

关于乔治·辛顿对人工智能（“人工智能教父”）可能给人类带来的危险的担忧（《卫报》，12月27日），我相信通过协作研究人工智能安全，可以最好地缓解这些担忧，监管机构的参与也是不可或缺的。

---
目前，前沿人工智能在开发后使用“红队”进行测试，力图引发负面结果。然而，这种方法永远不够；人工智能需要在设计与评估中融入安全性，而这可以借鉴在成熟安全相关行业中的专业知识和经验。 

辛顿似乎认为，人工智能的生存威胁并不是某种被故意编码的情形——那么，为什么不强制避免这种可能？虽然我并不赞同他对人类面临风险的看法，但谨慎原则提示我们必须立即行动。在传统的安全关键领域，构建物理系统（如飞机）的需求限制了安全受到影响的速度，而前沿人工智能没有这样的物理“速率限制”。这是监管需要发挥作用的地方。理想情况下，部署前应进行风险评估，但当前的风险指标是不足的——例如，它们未考虑应用领域或部署规模。 

监管机构需要具备“召回”已部署模型的权力（大型开发公司也需要包括停止特定用途的机制），并支持风险评估的工作，提供风险的前瞻性指标，而不仅仅是滞后指标。换句话说，政府需要关注市场后的监管控制，同时支持研究，使监管机构能够拥有强有力的信息，以实施市场前的控制。这虽然具有挑战性，但如果辛顿关于人类面临的风险是正确的，那么这也是势在必行的。
Prof John McDermid
约克大学安全自主研究所

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。