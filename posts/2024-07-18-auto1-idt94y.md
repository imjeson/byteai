---
title: 'AI技术越来越强大，专家警告抓捕儿童色情罪犯难度加大'
date: 2024-07-19
author: ByteAILab

---

专家警告称，使用人工智能生成儿童色情图像的罪犯数量已经超过了执法机构识别和救助真实受害者的能力。

---
检控官和从事打击儿童犯罪工作的儿童安全团体表示，由人工智能生成的图像已经变得如此逼真，以至于在某些情况下很难确定真实儿童是否已被用于制作。单个人工智能模型可以在很短的时间内生成数以万计的新图像内容，这些内容开始涌入暗网并渗入主流互联网。

“我们开始看到一些图像报告显示，图像中的孩子是真实的，但却是人工智能生成的，这些孩子并没有遭受性虐待。但现在他们的脸被放在了被虐待的孩子的身上。”明尼苏达州儿童安全非营利组织Zero Abuse Project的高级律师Kristina Korobov说道。部分情况下，我们通过视频或图像中的床品或背景、作案者或所属系列来辨认，但现在又加入了另一个孩子的脸”。

每年已经有数千万份实际儿童性虐待材料（CSAM）的报告，儿童安全团体和执法部门难以进行调查。

“我们已经被这些内容淹没了。”美国司法部的一名检察官匿名表示。“从执法角度看，针对儿童的犯罪是资源匮乏的领域之一，而从AI生成的内容将出现爆炸性增长。”

去年，国家失踪与被剥削儿童中心（NCMEC）接到了多份关于罪犯以多种方式使用AI的报告，例如输入文本提示来生成虐童图像，修改先前上载的文件使其具有性虐待特征，以及上载已知的CSAM并根据这些图片生成新的图像。在一些报告中，罪犯们借助聊天机器人指导他们如何找到未成年儿童进行性行为或伤害。

专家和检察官认为，罪犯试图通过使用生成式人工智能来修改儿童性虐待受害者的图像以躲避检测。

“在联邦系统中提起案件时，AI并不会改变我们能够起诉的内容，但在许多州，你必须能够证明这是一个真实的孩子。就图像的合法性进行争论将在审判中造成问题。如果我是一名辩护律师，这正是我会提出的。”司法部检察官表示。

美国联邦法律规定持有虐待儿童图像是犯罪行为，今年已有几起在美国被逮捕的据称持有被识别为AI生成的CSAM的嫌疑人。然而，在大多数州，没有法律禁止持有以AI生成的描绘未成年人的性行为图像。目前的法律并未覆盖图像的创作行为。

然而，华盛顿州的立法机构在今年三月通过了一项法案，禁止持有AI生成的CSAM和故意泄露他人的以AI生成的亲密图像。四月，国会提出了一项旨在刑事化生成AI CSAM的草案，该草案得到了全国检察长协会的支持。儿童安全专家警告称，AI内容的涌入将耗尽NCMEC CyberTipline的资源，该中心是全球儿童虐待报告的处理中心。该组织在确定地理位置、优先级别和受害者是否已知后，会将这些报告转发给执法机构进行调查。

“警方现在有更多的内容需要处理。他们怎么知道这是一个需要救援的真实孩子？你不知道。这是一个巨大的问题。”加拿大儿童保护中心的研究和分析主管Jacques Marcoux表示。

已知的虐待儿童图像可以通过图像的数字指纹（称为哈希值）来识别。NCMEC保持着超过500万个哈希值的数据库，图像可以与之匹配，这对于执法机构而言是一项关键工具。

当上传已知的虐待儿童图像时，运行监控此类活动的软件的技术公司有能力根据哈希值拦截并阻止这些图像，并将用户报告给执法机构。

不能通过已知哈希值匹配的材料，例如新生成的内容，对于这种类型的扫描软件来说是不可识别的。使用AI对图像进行任何编辑或修改都会改变其哈希值。

“哈希匹配是防线，”Marcoux说。“使用AI生成的每张图像都被视为全新图像，并具有不同的哈希值。这侵蚀了现有防线的效率。它可能会破坏哈希匹配系统。”

儿童安全专家将AI生成的CSAM不断升级追溯到2022年底，与OpenAI发布ChatGPT和向公众介绍生成式AI同时发生。早在那一年，LAION-5B数据库推出，这是一个包含超过50亿张图像的开源目录，任何人都可以使用它来训练AI模型。斯坦福研究人员在2023年底发现，曾被检测出的虐待儿童图像包含在该数据库中，这意味着在该数据库上训练的AI模型可以生成CSAM。儿童安全专家们强调，在生成大部分、如果不是全部CSAM时，儿童已经受到了伤害。

“每次将CSAM图片输入AI系统，它就会学到一项新技能。”Zero Abuse Project的Korobov表示。

当用户将已知的CSAM上传到其图像工具时，OpenAI会对其进行审查并向NCMEC报告，该公司的一位发言人表示。

“我们在减少我们的模型生成危害儿童内容的潜力方面付出了巨大努力，”该发言人表示。

在2023年，NCMEC收到了3620万份关于在线虐待儿童的报告，比去年增加了12%。大多数收到的线索与被虐待儿童的真实照片和视频相关。然而，他们还收到了4700份关于AI生成虐待儿童图像或视频的报告。

NCMEC指责AI公司没有积极尝试阻止或检测CSAM的制作。去年只有五个生成式AI平台向该组织发送了报告。超过70%的AI生成CSAM报告来自社交媒体平台，这些平台被用于分享材料，而不是AI公司。

“有许多网站和应用可以用于创建此类内容，包括开源模型，尚不清楚他们是否与CyberTipline合作，或者是否采取其他安全措施，”NCMEC CyberTipline主管Fallon McNulty表示。

考虑到AI使罪犯能够在很短的时间内轻松生成成千上万张新的CSAM图像，儿童安全专家预计他们的资源将面临越来越大的压力，以应对儿童被剥削问题。NCMEC表示，他们预计AI将对其CyberTipline的报告产生激增。

这种预计的报告激增将影响受害者的识别和营救，威胁着已经资源匮乏且不堪重负的执法领域，儿童安全专家表示。

罪犯惯常在点对点平台上与他们的社区分享CSAM，使用加密消息应用程序来逃避检测。

Meta公司在去年12月将Facebook Messenger加密，并计划在Instagram上也加密消息，这一举措引起了儿童安全团体的强烈反对，他们担心每年在其平台上发生的数百万案件中的许多案例现在将不被察觉。

Meta还在过去一年内在其社交网络中引入了大量生成式AI功能。生成的AI图片已成为社交网络上最受欢迎的内容之一。

Meta的一位发言人在接受《卫报》采访时表示：“我们对儿童裸露、虐待和剥削等内容有详细而严格的政策，包括由GenAI创建的CSAM和涉及GenAI创建的材料。我们会向NCMEC报告所有明显的CSAM情况，以符合我们的法律义务。”

儿童安全专家表示，开发AI平台的公司和立法者应该主要负责阻止AI生成的CSAM的传播。

“在向市场推出之前，设计工具以确保它们无法用于制作CSAM至关重要，”McNulty表示。“不幸的是，正如我们在一些开源生成式AI模型上所看到的，当公司没有遵循安全建设时，可能会造成无法逆转的巨大后果。”

此外，Korobov表示，可能用于交换AI生成CSAM的平台需要分配更多资源用于检测和报告。

“这将要求更多的人工审核员查看图像，或者进入人们交换这类材料的聊天室和其他服务器，查看其中的内容，鉴别其中是否也包含儿童性虐待材料；这些都是新生成的。”她说。“您将不得不亲眼看一看，并认识到这也是虐待儿童的材料，只是新制造的。”

同时，主要社交媒体公司通过裁员削减了用于扫描和报告儿童虐待的工作人员。

“如果主要公司不愿意在CSAM检测方面做基础工作，为什么我们要认为他们会在没有监管的情况下在这个AI世界中采取所有这些额外的措施呢？”洛杉矶儿童安全团体HEAT Initiative的CEO Sarah Gardner表示。“我们已经看到，仅凭自愿是行不通的。”

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。