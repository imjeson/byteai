---
title: '仅用4块GPU、不到3天训练出「开源版GPT-4o」，这是国内团队最新研究'
date: 2024-09-24
author: ByteAILab

---

LLaMA-Omni能够接收语音指令，同步生成文本和语音响应，响应延迟低至 226ms，低于 GPT-4o 的平均音频响应延迟 320ms。

以 ChatGPT 为代表的大型语言模型（LLM）已成为强大的通用任务解决器，但大多数 LLM 仅支持基于文本的交互，这限制了它们在不适合文本输入输出的场景中的应用。

---
GPT-4o 的出现使得通过语音与 LLM 进行交互成为可能。然而，开源社区对于构建此类基于 LLM 的语音交互模型仍然缺乏探索。

实现与 LLM 进行语音交互最简单的方法是采用基于自动语音识别 (ASR) 和语音合成 (TTS) 模型的级联系统，其中 ASR 模型将用户的语音指令转录为文本， TTS 模型将 LLM 的响应合成为语音。

然而，由于级联系统依次输出转录文本、文本响应和语音响应，整个系统往往具有较高的延迟。相比之下，一些多模态语音 - 语言模型将语音离散化为 token 并扩展 LLM 的词表以支持语音输入和输出。这种语音 - 语言模型理论上可以直接从语音指令生成语音响应，无需生成中间文本，从而实现极低的响应延迟。然而，在实践中，由于涉及语音之间复杂的映射，直接语音到语音的生成通常极具挑战性。

为了解决上述问题，来自中国科学院计算技术研究所、中国科学院大学的研究者提出了一种新型模型架构 ——LLaMA-Omni，它可以实现与 LLM 的低延迟、高质量交互。

LLaMA-Omni 由语音编码器、语音适配器、LLM 和流式语音解码器组成。用户的语音指令由语音编码器进行编码，经过语音适配器后输入到 LLM。LLM 直接从语音指令中解码文本响应，无需首先将语音转录为文本。语音解码器是一个非自回归（NAR）流式 Transformer，它将 LLM 的输出表示作为输入，并使用连接时序分类（Connectionist Temporal Classification, CTC）来预测与语音响应相对应的离散单元序列。

在推理过程中，当 LLM 自回归生成文本响应时，语音解码器同步生成相应的离散单元。为了更好地契合语音交互场景的特点，该研究通过重写现有的文本指令数据并进行语音合成，构建了名为 InstructS2S-200K 的数据集。实验结果表明，LLaMA-Omni 可以同步生成高质量的文本和语音响应，延迟低至 226ms。

此外，与 SpeechGPT 等语音 - 语言模型相比，LLaMA-Omni 显著减少了所需的训练数据和计算资源，从而能够基于最新的 LLM 高效开发强大的语音交互模型。

LLaMA-Omni 的模型概览以及语音编码器、语音适配器、LLM 和语音解码器的工作原理也得到了详细阐述。

论文：[https://arxiv.org/pdf/2409.06666](https://arxiv.org/pdf/2409.06666)
代码：[https://github.com/ictnlp/LLaMA-Omni](https://github.com/ictnlp/LLaMA-Omni)
模型：[https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni](https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni)
论文标题：LLaMA-Omni: Seamless Speech Interaction with Large Language Models

---

...

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。