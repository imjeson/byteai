---
title: 'AI出图更快、更美、更懂你心意，高美感文生图模型修炼了哪些技术秘籍？'
date: 2024-08-13
author: ByteAILab

---

随着大模型的落地按下加速键，文生图无疑是最火热的应用方向之一。自从 Stable Diffusion 诞生以来，海内外的文生图大模型层出不穷，一时有「神仙打架」之感。

---
短短几个月，「最强 AI 画师」的称号几次易主。每一次技术迭代，都不断刷新着AI图像生成质量和速度的上限。

于是现在，我们输入几个文字就能得到任何想要的画面。无论是专业级别的商业海报，还是超写实画风的写真照片，AI 制图的逼真程度已经让我们叹为观止。甚至 AI 赢下了 2023 年度的索尼世界摄影奖。在大奖公布之前，这幅「照片」已经在伦敦萨默赛特宫进行展览——如果作者不公开说明，可能没有人会发现这张照片实际出自 AI 之手。

![Eldagse和他的AI生成作品《电工》](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5ZpUPKovSMJmZlnNEzsxGlp2cD2ZAYKwN4yS7ia1p7dpjmMa9xf7p0iaA/640?wx_fmt=png&from=appmsg)

如何让 AI 画出来的图片更具美感，这离不开 AI 技术人员持之以恒的付出。第六期的《AIGC体验派》就邀请到了豆包文生图技术专家李亮、NVIDIA 解决方案架构师赵一嘉，为我们深入剖析了文生图模型出图更美、更快、更懂用户心意背后的技术链路。

直播开始，李亮首先详细拆解了近期国产大模型「顶流」—— 字节跳动豆包大模型在文生图模型方面的技术升级。

李亮表示，豆包团队想解决的问题主要包含三个方面：一是如何实现更强的图文匹配来满足用户的想法设计；第二个是如何生成更具美感的图像来提供更极致的用户体验；第三个是如何更快速地出图来满足超大规模的服务调用。

在图文匹配方面，豆包团队从数据入手，对海量图文数据做精细化筛选和过滤，最终入库了千亿量级的高质量图像。此外，团队还专门训练了一个多模态大语言模型进行 recapiton 任务。这个模型将更加全面、客观地描述图片中图像的物理关系。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5G5RPUGequE6MuzfENxkibLUfzQqibmQEiata2SPTlicyEQErdDE0ChEcCg/640?wx_fmt=png&from=appmsg)

有了高质量高细节的图文对数据之后，想要更好地发挥出模型的实力，还需要提升文本理解模块的能力。团队采用原生双语大语言模型作为文本编码器，显著提升了模型理解中文的能力，因此，面对「唐代」、「元宵节」等国风元素，豆包・文生图模型也展现出了更加深刻的理解力。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto571EMR6G4g4S0Y3r9A6HIaAegxpVp6mmOXoaZqs4icm12CW6dfJUFRoQ/640?wx_fmt=png&from=appmsg)

对于 Diffsuion 模型架构，豆包团队也注入了独门秘籍，他们 UNet 进行了有效地scaling，通过增加参数量，豆包・文生图模型进一步地提升了图像文本对的理解和高保真的生成能力。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5OExnUHnU4LRXIn2lKhwj043850KtvyAARneOHu0CgGbugdYs7j7Vnw/640?wx_fmt=png&from=appmsg)

针对用户直观感受最明显的美学风格，豆包团队引入了专业的美学指导，也时刻关注用户和大众审美的偏好。与此同时，团队也在数据和模型架构上下了一番功夫。很多时候，用户得到的图像和 demo 展示的效果对比好比「买家秀」和「卖家秀」，实际上是给出的 prompt 对于模型来说不够详细和明确，而豆包·文生图模型引入了一个「Rephraser」，在遵循用户原始意图的同时，为提示词增加更多的细节描述，所有用户也将因此体验到更完美的生成效果。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5ISgJenYBjib576qicD3b7YaLKbnfvRwD1Hl2hnmpxZtDibFiatbP7UYYOg/640?wx_fmt=png&from=appmsg)

为了让模型出图速度更快，每张图消耗的成本更低，豆包团队在模型的蒸馏方式上也给出了新的解题思路，一项代表性的成果是 Hyber-SD，这是一种新颖的扩散模型蒸馏框架，在压缩去噪步数的同时可保持接近无损的性能。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5cnnnqUIg5EtmBGicT3WXP5orYWAQyZbcYxNCXWR8jPZPCl5oEOPRoeg/640?wx_fmt=png&from=appmsg)

接下来，英伟达解决方案架构师赵一嘉从底层技术出发，讲解了文生图最主流的基于Unet的SD和DIT两种模型架构及其相应的特性，并介绍了英伟达的Tensorrt, Tensorrt-LLM, Triton, Nemo Megatron 等工具如何为部署模型提供支持，助力大模型更加高效地推理。

赵一嘉首先分享了 Stable Diffusion 背后模型的原理详解，细致地阐述了 Clip、VAE 和 Unet 等关键组件的工作原理。随着 Sora 爆火，也带火了背后的 DiT（扩散 Transformer）架构。赵一嘉进一步从模型结构、特性和算力消耗三方面，从模型结构、特性和资源消耗三个方面，对 SD 和 DiT 的优势进行了全面的比较。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5Mra6quSyq3tdoQAG06P6LoXvVxaklPPTrsicDsjGfYYNxtDicicj9XZNA/640?wx_fmt=png&amp;from=appmsg)

使用 Stable diffusion 生成图像时，往往会感觉提示词内容在生成结果中都得到了呈现，但图不是自己想要的，这是因为基于文字出图的 Stable diffusion 并不擅长控制图像的细节，例如构图、动作、面部特征、空间关系等。因此，基于Stable diffusion 的工作原理，研究人员们设计了许多控制模块，弥补 Stable diffusion 的短板。赵一嘉补充了其中具有代表性的 IP-adapter 和 ControlNet。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5icDZOS964J3oZ9kpbdBtsCjezLDoTrpZgJotb2OgYRLTyibjk9gQToPQ/640?wx_fmt=png&from=appmsg)

想要加快吃算力的文生图模型的推理速度，英伟达的技术支持发挥了关键作用。赵一嘉介绍了 Nvidia TensorRT 和 TensorRT-LLM 工具，这些工具通过高性能卷积、高效调度和分布式部署等技术，优化了图文生成模型的推理过程。同时，英伟达的 Ada、Hopper 以及即将推出的 BlackWell 硬件架构，都已支持 FP8 训练和推理，将为模型训练带来更加丝滑的体验。

![image](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibv1kAx14e5hrRfl4zwdto5KUicVrBoa8qp0ibuOadFKiajwKChyXlOQicXgERI4ibicmGExItc2FYSe5w/640?wx_fmt=png&from=appmsg)

经历了六场精彩的直播，由火山引擎、NVIDIA 联手机器之心和 CMO CLUB 共同推出的《AIGC体验派》迎来了圆满收官。通过这六期节目，相信大家对 AIGC 如何从「有趣」变为「有用」有了更深的理解。我们也期待着《AIGC 体验派》不止停留在节目的讨论中，并更能在实际中加速营销领域智能化升级的进程。

**《AIGC 体验派》全六期回顾地址**：[https://vtizr.xetlk.com/s/7CjTy](https://vtizr.xetlk.com/s/7CjTy)

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。