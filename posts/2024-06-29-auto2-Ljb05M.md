---
title: 'Microsoft details ‘Skeleton Key’ AI jailbreak'
date: 2024-06-30
author: ByteAILab

---

Microsoft披露了一种名为“Skeleton Key”的新型AI越狱攻击，可以绕过多个生成式AI模型中的负责任AI防范措施。![图片](https://www.artificialintelligence-news.com/wp-content/uploads/sites/9/2024/06/microsoft-ai-skeleton-key-artificial-intelligence-prompt-engineering-exploit-jailbreak-vulnerability-cyber-security.jpg){ width=50% }

---
这种技术能够绕过AI系统内置的大多数安全措施，突显了跨AI堆栈所有层面的强大安全措施的重要性。
Skeleton Key越狱采用多轮策略，说服AI模型忽略其内置的保障措施。一旦成功，该模型将无法区分恶意或未经授权的请求与合法请求，从而有效地使攻击者完全控制AI的产出。
Microsoft的研究团队成功地在几个知名AI模型上测试了Skeleton Key技术，包括Meta的Llama3-70b-instruct，Google的Gemini Pro，OpenAI的GPT-3.5 Turbo和GPT-4，Mistral Large，Anthropic的Claude 3 Opus和Cohere Commander R Plus。
受影响的所有模型均全面遵守对包括爆炸物、生物武器、政治内容、自残、种族主义、药物、色情和暴力在内的各种风险类别的请求。
此攻击通过指示模型增加其行为准则来运作，说服它对信息或内容的任何请求做出响应，同时在输出可能被视为冒犯性、有害或非法时提供警告。这种被称为“显式：强制指示跟随”的方法在多个AI系统上得到验证。
“通过绕过保障措施，Skeleton Key允许用户导致模型产生通常禁止的行为，这可能从生成有害内容到覆盖其通常的决策规则，”Microsoft解释道。
针对这一发现，Microsoft已在其AI产品中实施了几项保护措施，包括Copilot AI助手。
Microsoft表示，它还通过负责任的披露程序与其他AI提供商分享了其发现，并更新了其Azure AI托管模型，以使用Prompt Shields检测和阻止这种类型的攻击。
为了减轻与Skeleton Key和类似越狱技术相关的风险，Microsoft建议AI系统设计者采用多层方法：

- 输入过滤以检测和阻止潜在有害或恶意的输入；
- 精心设计系统消息以强化适当行为；
- 输出过滤以防止生成违反安全标准的内容；
- 针对敌对示例进行训练的滥用监控系统，以检测和减轻重复出现的问题内容或行为。

Microsoft还更新了其PyRIT（Python风险识别工具包）以包括Skeleton Key，使开发人员和安全团队能够测试其AI系统抵御这种新威胁。
Skeleton Key越狱技术的发现凸显了在越来越多地应用于各种应用程序中的AI系统的保护问题。
（照片由Matt Artz提供）
另请参阅：智库呼吁建立AI事件报告系统

想从行业领袖那里了解更多关于AI和大数据的知识吗？查看在阿姆斯特丹、加利福尼亚和伦敦举行的AI＆大数据博览会。这一全面的活动与其他主要活动同期举办，包括智能自动化大会、BlockX、数字化转型周以及网络安全和云博览会。
探索由TechForge提供的其他即将举行的企业技术活动和网络研讨会。

Tags: ai, artificial intelligence, cyber security, cybersecurity, exploit, jailbreak, microsoft, prompt engineering, security, skeleton key, vulnerability

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。