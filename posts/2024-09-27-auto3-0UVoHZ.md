---
title: 'AMD Instinct MI300X加速器现已在OCI上面向苛刻的AI应用程序提供'
date: 2024-09-28
author: ByteAILab
---

Customers including Fireworks AI are powering their AI inference and training workloads with new OCI Compute instances
OCI Supercluster leads among cloud providers with support for up to 16,384 AMD Instinct MI300X GPUs in a single ultrafast network fabric
AMD（NASDAQ: AMD）今天宣布，Oracle Cloud Infrastructure（OCI）已选择AMD Instinct™ MI300X加速器与ROCm™开放软件来驱动其最新的OCI Compute Supercluster实例，名为BM.GPU.MI300X.8. 对于可以包含数百亿参数的AI模型，配备了AMD MI300X的OCI Supercluster支持在单个集群中最多可达16384个GPU，利用了OCI上其他加速器使用的同样超快网络结构技术。![图片](https://ai-techpark.com/wp-content/uploads/2024/09/AMD-Instinct-960x540.jpg){ width=60% }

---
OCI裸金属实例设计用于运行需要高吞吐量、领先的内存容量和带宽的苛刻AI工作负载，包括需要高性能的大语言模型（LLM）推理和训练，这些实例已被Fireworks AI等公司采用。
“AMD Instinct MI300X和ROCm开放软件在为最关键的OCI AI工作负载提供支持方面不断获得动力，”AMD数据中心GPU业务公司副总裁兼总经理Andrew Dieckmann表示。“随着这些解决方案进一步扩展到增长的AI密集型市场，这种组合将使OCI客户受益于高性能、高效率和更大的系统设计灵活性。”
“AMD Instinct MI300X加速器的推理能力增加了OCI丰富的高性能裸金属实例选择，消除了常用于AI基础设施的虚拟化计算的开销，”Oracle Cloud Infrastructure软件开发高级副总裁Donald Lu表示。“我们很高兴为希望以具有竞争力的价格加速AI工作负载的客户提供更多选择。”
为AI训练和推理提供信赖的性能和开放选择AMD Instinct MI300X经过OCI验证的广泛测试强调了其用于服务延迟最佳化用例的AI推理和训练能力，即使使用更大的批处理大小，以及将最大的LLM模型放入单个节点的能力。这些Instinct MI300X性能结果已引起AI模型开发人员的关注。
Fireworks AI提供了一个快速平台，旨在构建和部署生成式AI。Fireworks AI利用OCI上AMD Instinct MI300X的性能优势，拥有超过100个模型。
“Fireworks AI帮助企业在各种行业和用例中构建和部署复合AI系统，”Fireworks AI首席执行官林乔表示。“AMD Instinct MI300X和ROCm开放软件上可用的大容量内存使我们能够根据模型的持续增长扩展为客户提供服务。”

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。