---
title: '又一届「AI春晚」拉开序幕！智源大模型集体爆发了'
date: 2024-06-17
author: ByteAILab

---

每一年的智源大会不仅是分享前沿 AI 科技成果的平台，更为国内外业内人士提供了一个交流的舞台。果然，2024 智源大会又为我们带来了诸多惊喜。

---


一年一度的国内「AI 春晚」—— 智源大会又一次拉开了序幕。

20+ 个不同主题的论坛、百场精彩报告让现场和线上的观众目不暇接，切实感受到了当下 AI 尤其是大模型对内容创作、生产办公、机器人、生物医疗等千行百业的深度赋能。

在这场「AI 内行顶级盛会」上，不仅有 Llama、Sora 等大模型及 DiT 架构的作者参与交流，详解他们的研究成果；还有百度、零一万物、百川智能、智谱 AI、月之暗面、面壁智能等耳熟能详的国内大模型公司的 CEO 与 CTO 展开对话，探讨人工智能关键技术路径以及通往 AGI 之路。

同时，在 AI 安全这个愈加受到全球关注的课题上，包括图灵奖得主姚期智、加州大学伯克利分校教授 Stuart Russell、谷歌 DeepMind 前沿安全与治理主任 Allan Dafoe 等一众国内外大咖嘉宾分享洞见，为人工智能接下来的健康、可持续发展建言献策。

自 2019 年 10 月举办首届以来，智源大会已经来到第六个年头。每一年，智源研究院都会在大会上发布诸多重磅进展。

在今天的智源大会上，我们又一次看到了多项「全球首个」研究进展：

智源研究院带来了持续迭代后覆盖面更全、性能更强大、影响更深远的大模型全家桶，包括语言大模型、多模态大模型、具身大模型、生物计算大模型。新技术的亮相预示着智源始终走在大模型发展前列，引领大模型研究方向。

训练万亿级大模型仅需四个月和 112 台 A800

在过去一年，Scaling Law 被研究者们反复提及和议论。这个定律揭示了一点：随着模型参数、训练数据量和计算量的持续增加，模型的性能通常会持续提升。正因此，大模型的参数每年都在指数级提升。

科学家们普遍认为人类大脑参数在万亿到 1000 万亿之间，而大模型参数与人类大脑参数的差距在不断缩小，从过去几年的相差 100 万倍到 1000 倍再到最近的仅差 100 倍。按照这个速度发展，未来几年，大模型参数很可能就会赶上或者超过人类大脑的参数，AGI 也会更快到来。

但人们同时意识到，算力的短缺将成为一大挑战。

为此，智源研究院与中国电信人工智能研究院（TeleAI）基于模型生长和损失预测等技术联合研发了全球首个低碳单体稠密万亿语言模型「Tele-FLM-1T」。该模型与百亿级的 52B 版本、千亿级的 102B 版本共同构成 Tele-FLM 系列，团队用 4 个月完成了 3 个模型总计 2.3T tokens 的训练，训练全程做到了零调整零重试，算力能效高且模型收敛性和稳定性好。

作为一个万亿级参数的模型，Tele-FLM-1T 仅需业界普通训练方案 9% 的算力资源和 112 台 A800 服务器。

Tele-FLM 系列模型在基础性能方面取得多项突破：BPB 显示，英文能力上，Tele-FLM-52B 接近 Llama3-70B，优于 Llama2-70B 和 Llama3-8B；中文能力上，Tele-FLM-52B 为开源最强，优于 Llama3-70B 和 Qwen1.5-72B。在对话模型性能方面：AlignBench 评测显示，Tele-FLM-Chat（52B）已经达到 GPT-4 中文语言能力的 96%，总体能力达到 GPT-4 的 80%。

大会上，智源研究院院长王仲远博士宣布，TeleFLM 系列中，52B 版本已经全面开源，包括所有的核心技术（生长技术、最优超参预测）、训练细节（loss 曲线、最优超参、数据配比和 Grad Norm 等），Tele-FLM-1T 版本也即将开源。

- Tele-FLM-52B 版本开源地址：https://huggingface.co/CofeAI/Tele-FLM
- Tele-FLM-Chat 试用（纯模型单轮对话版）地址：https://modelscope.cn/studios/FLM/ChatFLM

对于 AI 研究者们来说，同样关心的还有模型的幻觉问题。针对这个问题，智源研究院推出了通用语义向量模型 BGE（BAAI General Embedding）。

自 2023 年 8 月发布以来，智源团队陆续发布了中英文模型 BGE v1.0、v1.5 以及多语言模型 BGE-M3。截至目前，BGE 系列模型全球下载量超过 1500 万，位居国内开源 AI 模型首位。BGE-M3 模型一度跃居 Hugging Face 热门模型前三，其所属代码仓库 FlagEmbedding 位居 Github 热门项目前 10；BGE-M3 所带来的全新的通用检索模式也相继被 Milvus、Vespa 等主流向量数据库集成。

同时，智源再度推出新一代检索排序模型 BGE Re-Ranker v2.0，同时扩展了向量模型 BGE 的「文本 + 图片」混合检索能力。

模型地址：https://github.com/FlagOpen/FlagEmbedding

打造原生多模态世界模型

8B 小模型可达 GPT-4o 87% 性能

如今，行业主流多模态大模型多为针对不同任务而训练的专用模型，例如文生图的 Stable Diffusion，文生视频的 Sora，图生文的 GPT-4V，每类模型都有对应的架构和方法。

这就导致现有模型的能力多为单一分散的能力组合，而非原生的统一能力，比如 Sora 目前就做不到对图像和视频的理解。此次，智源研究院推出了 Emu3 原生多模态世界模型，让我们看到了不一样的选择。

值得关注的是，在技术路线上，智源研究院没有像行业其他玩家一样选择因 Sora 而爆火的 DiT 路线。Emu3 采用了智源自研的多
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。