---
title: 'ECCV 2024 | 比基准高30%，媲美Gemini 1.5 Pro，基于记忆的视频理解智能体来了'
date: 2024-09-06
author: ByteAILab

---

AIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。

---
如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com

视频理解仍然是计算机视觉和人工智能领域的一个主要挑战。最近在视频理解上的许多进展都是通过端到端地训练多模态大语言模型实现的。然而，当这些模型处理较长的视频时，内存消耗可能会显著增加，甚至变得难以承受，并且自注意力机制有时可能难以捕捉长程关系。这些问题阻碍了将端到端模型进一步应用于视频理解。

为解决这一问题，北京通用人工智能研究院联合北京大学的研究人员提出了**首个基于记忆和工具使用的视频理解智能体VideoAgent，在视频理解任务上媲美Gemini 1.5 Pro**。该论文已被ECCV 2024接收。[论文链接](https://arxiv.org/abs/2403.11481)，[项目主页](https://videoagent.github.io/)，[代码链接](https://github.com/YueFan1014/VideoAgent)

**模型介绍**
VideoAgent 的主要思想是将视频表示为结构化的记忆，并运用大语言模型的强大推理能力和工具使用能力从记忆中抽取关键信息，实现对视频的理解以及对视频问题的回答。
![图 1：VideoAgent 流程图。](https://image.jiqizhixin.com/uploads/editor/9dd1d320-cf48-4158-9be9-678a7c5cea71/640.png)
VideoAgent 的记忆设计遵循简约原则：作者发现视频中发生的事件以及视频中出现的人和物体能够涵盖最常见的视频问题。因此，作者设计了如图 1 所示的两个记忆组件：1）时间记忆，用于存储每2秒视频片段所发生的事件；2）物体记忆，用于存储视频中出现的人和物体的信息。

给定一个视频，VideoAgent会首先构建该视频的时间记忆和物体记忆。在推理阶段，对于该视频的一个问题，VideoAgent会调用一系列工具，从记忆中抽取与问题有关的信息来回答该问题。
![视频 1：VideoAgent 运用思维链和工具回答视频问题。](https://image.jiqizhixin.com/uploads/editor/218ec7ba-f5a4-41da-941d-ff9fca1076f8/1725550620732.png)

**记忆构建**
对于时间记忆，作者使用预训练的视频文本模型 LaViLa 为每 2 秒的视频片段生成描述文本，反映了视频片段中发生的事件。除了描述片段的文本外，时间记忆还存储了每个视频片段的特征，片段特征包括：1）文本特征，通过使用文本编码器 text-embedding-3-large 得到片段的描述文本的嵌入向量；2）视觉特征，通过使用视频语言模型 ViCLIP 对视频片段进行视觉编码获得的嵌入向量。这些片段特征在推理阶段时可被 VideoAgent 用于定位关键片段。

物体记忆的核心是跟踪 (tracking) 并且重识别 (re-identification) 视频中的所有物体和人物。作者首先使用 RT-DETR 和 Byte-track 对视频进行物体检测和跟踪。然而，仅仅使用物体跟踪算法无法解决同一物体由于在视频中多次出现而被识别成多个物体的情况。因此，作者提出一种基于 CLIP 特征 和 DINO-v2 特征 的物体重识别算法，将物体跟踪结果中相同的物体赋予同一物体 ID。
![视频2：物体重识别效果展示。杯子和牛奶瓶能够在不同位姿下被重识别。](https://image.jiqizhixin.com/uploads/editor/78de379d-1c75-4bea-8606-ff4f368c2ffa/1725550641624.png)

值得一提的是，记忆构建阶段所涉及的所有模型都满足实时性的要求，这使得VideoAgent也能够接受视频流输入，并且实时地构建记忆。最后，物体记忆中存储的内容有：1）包括物体 ID、物体类别和物体所出现的视频片段三个字段的物体数据库；2）物体ID所对应的 CLIP 特征，用以支持在推理阶段的开放词表物体检索。

**视频问答**
为了回答一个视频问题，大型语言模型（LLM）会将其分解为多个子任务，并调用工具来解决。这些工具围绕统一的记忆库运作，主要包括以下几个工具：
1. 片段描述召回：给定两个时刻，返回两个查询时刻之间所有片段的文本描述。
2. 片段定位：给定一个文本描述，通过将该文本特征与所有片段特征进行比较，检索与之最相关的 5 个片段。
3. 视觉问答：给定一个视频片段和问题，利用 VideoLLaVA 工具，根据视频片段回答该问题。
4. 物体记忆查询：给定一个有关视频中物体的问题，结合 SQL 查询功能和基于 CLIP 特征的开放词表物体检索，从物体记忆中进行复杂的物体信息查询。

最后，LLM 会整合这些工具的使用结果，生成对视频问题的回答。
![图 2：VideoAgent 回答视频问题的示例。](https://image.jiqizhixin.com/uploads/editor/5ab5bb7c-4ca7-43f3-aec3-877773ce36ab/640.png)

在关于视频中有几艘船的问题上，端到端的多模态大语言模型由于其视觉特征的缺陷，无法准确回答出视频中船的数量。但 VideoAgent 能借助视觉基础模型的能力以及物体重识别算法得到精确的物体细节并存放到物体记忆中，因此能够准确回答出视频中有 6 艘船。
![图 3：VideoAgent 与多模态大语言模型的对比。](https://image.jiqizhixin.com/uploads/editor/fb9f857d-93d1-46d9-a326-5cbb36477021/640.png)

**实验分析**
作者在 EgoSchema, WorldQA, NExT-QA 三个长视频理解数据集上测试了 VideoAgent 的性能。实验表明 VideoAgent 能够取得比目前开源的多模态大语言模型更好的表现，并且能够与目前最好的闭源模型相媲美。

在 EgoSchema 长视频多选题数据集上，VideoAgent 的 60.2% 的准确率相比基准的多模态大语言模型高出了近 30 个百分点，接近 Gemini 1.5 Pro 的 63.2% 的准确率。在 WorldQA 数据集上，VideoAgent 在选择题和开放问题上都取得了不错的性能，这归功于 VideoAgent 能够结合大语言模型中的常识知识、推理能力以及视频记忆共同实现对于长视频的理解。

在 NExT-QA 数据集上，作者对于 VideoAgent 中的 4 种不同工具进行了消融实验。实验表明片段描述召回对于 VideoAgent 理解视频是十分必要的。物体记忆对于 VideoAgent 在时序、因果、描述三类问题的回答准确率都有提升。片段定位和视觉问答这两个工具对于 VideoAgent 正确回答问题的贡献最大。
![表1：在EgoSchema数据集上的实验结果。](https://image.jiqizhixin.com/uploads/editor/632e7bc3-eb65-495e-a51d-216b7afa40ad/640.png)
![表2: 在WorldQA数据集上的实验结果。](https://image.jiqizhixin.com/uploads/editor/bd1ac1c4-71cc-4707-9369-4d6e4786f8ab/640.png)
![表3: 在NExT-QA上的实验结果。](https://image.jiqizhixin.com/uploads/editor/a2055618-5209-4f45-bda9-45c79355ab53/640.png)
![表4: 在NExT-QA上对不同工具的消融实验。](https://image.jiqizhixin.com/uploads/editor/b9b936c6-28ca-4085-9427-3cb5bd48bd8f/640.png)

**总结**
本文提出的 VideoAgent 是一种多模态智能体，通过一种新颖的记忆机制结合了多个基础模型，用于视频理解。与端到端的多模态大语言模型（LLM）和其他工具使用智能体相比，VideoAgent 采用了极简的工具使用流程，不需要昂贵的训练，同时在 EgoSchema、WorldQA 和 NExT-QA 等具有挑战性的长视频理解基准上，产生了相当或更好的表现。未来的研究方向可能包括在具身智能、制造业和增强现实领域的实际应用。

**团队介绍**
论文核心团队来自北京通用人工智能研究院机器学习实验室，团队负责人李庆博士长期从事关于多模态理解、AI Agents、具身智能等方向的相关工作，主页：https://liqing.io。该团队拥有包括全职研究员、工程师、以及实习生在内的二十余人团队，也在持续招聘全职人员和实习生中，团队的长期目标是打造集交互、推理、学习于一体的通用智能体。

**参考文献**
[1] Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122 (2023)
[2] Song, E., Chai, W., Wang, G., Zhang, Y., Zhou, H., Wu, F., Chi, H., Guo, X., Ye, T., Zhang, Y., et al.: Moviechat: From dense token to sparse memory for long video understanding. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2024)
[3] Wang, Y., Wang, Y., Wu, P., Liang, J., Zhao, D., Zheng, Z.: Lstp: Language-guided spatial-temporal prompt learning for long-form video-text understanding. arXiv preprint arXiv:2402.16050 (2024)
[4] Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., Metzler, D.: Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006 (2020)
[5] Surís, D., Menon, S., Vondrick, C.: Vipergpt: Visual inference via python execution for reasoning. In: International Conference on Computer Vision (ICCV) (2023)
[6] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 (2023)
[7] Zhao, Y., Misra, I., Krähenbühl, P., Girdhar, R.: Learning video representations from large language models. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)
[8] https://platform.openai.com/docs/guides/embeddings
[9] Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang, Y., et al.: Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942 (2023)
[10] Zhao, Y., Lv, W., Xu, S., Wei, J., Wang, G., Dang, Q., Liu, Y., Chen, J.: Detrs beat yolos on real-time object detection. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2024)
[11] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., Luo, P., Liu, W., Wang, X.: Bytetrack: Multi-object tracking by associating every detection box. In: European Conference on Computer Vision (ECCV) (2022)
[12] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (ICML) (2021)
[13] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)
[14] Mangalam, K., Akshulakov, R., Malik, J.: Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems (NeurIPS) (2024)
[15] Zhang, Y., Zhang, K., Li, B., Pu, F., Setiadharma, C.A., Yang, J., Liu, Z.: Worldqa: Multimodal world knowledge in videos through long-chain reasoning. arXiv preprint arXiv:2405.03272 (2024)
[16] Xiao, J., Shang, X., Yao, A., Chua, T.S.: Next-qa: Next phase of question-answering to explaining temporal actions. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2021)
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。