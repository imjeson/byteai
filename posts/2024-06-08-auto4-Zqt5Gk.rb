---
title: '速递｜前 OpenAI 超级对齐研究员宣布成立 AGI 投资机构！获 AI Grant 与 Stripe 创始人投资'
date: 2024-06-09
author: ByteAILab

---

<div class="article-content__container" data-v-0aea98aa="" data-v-7f057cc4="">
<div class="bbt-html" data-v-7f057cc4="">
<div class="ne-render-content" data-editby="ne">
<p>文章来源：有新 有新Newin </p>
<p>近日，前 OpenAI 超级对齐研究员 Leopold Aschenbrenner 宣布成立了一家 AGI 投资机构，将其定位为对冲基金与智库的结合体。</p>
<p>据悉，Aschenbrenner 的资金主要来自前 Github CEO Nat Friedman、投资者 Daniel Gross、Stripe CEO Patrick Collision 以及 Stripe 总裁 John Collision。

---
</p>
<h3><strong><u>▍从经济学研究到 AI 超级对齐</u></strong></h3>
<p>在加入 OpenAI 前，Aschenbrenner 曾在多个研究机构工作。Aschenbrenner 早年从德国移居美国，年仅 15 岁就进入哥伦比亚大学学习，并于19岁以优秀的成绩毕业，在哥伦比亚大学主修经济学和数学统计学，并担任经济学和政治学部门的研究助理。</p>
<p>2020 年至今，他成为牛津大学全球优先研究所（Global Priorities Institute）的研究员，专注于经济增长和长期风险的研究。</p>
<p>在加入 OpenAI 之前，Aschenbrenner 还在 FTX 创始人 Sam Bankman-Fried 创办的慈善基金 Future Fund 短暂任职，该基金旨在资助“改善人类长期前景”的项目。</p>
<p>2023 年，Aschenbrenner 加入 OpenAI 超对齐团队（Superalignment Team），专注于解决 AI 对齐问题，以确保 AI 系统按照人类的意图行事。</p>
<h3><strong><u>▍因泄露信息遭 OpenAI 解雇</u></strong></h3>
<p>今年，OpenAI 因涉嫌泄露信息解雇了两名员工，分别是 Leopold Aschenbrenner 和 Pavel Izmailov，他们曾在一个专注于 AI 安全的团队工作，这个团队由 Ilya Sutskever 在去年夏天组建，旨在开发控制和引导先进 AI 的技术。</p>
<p>对于解雇的解释，Aschenbrenner 表示他曾撰写了一份内部备忘录，批评 OpenAI 在保护关键算法信息和模型权重方面的能力，之后这份文件最初在他的同事和一些领导之间分发，后来因一次显著的安全事件被提交给董事会。</p>
<p>随后，OpenAI HR 向他发出正式警告，认为他的备忘录是一个重大问题。尽管如此，他仍然坚持推动更好的安全措施。据称，OpenAI 管理层对他决定通知董事会特别不满，随后对安全问题施加了压力。</p>
<p>此外，Aschenbrenner 被解雇的直接原因是他与外部研究人员分享了一份文件，OpenAI 认为该文件包含敏感信息，但 Aschenbrenner 坚持认为文件中没有机密信息，并且分享以获取反馈是公司的常规做法，该文件包括一个 2027～2028 年实现 AGI 时间表，Aschenbrenner 认为这已经是公开信息。</p>
<h3><strong><u>▍2027 年实现 AGI，伴随超智能</u></strong></h3>
<p>Aschenbrenner 将当前关于 AI 的讨论分为两派：一种是完全停止AI发展（如Eliezer Yudkowsky 所倡导的），另一种是全速推进（无视风险），他认为这两种极端立场都是不可行的。</p>
<p>他建议最大限度地加速 AGI 对齐的研究，类似于“曲速行动”（Operation Warp Speed）加速疫苗生产，同时在 AI 实验室的自我监管和独立监控下，通过独立评估（如 ARC 评估）确保安全。</p>
<p>Aschenbrenner 还指出，美国正经历一场前所未有的工业运动，以应对 AGI 需求。数万亿美元将被投入到GPU、数据中心和电力建设上，美国的电力生产预计将在本十年末增长数十个百分点。</p>
<p>他认为，到 2027 年实现 AGI 是非常有可能的。从 GPT-2 到 GPT-4 的进步显示了 AI 能力的巨大跃升，他预计在未来几年内，计算能力和算法效率的持续提升将使AGI 成为现实。</p>
<p>此外，Aschenbrenner 还认为，AI 进步不会停留在与人类相当的水平。数百万的AGI 将能够自动化 AI 研究，可能在一年内压缩十年的算法进步，从而迅速从人类水平发展到超智能。</p>
<h3><strong><u>▍对 AI 安全投入不足</u></strong></h3>
<p>Aschenbrenner 指出，目前美国 AI 实验室在安全方面投入不足，亟需加强防护措施，控制比人类更聪明的 AI 系统是一个尚未解决的技术问题。虽然这个问题是可以解决的，但在快速的智能爆炸期间，可能会出现失控的风险，这将是极其紧张的局面。</p>
<p>他还认为超智能将赋予决定性的经济和军事优势，AGI 将是人类有史以来最强大的武器，与冷战时期的核武器相似，失控的 AGI 可能带来灾难性后果。</p>
<p>此外，他预测到 2027～2028 年，美国将启动某种形式的 AGI 项目，因为仅靠初创公司无法应对超智能的挑战。</p>
</div></div> <!-- -->
<div class="bbt-html" data-v-7f057cc4="">
<p>本文链接：<a data-v-7f057cc4="" href="https://www.aixinzhijie.com/article/6845986">https://www.aixinzhijie.com/article/6845986</a></p>
</div></div>。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。