---

title: '澳大利亚人权专员警告人工智能可能加剧种族主义和性别歧视'
date: 2025-08-14
author: ByteAILab

---

人工智能风险加深澳大利亚的种族主义和性别歧视，人权专员警告道，这一言论出现于工党内部关于如何应对这一新兴技术的辩论之中。

---
洛雷娜·芬利表示，追求人工智能带来的生产力提升不应以牺牲反歧视为代价，特别是在缺乏适当监管的情况下。芬利的评论是针对工党参议员米歇尔·阿南达-拉贾（Michelle Ananda-Rajah）公开呼吁将所有澳大利亚数据“解放”给科技公司，以防止人工智能延续海外偏见，并反映澳大利亚的生活和文化。

阿南达-拉贾反对制定专门的人工智能法案，但她认为内容创作者应获得报酬。人工智能带来的生产力提升将在下周的联邦政府经济峰会上讨论，工会和行业团体对版权和隐私保护表示担忧。媒体和艺术团体警告称，如果大型科技公司能够用他们的内容来训练人工智能模型，将会出现“肆虐的知识产权盗窃”。

芬利表示，由于缺乏透明度，人工智能工具训练所用数据集的偏见很难被识别。“算法偏见意味着偏见和不公正是内置于我们使用的工具中，因此由此产生的决策将反映这种偏见，”她说道。“当算法偏见与自动化偏见结合时——后者是指人类更倾向于依赖机器的决策，几乎取代自己的思考——我们实际上在创建一种歧视和偏见的形式，这种形式可能是如此根深蒂固，以至于我们甚至没有意识到它的存在。”

人权委员会始终倡导设立人工智能法案，以增强包括隐私法案在内的现有立法，并对人工智能工具进行严格偏见测试。芬利表示，政府应紧急建立新的立法保护。“偏见测试和审计，确保适当的人类监督和审查，你需要采取多种不同的措施。”

在阿南达-拉贾的医师和人工智能研究背景下，她强调，重要的是让人工智能工具训练于澳大利亚的数据，否则将延续海外的偏见。

政府虽然强调保护知识产权的重要性，但阿南达-拉贾警告称，如果不开放国内数据，澳大利亚将会“永远依赖于[大型科技公司]的人工智能模型，而没有任何监督或洞悉他们的模型和平台”。“人工智能必须在尽可能广泛的群体基础上训练出尽可能多的数据，否则将加剧偏见，可能对其本应服务的人群造成伤害，”阿南达-拉贾表示。“我们需要解放我们的数据，以便训练这些模型，使其能够更好地代表我们。”

芬利表示，在释放澳大利亚数据时，应采取公平的方式，但她认为当务之急应是监管。“拥有多样化和具有代表性的数据绝对是件好事……但这只是解决方案的一部分，”她说。“我们需要确保这项技术以公平的方式实施，真正承认人类所做的工作和贡献。”

劳特布大学的一位人工智能专家、前人工智能公司数据研究员朱迪斯·比肖普表示，解放更多的澳大利亚数据可以帮助更合适地训练人工智能工具——同时警告称，使用国际数据开发的人工智能工具可能无法反映澳大利亚人的需求，但这只是解决方案的一小部分。

电子安全专员朱莉·因曼·格兰特（Julie Inman Grant）也对人工智能工具使用数据缺乏透明性表示担忧。她在一份声明中表示，科技公司应对其训练数据保持透明，开发报告工具，并在其产品中使用多样化、准确和具有代表性的数据。“生成性人工智能开发和部署的非透明性是深具问题的，”因曼·格兰特表示。“这引发了关于大型语言模型[LAMs]可能放大甚至加速有害偏见的重要问题——包括狭隘或有害的性别规范和种族偏见。”

随着这些系统的开发集中在少数几家公司手中，仍然存在某些证据、声音和视角在生成输出中可能被淹没或边缘化的真正风险。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。