---

title: 'KAYTUS推出升级版MotusAI以加速LLM部署'
date: 2025-06-14
author: ByteAILab

---

精简推理性能、工具兼容性、资源调度和系统稳定性，以快速推进大型AI模型部署。![图片](https://ai-techpark.com/wp-content/uploads/KAYTUS-1.jpg){ width=60% }

---


KAYTUS，一家领先的端到端AI和液冷解决方案提供商，今天在2025年ISC高性能计算大会上宣布推出最新版本的MotusAI AI DevOps平台。升级后的MotusAI平台在大型模型推理性能上提供了显著增强，并且与涵盖大型模型整个生命周期的多种开源工具广泛兼容。该平台旨在实现统一和动态的资源调度，显著提高大型AI模型开发和部署过程中的资源利用率和操作效率。MotusAI的这一最新发布预计将进一步加速AI的采用，并推动教育、金融、能源、汽车和制造等关键行业的商业创新。

随着大型AI模型越来越多地嵌入到现实应用中，企业正在大规模部署它们，以在多种行业中创造实际价值。然而，许多组织仍然面临AI采用中的关键挑战，包括较长的部署周期、严格的稳定性要求、分散的开源工具管理以及低计算资源利用率。为了解决这些痛点，KAYTUS推出了最新版本的MotusAI AI DevOps平台，专门旨在简化AI部署、增强系统稳定性，并优化大型模型操作的AI基础设施效率。

增强的推理性能以确保服务质量

部署AI推理服务是一项复杂的工作，涉及服务的部署、管理和持续健康监控。这些任务需要在模型和服务治理、通过加速框架进行性能调优及长期服务稳定性方面达到严格的标准，这通常需要大量的人力、时间和技术专长的投资。

升级后的MotusAI提供了强大的大型模型部署能力，使可视性和性能得到完美结合。通过集成优化框架如SGLang和vLLM，MotusAI确保企业可以快速自信地部署高性能的分布式推理服务。MotusAI旨在支持大参数模型，利用智能资源和网络亲和性调度加快上市时间，同时最大化硬件利用率。其内置监控能力覆盖整个堆栈——从硬件和平台到POD和服务——提供自动故障诊断和快速服务恢复。MotusAI还支持基于实时使用情况和资源监控对推理工作负载进行动态扩展，从而增强服务稳定性。

全面的工具支持以加速AI采用

随着AI模型技术的快速发展，支持生态系统的开发工具变得越来越复杂。开发人员需要一个简化的、通用的平台，以高效地选择、部署和操作这些工具。

升级后的MotusAI提供对多种领先开源工具的广泛支持，使企业用户能够按需配置和管理其模型开发环境。MotusAI内置工具如LabelStudio，加速数据标注和跨不同类别的同步，提高数据处理效率，加快模型开发周期。MotusAI还提供了整个AI模型生命周期的集成工具链，包括LabelStudio和OpenRefine用于数据标注和治理、LLaMA-Factory用于大型模型微调、Dify和Confluence用于大型模型应用开发以及Stable Diffusion用于文本到图像生成。这些工具共同使用户能够快速采用大型模型并在规模上提高开发生产力。

在同一节点上实现训练-推理混合调度以最大化资源效率

高效利用计算资源仍然是处于AI采用早期阶段的AI初创企业和中小型企业的关键优先事项。传统的AI集群通常为训练和推理任务分别分配计算节点，从而限制了两种工作负载在资源调度上的灵活性和效率。

升级后的MotusAI通过启用在单个节点上训练和推理工作负载的混合调度，克服了传统限制，实现了多种任务类型的无缝集成和动态编排。凭借先进的GPU调度能力，MotusAI支持按需资源分配，赋予用户根据工作负载要求高效管理GPU资源的能力。MotusAI还具有多维GPU调度功能，包括精细分区和支持多实例GPU（MIG），满足模型开发、调试和推理等广泛用例的需求。

MotusAI的增强调度器显著超越了社区版本，在大型POD部署中实现了任务吞吐量的5倍提高和延迟的5倍降低。它能够快速启动并准备数百个POD环境，同时支持训练和推理的动态工作负载扩展和潮汐调度。这些能力在多种真实世界的AI场景中实现了无缝任务编排。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。