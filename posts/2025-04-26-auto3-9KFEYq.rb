title: 'NTT科学家在ICLR 2025上展示突破性的人工智能深度学习研究'
date: 2025-04-27
author: ByteAILab

---

NTT Research和NTT R&D合作撰写的论文探讨了大型语言模型（LLMs）不确定性和开放性特性、“涌现”现象、上下文学习等方面的研究成果。![图片](https://ai-techpark.com/wp-content/uploads/NTT-Research.jpg){ width=60% }

---


新闻亮点：

- NTT Research和NTT R&D的科学家在国际顶级机器学习会议ICLR 2025上展示了九篇关于人工智能深度学习领域突破性研究的论文。
- NTT Research新成立的人工智能物理（PAI）小组成员共同撰写的五篇论文探讨了人工智能学习、理解和成长的基本要素。
- PAI小组于2025年4月成立，旨在深入理解人工智能机制，观察人工智能的学习和预测行为，修复人工智能与人类操作员之间的信任裂痕。

NTT Research, Inc.和NTT R&D（NTT（TYO:9432）的两个部门）宣布，他们的科学家在国际学习表现会议（ICLR）2025上将展示九篇论文，这是一个致力于推进表示学习，特别是深度学习的顶级会议。五篇已接受的演讲来自NTT Research最近成立的人工智能物理（PAI）小组的科学家共同撰写，组长为Hidenori Tanaka。

这些研究在理解人工智能模型如何学习、成长和克服不确定性方面开辟了新的领域，所有这些都支持NTT在推动变革性、社会韧性、可持续和负责任的人工智能方面的承诺。

“人工智能物理小组及其合作者与公众、科技行业和学术界分享了对人工智能潜力的期待，”Tanaka表示。“然而，正如ICLR 2025上接受的研究所示，关于人工智能如何根本学习以及生成式人工智能如何根本创造输出的重要问题仍然存在。神经网络在人工智能的‘深度学习’中起着至关重要的作用，改善我们对它们的理解对于最终促进可持续、可靠和可信赖的人工智能技术的发展至关重要。”

其中一篇论文《神经文本生成的分叉路径》解决了大型语言模型（LLMs）不确定性估计的问题，以便进行适当的评估和用户安全。以往的研究方法专注于生成文本中的最终答案——忽视了潜在影响深远的中间步骤——该研究测试了一个假设，即存在关键的分叉标记，重新采样系统在这些特定标记处，而不是其他地方，会导致完全不同的结果。研究人员发现了许多分叉标记的例子，包括标点符号，暗示LLMs往往只需一个标记便能生成不同的输出。

该论文由Eric Bigelow、Ari Holtzman、Hidenori Tanaka和Tomer Ullman共同撰写。

PAI小组的其他四篇论文将在会议上展示，包括：

- 《上下文学习的表示》：研究人员探讨了LLMs的开放性特性（例如，它们的上下文学习能力）以及模型是否改变这些预训练语义以采用替代的上下文特定语义。研究结果表明，扩展上下文大小可以灵活地重新组织模型表示，可能解锁新的能力。作者包括：Core Francisco Park、Andrew Lee、Ekdeep Singh Lubana、Yongyi Yang、Maya Okawa、Kento Nishi、Martin Wattenberg和Hidenori Tanaka。
  
- 《竞争动态塑造上下文学习的算法阶段》：研究人员提出了一个合成序列建模任务，涉及学习模拟有限混合马尔可夫链。他们认为上下文学习（ICL）最好被视为不同算法的混合，每个算法都有其独特之处，这也意味着在所有环境中对ICL做出普遍声明可能是不可行的。作者包括：Core Francisco Park、Ekdeep Singh Lubana、Itamar Pres和Hidenori Tanaka。

- 《概念学习和组合泛化的动态》：研究人员通过引入结构身份映射（SIM）任务，提出了先前工作中组合泛化问题的抽象，模型被训练以学习高斯混合的身份映射，并且结构性组织的中心。总体而言，该研究确立了SIM任务作为现代生成模型中概念学习动态的有意义理论抽象。作者包括：Yongyi Yang、Core Francisco Park、Ekdeep Singh Lubana、Maya Okawa、Wei Hu和Hidenori Tanaka。

- 《涌现的渗流模型：分析在形式语言上训练的变压器》：承认建立神经网络中“涌现”现象背后的因果因素的必要性，研究人员从其他领域的涌现特性的研究中获得灵感，并提出了该概念在神经网络上下文中的现象学定义。作者包括：Ekdeep Singh Lubana、Kyogo Kawaguchi、Robert P. Dick和Hidenori Tanaka。

此外，由NTT R&D科学家在日本撰写或共同撰写的四篇论文将在会议上展示，包括：

- 《基于子空间对齐的回归测试时适应》作者包括：Kazuki Adachi、Shin’ya Yamaguchi、Atsutoshi Kumagai和Tomoki Hamagami。
  
- 《通过基于置换的权重匹配分析线性模式连通性：附带对其他置换搜索方法的见解》作者包括：Akira Ito、Masanori Yamada和Atsutoshi Kumagai。
  
- 《防止敏感数据生成的正标签-无标签扩散模型》作者包括：Hiroshi Takahashi、Tomoharu Iwata、Atsutoshi Kumagai、Yuuki Yamanaka和Tomoya Yamashita。

- 《基于小波的长上下文位置表示》作者包括：Yui Oka、Taku Hasegawa、Kyosuke Nishida和Kuniko Saito。

ICLR 2025，第十三届国际学习表示会议，是全球享有盛誉的深度学习会议，正于2025年4月24日至28日在新加坡举行。在2024年ICLR上，NTT Research物理与信息学（PHI）实验室的科学家共同撰写了两篇重要论文，分别关于“利用随机二进制序列分析上下文学习动态，揭示LLM行为的急剧转变”和“微调对模型能力的影响，显示变化很小”。

NTT Research人工智能物理小组致力于推动我们对深度神经网络和人工智能心理学的理解。其三重使命包括：1）深化对人工智能机制的理解，以更好地体现伦理，而不是通过补丁式微调（即强制学习）；2）借鉴实验物理学，继续创建可系统控制的人工智能空间，并逐步观察人工智能的学习和预测行为；3）通过改善操作和数据控制，修复人工智能与人类操作员之间的信任裂痕。

该小组于2025年4月正式成立，由PHI实验室的成员组成，起初是NTT Research与哈佛大学脑科学中心之间的合作，曾称为哈佛大学CBS-NTT奖学金项目。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。