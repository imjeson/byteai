---
title: '「机器学习之父」Mitchell 撰文：AI 如何加速科学发展，美国如何抓住机遇'
date: 2024-07-30
author: ByteAILab

---

Editor | ScienceAI

近日，卡内基梅隆大学（Carnegie Mellon University）教授，有着「机器学习之父」之称的 Tom M. Mitchell 撰写了新的 AI for Science 白皮书，重点讨论了「人工智能如何加速科学发展？美国政府如何帮助实现这一目标？」这一主题。

ScienceAI 对白皮书原文进行了不改变原意的全文编译，内容如下。

---


人工智能领域最近取得了显著进展，包括 GPT、Claude 和 Gemini 等大型语言模型，因此提出了这样一种可能性：人工智能的一个非常积极的影响，也许是大大加速从细胞生物学到材料科学、天气和气候建模到神经科学等各种科学领域的研究进展。这里我们简要总结一下这个人工智能科学机遇，以及美国政府可以做些什么来抓住这个机遇。

人工智能与科学的机遇

当今几乎所有领域的绝大多数科学研究都可以归为「独行侠」科学。

换句话说，科学家和他们的十几名研究人员组成的研究团队提出一个想法，进行实验来测试它，撰写并发表结果，也许在互联网上分享他们的实验数据，然后重复这个过程。

其他科学家可以通过阅读已发表的论文来巩固这些成果，但由于以下几个原因，这一过程容易出错且效率极低：

（1）个别科学家不可能读到其领域内已发表的所有文章，因此对其他相关研究部分视而不见；
（2）期刊出版物中描述的实验必然会省略许多细节，这使得其他人很难复制其结果并在结果基础上进行研究；
（3）单个实验数据集的分析通常是孤立进行的，未能纳入其他科学家进行的其他相关实验的数据（因此也没有纳入有价值的信息）。

在未来十年，人工智能可以帮助科学家克服上述三个问题
AI 可以将这种「独行侠」式的科学研究模式转变为「社区科学发现」模式。特别是，人工智能可以用来创造一种新型的计算机研究助手，帮助人类科学家克服这些问题，方法是：

发现复杂数据集（包括由多个实验室进行的许多实验建立的数据集）中的规律，而不是对单个、规模小得多且代表性较差的数据集进行孤立的分析。通过基于超出人类能力的更大数量级的数据集进行分析，可以实现更全面、更准确的分析。
使用 GPT 等人工智能大型语言模型阅读和消化该领域的每一篇相关出版物，从而帮助科学家不仅根据自己实验室和其他实验室的实验数据形成新的假设，还可以根据已发表的研究文献中的假设和论据形成新的假设，从而得出比没有这种自然语言人工智能工具时可能得出的更为明智的假设。
创建「基础模型」，通过利用实验室和科学家收集的多种不同类型的实验数据来训练这些模型，从而将领域内不断增长的知识集中到一个地方，并提供这些知识的计算机可执行模型。这些可执行的「基础模型」可以发挥与方程（例如 f = ma）相同的作用，即它们根据其他观察到的量对某些量进行预测。并且，与经典的方程不同，这些基础模型可以捕捉数十万个不同变量之间的经验关系，而不是少数几个变量。
实现新实验设计和机器人执行的自动化或半自动化，从而加快新相关实验的速度，提高...
---

美国政府可以做些什么来抓住这个机会？

将这一机遇转化为现实需要几个要素：

大量实验数据

基于文本的基础模型的一个教训是，它们训练的数据越多，其能力就越强。有经验的科学家也非常清楚，更多、更多样化的实验数据的价值。要实现科学的多个数量级的进步，并训练我们想要的基础模型类型，我们需要在共享和联合分析整个科学界贡献的各种数据集的能力方面取得非常显著的进步。

获取科学出版物和用计算机阅读它们的能力

这里机遇的一个关键部分是改变现在的状态：科学家不太可能阅读其领域中 1% 的相关出版物，计算机通过阅读 100% 的出版物、总结它们及其与当前科学问题的相关性，并提供对话界面来讨论其内容和含义。这不仅需要访问在线文献，还需要 AI 研究构建这样一个「文学助手」。

计算和网络资源

GPT 和 Gemini 等基于文本的基础模型，因其开发过程中耗费的大量处理资源而闻名，开发不同科学领域的基础模型也需要大量计算资源。然而，许多 AI 科学工作中的计算需求可能比训练 GPT 等 LLM 所需的计算要小得多，因此可以通过与政府研究实验室正在进行的类似投资来实现。

例如，AlphaFold 是一种已经彻底改变了药物设计蛋白质分析的 AI 模型，它使用的训练计算量比 GPT 和 Gemini 等基于文本的基础模型要少得多。为了支持数据共享，我们需要大量的计算机网络，但当前的互联网已经为传输大型实验数据集提供了足够的起点。因此，与潜在收益相比，支持 AI 驱动的科学进步的硬件成本可能相当低。

新的机器学习和 AI 方法

当前的机器学习方法对于发现人类无法检查的庞大数据集中的统计规律极为有用（例如，AlphaFold 是在大量蛋白质序列及其精心测量的 3D 结构上进行训练的）。新机遇的关键部分是将当前的机器学习方法（发现数据中的统计相关性）扩展到两个重要方向：（1）从发现相关性转向发现数据中的因果关系，（2）从仅从大型结构化数据集学习转向从大型结构化数据集和大量研究文献中学习；也就是说，像人类科学家一样从实验数据和其他人用自然语言表达的已发表假设和论点中学习。最近出现的 LLM 具有消化、总结和推理大型文本集合的高级能力，可以为这种新的机器学习算法奠定基础。

政府应该做什么？关键是支持上述四部分，并团结科学界探索基于人工智能的新方法，以促进他们的研究进展。因此，政府应该考虑采取以下几种行动：

探索特定科学领域的特定机会，资助许多科学领域的多机构研究团队，提出愿景和初步结果，展示如何使用人工智能来显著加速其领域的进步，以及扩大该方法所需的条件。这项工作不应以拨款的形式资助给个别机构，因为最大的进步可能来自于整合许多机构的许多科学家的数据和研究。相反，如果由许多机构的科学家团队来执行，这可能是最有效的，他们提出的机会和方法可以激励他们参与整个科学界。

加速创建新的实验数据集以训练新的基础模型，并向整个科学家社区提供数据：

创建数据共享标准，使一位科学家能够方便使用由不同科学家创建的实验数据，并为每个相关科学领域的国家数据资源奠定基础。请注意，在制定和使用此类标准方面，之前已有成功案例，可以为标准工作提供起始模板（例如，人类基因组计划中数据共享的成功）。

为每个相关领域创建和支持数据共享网站。正如 GitHub 已成为软件开发人员贡献、共享和重用软件代码的首选网站一样，为科学数据集创建一个 GitHub，它既可用作数据存储库，又可用作搜索引擎，用于发现与特定主题、假设或计划实验最相关的数据集。

研究如何构建激励机制以实现数据共享最大化。目前，各个科学领域在个体科学家共享数据的程度，以及营利机构将其数据用于基础科学研究的程度方面差异很大。建立一个大型、可共享的国家数据资源是人工智能科学机遇不可或缺的组成部分，构建一个令人信服的数据共享激励结构将是成功的关键。

在适当的情况下，资助开发自动化实验室（例如，用于化学、生物等实验的机器人实验室，可通过互联网供众多科学家使用），以高效地进行实验，并以标准格式生成数据。创建此类实验室的一个主要好处是，它们还将推动制定标准，以精确说明要遵循的实验程序，从而提高实验结果的可重复性。正如我们可以从数据集的 GitHub 中受益一样，我们也可以从相关的 GitHub 中受益，以共享、修改和重复使用实验协议的组件。
要创建新一代人工智能工具，需要：

资助专门开发适用于科学研究方法的相关基础 AI 研究。这应包括开发广义上的「基础模型」，作为加速不同领域研究的工具，并加速从「独行侠」科学向更强大的「社区科学发现」范式的转变。

特别支持阅读研究文献的研究，对陈述的输入假设进行批评和提出改进建议，并帮助科学家以与他们当前问题直接相关的方式从科学文献中获取结果。

特别支持将机器学习从发现相关性扩展到发现因果关系的研究，特别是在可以计划和执行新实验以测试因果关系假设的环境中。

特别支持对机器学习算法的扩展研究，从仅将大数据作为输入，到同时将大实验数据和该领域的完整研究文献作为输入，以便产生由实验数据中的统计规律以及研究文献中讨论的假设、解释和论点共同提供的信息。
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。