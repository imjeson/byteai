---
title: '史上最快AI芯片「Sohu」，速度10倍于B200，哈佛辍学生打造'
date: 2024-06-27
author: ByteAILab
---

生成式 AI 推理性价比是 GPU 的 140 倍。
大模型时代，全球都缺算力，买铲子的英伟达市值被炒上了天。

---

现在，终于有一家公司带着自己的 AI 芯片来叫板了。
今天凌晨，科技圈迎来了一个重要新闻。美国芯片创业公司 Etched 推出了自己的第一块 AI 芯片 Sohu，它运行大模型的速度比英伟达 H100 要快 20 倍，比今年 3 月才推出的顶配芯片 B200 也要快上超过 10 倍。
一台 Sohu 的服务器运行 Llama 70B 每秒可输出超过 50 万个 token，比 H100 服务器（23,000 个 token / 秒）多20倍，比 B200 服务器（约45,000个token / 秒）多10倍。
Sohu 是世界第一款专用于 Transformer 计算的芯片，历时两年打造。
作为一块 ASIC（专用集成电路），Sohu 把对于 transformer 架构的优化硬化在芯片中，无法运行大多数传统的 AI 模型：如为 Instagram 广告提供支持的 DLRM、AlphaFold 2等蛋白质折叠模型或 Stable Diffusion 2等较旧的图像生成模型。我们也无法运行 CNN、RNN或LSTM。
但另一方面，对于 transformer 来说，Sohu 就是有史以来最快的芯片，与其他产品之间是量级的区别。如今的每款主流 AI 产品如 ChatGPT、Claude、Gemini和Sora都是由 transformer 驱动的。
最近一段时间，由于摩尔定律放缓，GPU性能的提升很大程度上需要依赖于增加芯片面积和功耗。不论是英伟达B200、AMD MI300X还是Intel Gaudi 3，都不约而同的使用「二合一」的方式提升性能，功耗也翻倍了。
但如果大模型广泛使用Transformer架构，追求专业化或许是提高性能的好方向。‍
作为一个新兴领域，AI模型的架构过去变化很大。但自GPT-2以来，最先进的模型几乎都在使用Transformer，从OpenAI的GPT系列、谷歌的PaLM、Facebook的LLaMa，再到特斯拉FSD自动驾驶所需的模型。
Etched给我们算了一笔账：芯片项目的成本为5000万至1亿美元，需要数年时间才能投入生产。另一方面，当模型训练成本超过10亿美元、推理成本超过100亿美元时，使用专用芯片是不可避免的。在这种产业规模下，1%的改进就能撬动硬件架构的更新。
速度超H100 20倍，FLOPS利用率超90%
作为世界上首款transformer ASIC（应用型专用集成电路）芯片，一台集成了8块Sohu的服务器可以匹敌160块H100 GPU。也即，Sohu的运行速度是H100的20多倍。
具体来讲，通过专门化，Sohu具备了前所未有的性能。一台集成8块Sohu芯片的服务器每秒可以处理50万的Llama 7B tokens。
针对Llama 3 70B的FP8精度基准测试显示：无稀疏性、8倍模型并行、2048输入或128输出长度。
此外，对于Llama、Stable Diffusion 3，Sohu仅支持transformer推理。Sohu支持了当前谷歌、Meta、微软、OpenAI、Anthropic等各家的模型，未来还会适配模型调整。
由于Sohu仅能运行一种算法，因此可以删除绝大多数控制流逻辑，从而允许拥有更多数学块。也因此，Sohu实现了90%以上的FLOPS利用率，而使用TRT-LLM的GPU约为30%。
Sohu为何能输出更多FLOPS？
英伟达H200支持989 TFLOPS的FP16/BF16计算能力，并且没有稀疏性。这是当前最先进的芯片，而2025年推出的GB200将在计算能力上提升25%，支持1250 TFLOPS。
由于GPU的绝大部分区域都是可编程的，因此专注于transformer会容纳更多的计算。这可以从第一性原理中证明：
构建单个FP16/BF16/FP8乘加电路需要10000个晶体管，这是所有矩阵数学的基石。H100 SXM拥有528个张量核心，每个核心拥有4×8×16 FMA电路。乘法告诉我们：H100有27亿个晶体管用于张量核心。
但是，H100却有800亿个晶体管。这意味着H100 GPU上只有3.3%的晶体管用于矩阵乘法。这是英伟达和其他芯片厂商经过深思熟虑的设计决定。如果你想支持所有类型的模型（CNN、LSTM、SSM等...
而通过仅运行transformer，Etched可以让Sohu芯片输出更多的FLOPS，且需要降低精度或稀疏性。
内存带宽也不是瓶颈
实际上，对于像Llama 3这样的模型，情况并非如此。
我们以英伟达和AMD的标准基准为例：2048个输入token和128个输出token。大多数AI产品的prompt更长，比如最新的Claude聊天机器人在系统prompt中拥有1000+tokens。
在Sohu上，推理是分batch运行的。每个batch都需要加载所有模型权重一次，并在batch的每个token中重复使用。通常来说，LLM输入是计算密集型的，而LLM输出是内存密集型的。当我们将输入和输出token与连续batch结合时，工作负载变成了高度计算密集型。
以下为LLM连续batching处理的示例，这里运行具有四个输入token和四个输出token的序列。每种颜色代表不同的序列。
我们可以扩展相同的技巧，从而运行具有2048个输入token和128个输出token的Llama 3 70B。每个batch中包含用于一个序列的2048个输入token，以及用于127个不同序列的127个输出token。
如果这样做了，则每个batch需要大约(2048+127)×70B参数×每个参数2字节=304 TFLOP，而仅需要加载70B参数×每个参数2字节=140 GB的模型权重和大约127×64×8×128×(2048+127)×2×2=72GB的KV缓存权重。这比内存带宽需要的计算量多得多：H200需要6.8PFLOPS的计算才能最大化其内存带宽。这还是利用率为100%的情况，如果利用率仅为30%，则需要3倍以上的内存。
Sohu拥有了更多的计算能力且利用率非常高，因此可以运行巨大的吞吐量，而不会出现内存带宽瓶颈。
软件如何工作
在GPU和TPU上，软件是一场噩梦。处理任意CUDA和PyTorch代码需要极其复杂的编译器。第三方AI芯片（如AMD、Intel、AWS等）在软件上总共花费了数十亿美元，但收效甚微。
而Sohu只运行transformer，因此只需要为transformer编写软件。
大多数运行开源或内部模型的公司都使用特定于transformer的推理库，比如TensorRT-LLM、vLLM或HuggingFace的TGI。
这些框架非常僵化，虽然你可以进行模型超参数调优，但实际上不支持更改底层模型代码。但这没关系，因为所有transformer模型都非常相似（甚至是文本/图像/视频模型），所以超参数调优就是你真正需要的。
虽然95%的AI公司是这样，但一些最大的AI实验室采用定制方式。他们有工程师团队来手动调整GPU核心以实现更高的利用率，并进行逆向工程以将寄存器对每个张量核心的延迟将至最低。
Etched让我们不需要再进行逆向工程，他们的软件（从驱动程序、内核到服务堆栈）都将是开源的。如果你想实现自定义transformer层，则内核向导可以自由地这样做。
创业团队：哈佛辍学生领衔
Etched的CEO Gavin Uberti告诉记者：「如果未来Transformer不再是主流，那我们就会灭亡。但如果它继续存在，我们就会成为有史以来最大的公司。」
打造Sohu芯片的Etched位于加州库比蒂诺，公司成立仅两年，目前团队只有35人，创始人是一对哈佛辍学生Gavin Uberti（前OctoML和前Xnor.ai员工）和Chris Zhu，他们与Robert Wachen和前赛普拉斯半导体公司首席技术官Mark Ross一起，一直致力于打造专用于AI大模型的芯片。
在Sohu芯片发布的同时，Etched也宣布已完成了1.2亿美元的A轮融资，由Primary Venture Partners和Positive Sum Ventures共同领投。Etched的总融资额已达到1.2536亿美元，本轮融资的重要投资者包括Peter Thiel、GitHub首席执行官Thomas Dohmke、Cruise联合创始人Kyle Vogt和Quora联合创始人Charlie Cheever。
不过对于占据超过80% AI芯片市场份额的英伟达来说，1.2亿美元只相当于它半天的收入。
「我们如此兴奋的原因，选择辍学的原因，以及我们召集团队，投身芯片项目的原因在于 —— 这是最重要的工作，」Etched运营主管Robert Wachen说道。「整个技术的未来将取决于算力基础设施能否实现大规模。」
Uberti声称到目前为止，已有匿名客户预订了「数千万美元」的硬件，预计在今年三季度，Sohu将推向市场。
未来真的如Uberti所说，只有在Sohu这样的芯片上，视频生成、音频生成、具身智能等技术才能真正落地吗？

参考内容：
https://www.etched.com/announcing-etched
https://twitter.com/Etched/status/1805625693113663834
https://www.cnbc.com/2024/06/25/etched-raises-120-million-to-build-chip-to-take-on-nvidia-in-ai.html
https://techcrunch.com/2024/06/25/etched-is-building-an-ai-chip-that-only-runs-transformer-models/
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。