title: 'Skywork-Reward-V2：引领开源奖励模型的新里程碑'
date: 2025-07-08
author: ByteAILab

---

在2024年9月，Skywork首次开源了Skywork-Reward系列模型及相关数据集。![图片](https://ai-techpark.com/wp-content/uploads/Skywork.jpg){ width=60% }

---
经过过去九个月的广泛采用，这些模型和数据在开源社区中得到了广泛应用，累计下载量超过750,000次，帮助多个前沿模型在RewardBench等权威评估中取得了优秀成绩。

在2025年7月4日，Skywork继续开源了第二代奖励模型——Skywork-Reward-V2系列，包括基于不同基础模型的8个奖励模型，参数范围从6亿到80亿。这些模型在七个主要的主流奖励模型评估基准上都取得了最高排名。

Skywork-Reward-V2 下载链接
HuggingFace: https://huggingface.co/collections/Skywork/skywork-reward-v2-685cc86ce5d9c9e4be500c84  
GitHub: https://github.com/SkyworkAI/Skywork-Reward-V2  
技术报告: https://arxiv.org/abs/2507.01352  

奖励模型在基于人类反馈的强化学习（RLHF）过程中发挥着至关重要的作用。在开发这一新一代奖励模型时，我们构建了一个名为Skywork-SynPref-40M的混合数据集，其中包含总共4000万个偏好对。

为了实现大规模、高效的数据筛选和过滤，Skywork特别设计了一个人机协作的两阶段过程，将高质量的人类注释与模型的可扩展处理能力相结合。在这一过程中，人类提供经过严格验证的高质量注释，而大型语言模型（LLMs）则根据人类的指导进行自动组织和扩展。

基于上述高质量的混合偏好数据，我们开发了Skywork-Reward-V2系列，展现了广泛的适用性和优秀的性能，在多个能力维度上表现出色，包括与人类偏好的整体对齐、客观正确性、安全性、抵御风格偏见的能力和最佳-N扩展能力。实验验证表明，这一系列模型在七个主流奖励模型评估基准上达到了最佳性能。

01 Skywork-SynPref-40M：人机合作的百万级人类偏好数据筛选  
即使是目前最先进的开源奖励模型，在多数主流评估基准上仍表现不佳。它们难以有效捕捉人类偏好的微妙和复杂特征，尤其是在面对多维、多层次的反馈时。此外，许多奖励模型往往在特定基准任务上表现良好，但在新的任务或场景中却难以迁移，显示出明显的“过拟合”现象。尽管现有研究已尝试通过优化目标函数、改进模型架构和最近出现的生成奖励模型来提高性能，但整体有效性仍然相当有限。

我们认为，当前奖励模型的脆弱性主要来源于现有偏好数据集的局限性，这些数据集往往覆盖范围有限、标签生成方法机械化，或缺乏严格的质量控制。因此，在开发新一代奖励模型的过程中，我们不仅延续了第一代的经验进行数据优化，还引入了更多样化、更大规模的真实人类偏好数据，努力在提高数据规模的同时保持数据质量。

因此，Skywork提出了Skywork-SynPref-40M——迄今为止最大规模的偏好混合数据集，包含总共4000万个偏好样本对。其核心创新在于“人机协作、两阶段迭代”的数据选择流程。

阶段1：人指导的小规模高质量偏好构建  
团队首先构建了一个未经验证的初始偏好池，并使用大型语言模型（LLMs）生成偏好相关的辅助属性，如任务类型、客观性和争议性。在此基础上，人类注释者遵循严格的验证协议并使用外部工具和先进的LLMs对部分数据进行详细审查，最终构建了一个小规模但高质量的“金标准”数据集，作为后续数据生成和模型评估的基础。

随后，我们使用金标准数据中的偏好标签作为指导，结合LLMs对高质量“银标准”数据进行大规模生成，从而实现数据体量的扩展。团队还进行了多轮迭代优化：在每轮中，训练奖励模型并根据其在金标准数据上的表现识别模型的弱点；然后检索相似样本并使用多模型共识机制进行自动注释，以进一步扩展和增强银标准数据。这个人机协作的闭环过程持续进行迭代，有效地提高了奖励模型对偏好的理解和判别能力。

阶段2：全自动的大规模偏好数据扩展  
在获得初步的高质量模型后，第二阶段转向自动化的大规模数据扩展。这个阶段不再依赖手动审核，而是使用训练好的奖励模型进行一致性过滤：

如果样本的标签与当前最佳模型的预测不一致，或者模型的信心较低，则调用LLMs进行自动重新注释；  
如果样本标签与“金模型”（仅由人类数据训练的模型）的预测一致，并且得到了当前模型或LLMs的支持，则可以直接通过筛选。

通过这一机制，团队成功地从原始4000万个样本中筛选出了2600万条选定数据点，在保持偏好数据规模与质量良好平衡的同时，大大减少了人工注释的负担。

02 Skywork-Reward-V2：在小模型规模中匹配大模型性能  
与前一代Skywork-Reward相比，Skywork新发布的Skywork-Reward-V2系列提供了8个基于Qwen3和LLaMA3系列模型训练的奖励模型，参数规模覆盖从6亿到80亿。

在七个主流奖励模型评估基准上，包括Reward Bench v1/v2、PPE偏好与正确性、RMB、RM-Bench和JudgeBench，Skywork-Reward-V2系列全面达到当前的最先进水平（SOTA）。

通过数据质量和丰富性补偿模型规模限制  
即使是规模最小的模型Skywork-Reward-V2-Qwen3-0.6B，其整体性能也几乎与先前一代最强模型Skywork-Reward-Gemma-2-27B-v0.2相当。而最大的Skywork-Reward-V2-Llama-3.1-8B在所有主流基准测试中表现出了全面的优越性，成为当前表现最好的开源奖励模型。

多维人类偏好能力的广泛覆盖  
此外，Skywork-Reward-V2在多项高级能力评估中取得了领先结果，包括最佳-N（BoN）任务、偏见抵抗能力测试（RM-Bench）、复杂指令理解和真实判断（RewardBench v2），展示了卓越的推广能力和实用性。

高度可扩展的数据筛选过程显著提高了奖励模型表现  
除了在评估中表现出色，团队还发现，在“人机协作、两阶段迭代”的数据构建过程中，经仔细筛选和过滤的偏好数据能够通过多个迭代训练轮次持续有效地提高奖励模型的整体表现，特别是在第二阶段的全自动数据扩展中表现显著。

相较之下，盲目扩展原始数据不仅未能提高初始性能，反而可能引入噪声和负面影响。为了进一步验证数据质量的关键作用，我们对早期版本的1600万个数据点进行了实验。结果表明，使用仅1.8%（约290,000个）的高质量数据训练的8B规模模型，其性能已超过当前70B级最先进奖励模型的表现。这一结果再次确认了Skywork-SynPref数据集不仅在规模上领先，在数据质量上也具有显著优势。

03 欢迎开源奖励模型的新里程碑：助力构建未来AI基础设施  
在关于第二代奖励模型Skywork-Reward-V2的研究中，团队提出了包含4000万个偏好对（其中2600万个经过仔细筛选的对）的Skywork-SynPref-40M混合数据集，以及一系列具有最先进性能、设计用于广泛任务适用性的Skywork-Reward-V2奖励模型。

我们相信，这项研究工作及奖励模型的持续迭代将有助于推动开源奖励模型的发展，并更广泛地促进人类反馈强化学习（RLHF）研究的进步。这标志着该领域向前发展的重要一步，能够进一步加速开源社区的繁荣。

Skywork-Reward-V2系列模型专注于偏好数据的扩展研究。未来，团队的研究范围将逐渐扩展到其他未充分探索的领域，如替代培训技术和建模目标。同时，考虑到该领域最近的发展趋势——奖励模型和奖励塑形机制已成为今天大规模语言模型训练管道的核心组成部分，适用于不仅基于人类偏好学习和行为指导的RLHF，也适用于数学、编程或一般推理任务的RLVR，以及基于代理的学习场景。

因此，我们展望奖励模型，或更广泛的统一奖励系统，有望成为未来AI基础设施的核心。它们将不再仅仅是行为或正确性的评估者，而将成为智能系统在复杂环境中导航的“指南针”，帮助它们与人类价值观对齐，并持续朝着更有意义的目标发展。

此外，Skywork在5月发布了世界首个深度研究AI工作区域代理，您可以通过访问：skywork.ai 进行体验。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。