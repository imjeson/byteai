---

title: '铰链物体的通用世界模型，超越扩散方法，入选CVPR 2025'
date: 2025-04-08
author: ByteAILab

---

基于当前观察，预测铰链物体的的运动，尤其是 part-level 级别的运动，是实现世界模型的关键一步。尽管现在基于 diffusion 的方法取得了很多进展，但是这些方法存在处理效率低，同时缺乏三维感知等问题，难以投入真实环境中使用。

---


清华大学联合北京大学提出了**第一个**基于重建模型的 part-level 运动的建模——**PartRM**。用户给定单张输入图像和对应的 drag ，PartRM 能生成观测物体未来状态的**三维表征**，使得生成数据能够真正服务于机器人操纵等任务。实验证明 PartRM 在生成结果上都取得了显著的提升。该研究已入选CVPR 2025。

![图片](https://image.jiqizhixin.com/uploads/editor/7fd05184-1f06-4e03-9d2f-d8b544c281fb/640.png)

- 论文题目：PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model
- 论文主页：https://partrm.c7w.tech/
- 论文链接：https://arxiv.org/abs/2503.19913
- 代码链接：https://github.com/GasaiYU/PartRM

**研究动机**

世界模型是一种基于当前观察和动作来预测未来状态的函数。该模型的研发使得计算机能够理解物理世界中的复杂规律，在机器人等领域得到了广泛应用。近期，对 part-level 的动态建模的兴趣日益增长，给定当前时刻的观察并给与用户给定的拖拽，预测下一时刻的铰链物体各个部件的运动受到越来越多的关注，这种类型的世界模型对于需要高精度的任务，例如机器人的操纵任务等，具有重要的意义。

然而，我们对这个充满前景的领域的调研表明，目前的前沿研究（如 Puppet-Master）通过对预训练的 大规模视频扩散模型进行微调，以实现增加拖拽控制的功能。尽管这种方法有效地利用了预训练过程中 学习到的丰富运动模式，但在实际应用中仍显不足。其中一个主要局限是它仅输出单视角视频作为表示，而模拟器需要三维表示来从多个视角渲染场景。此外，扩散去噪过程可能需要几分钟来模拟单个拖 拽交互，这与为操作策略（Manipulation Policies）提供快速试错反馈的目标相悖。

因此，我们需要采用**三维表征**，为了实现从输入单视角图像的快速三维重建，我们利用基于三维高斯泼溅（3DGS）的大规模重建模型，这些模型能以前馈方式从输入图像预测三维高斯泼溅，使重建时间从传统优化方法所需的几分钟减少到仅需几秒钟。同时，通过将用户指定的拖拽信息加入到大规模三维重建 网络中，我们实现了部件级别的动态建模。在这个问题中，我们认为联合建模运动和几何是至关重要的，因为部件级运动本质上与每个部件的几何特性相关联（例如，抽屉在打开时通常沿其法线方向滑动）。这种集成使我们能够实现更真实和可解释的部件级动态表示。

同时，由于我们是第一个做这个任务的，在这个任务上缺少相关的数据集，因此我们基于 PartNet- Mobility 构建了**PartDrag-4D** 数据集，并在这个数据集上建立了衡量对部件级别动态建模的基准（Benchmark），实验结果表明，我们的方法在定量和定性上都取得了最好的效果。

![图片](https://image.jiqizhixin.com/uploads/editor/4d49fee1-b12d-434d-ae49-e7ce6d382c07/640.png)

**PartDrag-4D 数据集的构建**

我们首先定义 PartRM 需要完成的任务，给定单张铰链物体的图像 ot 和用户指定的拖拽 at ，我们需要设计 一个模型，完成

![图片](https://image.jiqizhixin.com/uploads/editor/cac8d151-278a-4796-a056-0a77e234af9b/640.png)

其中 ![图片](https://image.jiqizhixin.com/uploads/editor/89c5b56a-71d1-4766-b1ad-9b511ddffee9/640.png) 是 Ot 在 at 作用下的三维表征。

现有的数据集分为两种， 一种是只含有![图片](https://image.jiqizhixin.com/uploads/editor/ca7e9faa-cf17-47d1-99fa-8f9fc25248c0/640.png)数据对，但是缺乏对应的三维表征（比如 DragAPart 中提出的 Drag-a-Move 数据集）。还有一种是通用数据集，比如 Objaverse 中的动态数据，这种数据不止还有部件级别的运动，还会含有物体整体的变形等运动，不适合我们训练。

因此，我们基于提供铰链物体部件级别标注的 PartNet-Mobility 数据集构建了 PartDrag-4D 数据集。我们选取了 8 种铰链物体（其中 7 种用于训练， 1 种用于测试），共 738 个 mesh。对于每个 mesh，如图所示，我们使其中某个部件在两种极限状态（如完全闭合到完全开启）间运动至 6 个状态，同时将其他部分状态 设置为随机，从而产生共 20548 个状态，其中 20057 个用于训练，491 个用于测试。为渲染多视角图像，我们利用 Blender 为每个 mesh 渲染了 12 个视角的图像。对于两个状态之间拖拽数据的采样，我们在铰链物体运动部件的 Mesh 表面选取采样点，并将两个状态中对应的采样点投影至 2D 图像空间，即可获得对应的拖拽数据。

**PartRM 方法**

**方法概览**

![图片](https://image.jiqizhixin.com/uploads/editor/44a8d910-30e7-40a8-bec1-8d8fc33d0a2d/640.png)

上图提供了 **PartRM 方法的概述**，给定一个单视角的铰链物体的图像 ot 和对应的拖拽 at，我们的目标是生成对应的 3D 高斯泼溅![图片](https://image.jiqizhixin.com/uploads/editor/9fa39936-381c-45ef-b6d3-0c590f9eafbf/640.png)。我们首先会利用多视角生成模型 Zero123++ 生成输入的多视角图像，然后对输入的拖拽在用户希望移动的 Part 上进行传播。这些多视角的图像和传播后的拖拽会输入进我们设计的网络中，这个网络会对输入的拖拽进行多尺度的嵌入，然后将得到的嵌入拼接到重建网络的下采样层中。在训练过程中，我们采用两阶段训练方法，第一阶段学习 Part 的运动，利用高斯库里的 3D 高斯进行 监督，第二阶段学习外观，利用数据集里的多视角图像进行监督。

**图像和拖拽的预处理**

**图像预处理**：由于我们的主网络是基于 LGM 设计的， LGM 需要多视角的图像作为输入，所以我们需要将 输入的单视角图像变成多视角，我们利用多视角图像生成网络 Zero123++，为了使得 Zero123++ 生成的  图像质量更高，我们会在训练集上对其进行微调。

![图片](https://image.jiqizhixin.com/uploads/editor/ed087217-c716-4590-847b-83755213083b/640.png)

**拖拽传播**：如果用户只输入一个拖拽，后续网络可能会对拖拽的区域产生幻觉从而出错，因此我们需要 对拖拽进行传播到需要被拖拽部分的各个区域，使得后续网络感知到需要被拖拽的区域，为此我们设计了一个拖拽传播策略。如图所示，我们首先拿用户给定的拖拽的起始点输入进 Segment Anything 模型中得到对应的被拖拽区域的掩码，然后在这个掩码区域内采样一些点作为被传播拖拽的起始点，这些被传播的拖拽的强度和用户给定的拖拽的强度一样。尽管在拖动强度大小的估计上可能存在不准确性，我们后续的模型仍然足够稳健，能够以数据驱动的方式学习生成预期的输出。

**拖拽嵌入**

![图片](https://image.jiqizhixin.com/uploads/editor/5cee8f54-4498-4dae-9de9-047846d70491/640.png)

PartRM 重建网络的 UNet 部分沿用了 LGM 的网络架构，为了将上一步处理好的拖拽注入到重建网络中， 我们设计了一个多尺度的拖拽嵌入。具体地，对于每一个拖拽，我们会将它的起始点和终止点先过一个 Fourier 嵌入，然后过一个三层的 MLP：

![图片](https://image.jiqizhixin.com/uploads/editor/4b63d8b5-7ec1-4ad2-9a55-58d9d3bbc882/640.png)

其中![图片](https://image.jiqizhixin.com/uploads/editor/9fa39936-381c-45ef-b6d3-0c590f9eafbf/640.png)代表第 i 个拖拽在第 l 层的嵌入，其余部分设为 0。F 代表 Fourier 嵌入和 MLP ，![图片](https://image.jiqizhixin.com/uploads/editor/ac4abfe6-1c3a-4ee2-b1d2-7ef125a49d11/640.png)代表在 channel 维度上连接。得到第 l 层的嵌入后，我们将 ![图片](https://image.jiqizhixin.com/uploads/editor/4b27f124-fc11-43b1-a0e8-88679d56509f/640.png) 和网络第 l 层的输出 Ol 在 channel 维度上连接，并过一个卷积层，作为 Ol 的残差加到 Ol 上作为下一层的输入，具体地：

![图片](https://image.jiqizhixin.com/uploads/editor/e7adc46f-de33-4204-ab2b-7986bdd3f4f2/640.png)

其中卷积层的参数全零初始化，![图片](https://image.jiqizhixin.com/uploads/editor/10ea35bb-732f-4879-ab4a-e84e7fd102bb/640.png)为第 l + 1 层的输入。

**两阶段训练流程**

为了保证对静态 3D 物体外观和几何的建模能力，我们在预训练的 LGM 基础上构建了 PartRM。但直接在新数据集上微调会导致已有知识灾难性遗忘，从而降低对真实场景数据的泛化能力。为此，我们提出了两阶段学习方法：先专注于学习之前未涉及的运动信息，再训练外观、几何和运动信息，以确保更好的性能。

运动学习阶段：在运动学习阶段，我们期望模型能够学到由输入的拖拽引起的运动。我们首先利用在我们的数据集上微调好的 LGM 去推理每个状态 Mesh 对应的 3D 高斯泼溅表征，拿这些作为监督数据我们第 一阶段的训练。对于两个 3D 高斯之间的对应，我们利用 LGM 输出的是一个 splatter image 这一优势，即 LGM 会对 2D 图像的每一个像素点学一个高斯泼溅，我们可以直接对监督数据和 PartRM 网络预测的输出  做像素级别的 L2 损失，即：

![图片](https://image.jiqizhixin.com/uploads/editor/410f6cd8-03aa-4606-a6a9-834887d05807/640.png)

其中 i 代表在 splatter image 里的坐标， GSi 和 GSj 均为每个像素点对应的 14 维高斯球参数。

外观学习阶段：在运动学习阶段之后，我们引入了一个额外的阶段来联合优化输出的外观，几何以及部件级别的运动。这个阶段我们会渲染我们输出的 3D 高斯，利用数据集中提供的多视角图像计算一个损失，具体地：

![图片](https://image.jiqizhixin.com/uploads/editor/0262cdc5-01cb-4f0e-a502-1c4670cba75b/640.png)

**实验结果**

**实验设置**

我们在两个数据集上来衡量我们提出的 PartRM 方法，这两个数据集包括我们提出的 PartDrag-4D 数据集以及通用数据集 Objaverse-Animation-HQ。因为 Objaverse-Animation-HQ 数据量比较大，我们只从其中采样 15000 条数据，然后手动拆分训练集和测试集。验证时，我们对输出的 3D 高斯渲染 8 个不同的视角，在这 8 个视角上算 PSNR ，SSIM 和 LPIPS 指标。

我们选用 DragAPart ， DiffEditor 和 Puppet-Master 作为我们的 baseline。对于不需要训练的 DiffEditor 方法，我们直接拿它官方的训练权重进行推理。对于需要训练的 DragAPart 和 Puppet-Master，我们在训练 集上对他们进行微调。

由于现有的方法只能输出 2D 图像，不能输出 3D 表征，为了和我们的任务对齐，我们设计了两种方法。第一种称为 **NVS-First**，即我们首先对输入的单视角图像利用 Zero123++ 生成多视角图像，再分别对每个视角结合每个视角对应的拖拽进行推理，生成对应的图像后再进行 3D 高斯重建；第二种称为 Drag-First，

即我们首先先对输入视角进行拖拽，然后对生成的结果利用 Zero123++ 进行多视角生成，最后进行 3D 高斯重建。我们采用了两种 3D 高斯重建方法，第一种为直接用 LGM （下图中两个时间的第一个）进行重建，第二种利用基于优化的 3D 高斯泼溅进行重建（下图中两个时间的第二个）。

**定性比较**

![图片](https://image.jiqizhixin.com/uploads/editor/28601bc2-00a2-4dc5-a330-f3b4db2e9354/640.png)

![图片](https://image.jiqizhixin.com/uploads/editor/777a2abc-b6ad-4a06-8bda-aa51c4e9df27/640.png)

在视觉效果方面， PartRM 通过对外观，几何和运动的联合建模，能够在抽屉开合等场景中生成物理合理的三维表征。相比之下， DiffEditor 由于缺乏三维感知，导致部件形变错位； DragAPart 虽然能够处理简单的关节运动，但在生成微波门板时出现了明显的伪影等问题，同时在通用数据集上表现不佳；Puppet- Master 在外观的时间连续性和运动部分的建模方面表现不佳。

![图片](https://image.jiqizhixin.com/uploads/editor/6ed0f471-e294-4cc4-8610-5287981e1934/640.png)

**定量比较**

![图片](https://image.jiqizhixin.com/uploads/editor/494b1878-e6fd-4220-847c-532e9bd4f6d6/640.png)

定量评估中， PartRM 在 PSNR、SSIM、 LPIPS 指标上较基线模型**均有提升**；同时**大幅提升了生成效率**， PartRM 仅需 4 秒即可完成单次生成，而传统方案需分步执行 2D 形变与三维重建。

**总结**

本文介绍了 PartRM ，一种同时建模外观、几何和部件级运动的新方法。为了解决 4D 部件级运动学习中的数据稀缺问题，我们提出了 PartDrag-4D 数据集，提供了部件级动态的多视角图像。实验结果表明，我们的方法在部件运动学习上优于以往的方法，并且可应用于具身 AI 任务。然而，对于与训练分布差异较大的关节数据，可能会遇到挑战。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。