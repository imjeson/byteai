---
title: '微软让MoE长出多个头，大幅提升专家激活率'
date: 2024-05-15
author: ByteAILab

---

MH-MoE 能优化几乎所有专家，实现起来非常简单。

混合专家（MoE）是个好方法，支持着现在一些非常优秀的大模型，比如谷歌家的 Gemini 1.5 以及备受关注的 Mixtral 8x7B。

---


稀疏混合专家（SMoE）可在不显著增加训练和推理成本的前提下提升模型的能力。比如 Mixtral 8×7B 就是一个 SMoE 模型，其包含 8 个专家（共 7B 参数），而其表现却可以超过或比肩 LLaMA-2 70B 和 GPT-3.5。

但是，它也有两个问题。一是专家激活率低 —— 也就是搞不好会出现下图这种情况：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9JliaD88gCB7QLXatial8ujIY5zVO3VfMuG4r6nuhmia1ibH9DCq1VUCpLJodWhfwQ0G8gznHKdOicmFQ/640?wx_fmt=jpeg&from=appmsg)

具体来说，就是在优化时只有一小部分专家会被激活，如图 1a 所示（8.33% 的激活率），这会导致在学习应对复杂任务的大量专家时，会出现性能次优和效果不佳的问题。

二是无法细粒度地分析单个 token 的多重语义概念，比如多义词和具有多重细节的图块。

近日，微软研究院和清华大学提出了多头混合专家（MH-MoE）。顾名思义，MH-MoE 采用了多头机制，可将每个输入 token 分成多个子 token。然后将这些子 token 分配给一组多样化的专家并行处理，之后再无缝地将它们整合进原来的 token 形式。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9JliaD88gCB7QLXatial8ujI9tyj5vrqBFbiczicQ2UuwrHtj6HJ7UBjYmY155zxrxME3CUvIhm3FHcA/640?wx_fmt=png&from=appmsg)

- 论文标题：Multi-Head Mixture-of-Experts
- 论文地址：https://arxiv.org/pdf/2404.15045
- 代码地址：https://github.com/yushuiwx/MH-MoE

图 2 展示了 MH-MoE 的工作流程。可以看到，当输入单个 token 时，MH-MoE 会将其分成 4 个子 token，进而激活 4 个专家，而 SMoE 仅激活 1 个专家。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9JliaD88gCB7QLXatial8ujIkaLJEZXtZTF2TwKyxbldYaQs76v6aG71ibjvb4M7uAyowHsL7rP6wTQ/640?wx_fmt=png&from=appmsg)

如图 2 所示，分配给专家 3 和 2 的子 token 包含对图块内每个角色动作的详细理解，而分配给专家 1 和 4 的子 token 则显式地建模了错误的同源词「camera」的语义。

专家处理完成后，再将子 token 无缝地重新整合进原来的 token 形式，由此可以避免后续非并行层（例如注意力层）的任何额外计算负担，同时还集成从多个专家捕获的语义信息。

这样的操作可让 MH-MoE 从整体上关注来自不同专家内不同表征空间的信息，从而可以加深上下文理解能力，同时提升专家激活率。该项目的代码也将发布。

MH-MoE 的具有以下优势：

1. 专家激活率更高且扩展性更好。MH-MoE 能优化几乎所有专家，从而可以缓解专家激活率低的问题并大幅提升更大专家的使用率，实现了 90.71% 的激活率，这能让模型能力获得更高效的扩展。
2. 具有更细粒度的理解能力。MH-MoE 采用的多头机制会将子 token 分配给不同的专家，联合关注来自不同专家的不同表征空间的信息，获得更好更细粒度的理解能力。
3. 可实现无缝整合。MH-MoE 实现简单，可与其他优化方法整合使用以获得更好性能。

**方法**

图 3 给出了 MH-MoE 的整体架构，其使用了多头机制将每个 token 分拆为子 token，然后将这些子 token 路由给不同的专家。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9JliaD88gCB7QLXatial8ujIUveVMqsKfktAXsoIQtCyeqQNH6yLZtSdfJ8bNiart8K9GketAPqBEyg/640?wx_fmt=png&from=appmsg)

**多头混合专家**

为了能清楚说明，这里...

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。