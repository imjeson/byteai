---

title: '英格兰各市议会使用的AI工具低估女性健康问题，研究发现'
date: 2025-08-12
author: ByteAILab

---

人工智能工具在英格兰超过一半的市议会中被使用，这些工具低估了女性的身体和心理健康问题，并存在在护理决策中造成性别偏见的风险，研究发现。

---
这项研究发现，当使用谷歌的AI工具“Gemma”生成并总结相同的案例记录时，描述男性的语言中出现“残疾”、“无法”和“复杂”等词汇的频率显著高于女性。伦敦政治经济学院（LSE）的研究还发现，女性的类似护理需求更可能被省略或以较轻的措辞描述。研究报告的首席作者、LSE护理政策与评估中心的研究员萨姆·里克曼（Dr Sam Rickman）表示，AI可能导致女性接受“不平等的护理”。“我们知道这些模型已经被广泛使用，令人担忧的是，我们发现不同模型之间在偏见衡量上有很大的差异，”他说。“尤其是谷歌的模型，在低估女性的身体和心理健康需求方面表现得更为明显。由于你所获得的护理是基于感知的需求，这可能导致女性在实际使用偏见模型时接受更少的护理。但是我们目前并不知道究竟使用了哪些模型。”

AI工具越来越多地被地方政府用来减轻被过度施压的社工的工作负担，尽管关于具体使用的AI模型、频率以及对决策的影响的信息甚少。LSE的研究使用了617位成年社会护理用户的真实案例记录，这些记录多次输入到不同的大型语言模型（LLMs）中，仅性别发生变化。研究人员分析了29,616对摘要，以查看AI模型如何不同地对待男性和女性的案例。在一个例子中，Gemma模型将一组案例记录总结为：“史密斯先生是一位84岁的男性，独居，具有复杂的医疗历史，没有护理方案，行动能力差。”同样的案例记录在性别转换后输入到同一模型中，摘要为：“史密斯夫人是一位84岁的女性，独居。尽管有一些限制，但她是独立的，能够保持个人护理。”在另一个例子中，案例摘要中提到史密斯先生“无法进入社区”，而史密斯夫人则被描述为“能够管理日常活动”。在测试的AI模型中，谷歌的Gemma模型所造成的性别差异更为明显，而Meta的Llama 3模型则未根据性别使用不同的语言，研究发现。

里克曼表示，这些工具“已经在公共部门被使用，但它们的使用不应以牺牲公平为代价。”他补充道，“尽管我的研究突显了一个模型的问题，但还有更多模型正在不断部署，确保所有AI系统的透明性、严谨的偏见测试以及严格的法律监督变得至关重要。”论文总结指出，监管机构“应该强制要求测量用于长期护理的LLMs中的偏见”，以优先考虑“算法公平性”。关于AI工具中的种族和性别偏见早有担忧，因为机器学习技术已被发现吸收人类语言中的偏见。一项美国研究分析了跨不同行业的133个AI系统，发现约44%显示出性别偏见，25%表现出性别和种族偏见。谷歌表示，其团队将检查研究报告的发现。谷歌的研究人员测试了Gemma模型的第一代，目前该模型已进入第三代，预计性能会有所改善，尽管从未声明该模型应用于医疗目的。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。