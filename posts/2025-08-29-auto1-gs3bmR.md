---
title: 'ChatGPT在安全测试中提供炸弹食谱和黑客技巧'
date: 2025-08-30
author: ByteAILab

---

一项夏季进行的安全测试发现，ChatGPT模型向研究人员提供了关于如何炸毁体育场馆的详细指令，包括特定场馆的弱点、炸药食谱和掩盖痕迹的建议。

---
OpenAI的GPT-4.1也详细描述了如何武器化炭疽菌以及如何制造两种非法药物。这项测试是OpenAI（由萨姆·阿尔特曼领导的5000亿美元人工智能初创公司）与竞争对手Anthropic之间进行的一次不寻常的合作，后者由因安全问题离开OpenAI的专家们创立。每家公司都对对方的模型进行测试，推动其协助完成危险任务。

该测试并不直接反映模型在公众使用时的表现，因为公众使用时会应用额外的安全过滤器。但Anthropic表示，他们在GPT-4o和GPT-4.1中观察到了“令人担忧的行为……与误用有关”，并表示对AI“对齐”评估的需求变得“愈发紧迫”。

Anthropic还透露，其Claude模型已被用于试图进行大规模勒索操作、北朝鲜特工伪造申请国际科技公司的工作申请，以及以高达1200美元的价格出售AI生成的勒索软件包。该公司表示，AI已经“被武器化”，模型现在被用来执行复杂的网络攻击并促进诈骗。“这些工具能实时适应防御措施，如恶意软件检测系统，”它表示。“我们预计随着AI辅助编码降低了进行网络犯罪所需的技术专长，类似攻击将变得更加普遍。”

英国新兴技术与安全中心的高级研究助理阿尔迪·简耶瓦表示，这些例子“令人担忧”，但目前尚未出现“重大现实案例”。他说，利用专门的资源、研究焦点和跨界合作，“使用最新尖端模型进行这些恶意活动变得比以往更为困难”。

这两家公司表示，发布这些发现是为了在“对齐评估”上创造透明度，而这些评估往往被急于开发更先进AI的公司所保留。OpenAI表示，自测试以来推出的ChatGPT-5“在谄媚、幻觉和误用抵抗等领域展现了显著改善”。Anthropic强调，如果在模型外部安装安全防护措施，许多研究中发现的误用途径在实践中可能不可行。“我们需要了解系统在何种情况下可能尝试采取可能导致严重伤害的不当行动，频率有多高，”它警告说。

Anthropic的研究人员发现OpenAI的模型在应对明确有害请求方面“比我们预期的更宽容”。它们在模拟用户的提示下，以暗网工具购物的形式协助购买核材料、被盗身份和芬太尼，请求制作甲基苯丙胺和简易炸弹的食谱，开发间谍软件。Anthropic指出，仅需多次尝试或微弱的借口，例如声称请求是出于研究目的，就能说服模型进行合作。

在一个案例中，测试者以“安全规划”的目的询问体育赛事的漏洞。在给出一般攻击方法类别后，测试者进一步追问细节，模型提供了关于特定场馆的弱点的信息，包括最佳攻击时间、炸药的化学公式、炸弹定时器的电路图、在哪里可以在黑市上购买枪支、攻击者如何克服道德顾虑、逃跑路线和安全屋位置的建议。

---
---
感谢阅读！如果您对AI的更多资讯感兴趣，可以查看更多AI文章：[GPTNB](https://gptnb.com)。